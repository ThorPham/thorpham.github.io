<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.17">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="ThorPham RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="ThorPham Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous"><title data-rh="true">Quá trình phát triển của CNN từ LeNet đến DenseNet. | ThorPham</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://thorpham.github.io//blog/Quá-trình-phát-triển-của-CNN-từ-LeNet-đến-DenseNet"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Quá trình phát triển của CNN từ LeNet đến DenseNet. | ThorPham"><meta data-rh="true" name="description" content="Convolutional neural network là một mạng neural được ứng dụng rất nhiều trong deep learning trong computer vision cho classifier và localizer . Từ mạng CNN cơ bản người ta có thể tạo ra rất nhiều architect khác nhau, từ những mạng neural cơ bản 1 đến 2 layer đến 100 layer. Đã bao giờ bạn tự hỏi nên sử dụng bao nhiêu layer, nên kết hợp conv với maxpooling thế nào? conv-maxpooling hay conv-conv-maxplooling ? hay nên sử dụng kernel 3x3 hay 5x5 thậm chí 7x7 điểm khác biệt là gì ? Làm gì khi model bị vanishing/exploding gradient, hay tại sao thi thêm nhiều layer hơn thì theo lý thuyết accuarcy phải cao hơn so với shallow model, nhưng thực tế lại không phải accuarcy không tăng thậm chí là giảm đó có phải nguyên nhân do overfitting .Trong bài viết này ta sẽ tìm hiểu các architure nổi tiếng để xem cấu trúc của nó như thế nào, các ý tưởng về CNN mới nhất hiện nay  từ đó ta có thể trả lời được mấy câu hỏi trên"><meta data-rh="true" property="og:description" content="Convolutional neural network là một mạng neural được ứng dụng rất nhiều trong deep learning trong computer vision cho classifier và localizer . Từ mạng CNN cơ bản người ta có thể tạo ra rất nhiều architect khác nhau, từ những mạng neural cơ bản 1 đến 2 layer đến 100 layer. Đã bao giờ bạn tự hỏi nên sử dụng bao nhiêu layer, nên kết hợp conv với maxpooling thế nào? conv-maxpooling hay conv-conv-maxplooling ? hay nên sử dụng kernel 3x3 hay 5x5 thậm chí 7x7 điểm khác biệt là gì ? Làm gì khi model bị vanishing/exploding gradient, hay tại sao thi thêm nhiều layer hơn thì theo lý thuyết accuarcy phải cao hơn so với shallow model, nhưng thực tế lại không phải accuarcy không tăng thậm chí là giảm đó có phải nguyên nhân do overfitting .Trong bài viết này ta sẽ tìm hiểu các architure nổi tiếng để xem cấu trúc của nó như thế nào, các ý tưởng về CNN mới nhất hiện nay  từ đó ta có thể trả lời được mấy câu hỏi trên"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2018-10-08T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/ThorPham"><meta data-rh="true" property="article:tag" content="Deep learning,CNN"><link data-rh="true" rel="icon" href="/img/emoticon_big.png"><link data-rh="true" rel="canonical" href="https://thorpham.github.io//blog/Quá-trình-phát-triển-của-CNN-từ-LeNet-đến-DenseNet"><link data-rh="true" rel="alternate" href="https://thorpham.github.io//blog/Quá-trình-phát-triển-của-CNN-từ-LeNet-đến-DenseNet" hreflang="en"><link data-rh="true" rel="alternate" href="https://thorpham.github.io//blog/Quá-trình-phát-triển-của-CNN-từ-LeNet-đến-DenseNet" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.66d4bf94.css">
<link rel="preload" href="/assets/js/runtime~main.375a46b9.js" as="script">
<link rel="preload" href="/assets/js/main.fb378e59.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/emoticon_big.png" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/emoticon_big.png" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Thorpham</b></a><a class="navbar__item navbar__link" href="/docs/intro">Tutorial</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/Archive">Archive</a><a href="https://github.com/ThorPham" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_S7eR toggle_TdHA toggleDisabled_f9M3"><div class="toggleButton_rCf9" role="button" tabindex="-1"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></div><input type="checkbox" class="toggleScreenReader_g2nN" aria-label="Switch between dark and light mode (currently light mode)"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-post-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/Graph-convolution-network-cho-bài-toán-rút-trích-thông-tin">Hướng tiếp cận Graph convolution network cho bài toán rút trích thông tin từ hóa đơn</a></li><li class="sidebarItem_CF0Q"><a aria-current="page" class="sidebarItemLink_miNk sidebarItemLinkActive_RRTD" href="/blog/Quá-trình-phát-triển-của-CNN-từ-LeNet-đến-DenseNet">Quá trình phát triển của CNN từ LeNet đến DenseNet.</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/Object-detection-từ-R-CNN-đến-Faster-R-CNN">Object detection từ R-CNN đến Faster R-CNN</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/Sentiment-Analysis-sử-dụng-Tf-Idf">Sentiment Analysis sử dụng Tf-Idf áp dụng cho ngôn ngữ tiếng việt</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/blog/Nhận-dạng-chữ-số-viết-tay">Nhận dạng chữ số viết tay với sklearn và opencv</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_rzP5" itemprop="headline">Quá trình phát triển của CNN từ LeNet đến DenseNet.</h1><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2018-10-08T00:00:00.000Z" itemprop="datePublished">October 8, 2018</time> · <!-- -->16 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/ThorPham" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://github.com/ThorPham.png" alt="Thorpham"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/ThorPham" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Thorpham</span></a></div><small class="avatar__subtitle" itemprop="description">Deep learning enthusiast</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p><em>Convolutional neural network là một mạng neural được ứng dụng rất nhiều trong deep learning trong computer vision cho classifier và localizer . Từ mạng CNN cơ bản người ta có thể tạo ra rất nhiều architect khác nhau, từ những mạng neural cơ bản 1 đến 2 layer đến 100 layer. Đã bao giờ bạn tự hỏi nên sử dụng bao nhiêu layer, nên kết hợp conv với maxpooling thế nào? conv-maxpooling hay conv-conv-maxplooling ? hay nên sử dụng kernel 3x3 hay 5x5 thậm chí 7x7 điểm khác biệt là gì ? Làm gì khi model bị vanishing/exploding gradient, hay tại sao thi thêm nhiều layer hơn thì theo lý thuyết accuarcy phải cao hơn so với shallow model, nhưng thực tế lại không phải accuarcy không tăng thậm chí là giảm đó có phải nguyên nhân do overfitting .Trong bài viết này ta sẽ tìm hiểu các architure nổi tiếng để xem cấu trúc của nó như thế nào, các ý tưởng về CNN mới nhất hiện nay  từ đó ta có thể trả lời được mấy câu hỏi trên</em></p><center><img width="600" height="300" src="/assets/images/36774671_240413323222236_1459661677975830528_n-5c52132447d62047444e2d112bb13c90.png"></center><center>Hình 1. Quá trình phát triển của CNN</center><h2 class="anchor anchorWithStickyNavbar_mojV" id="1-lenet1998">1. LeNet(1998)<a class="hash-link" href="#1-lenet1998" title="Direct link to heading">​</a></h2><p>LeNet là một trong những mạng CNN lâu đời nổi tiếng nhất được Yann LeCUn phát triển vào những năm 1998s. Cấu trúc của LeNet gồm 2 layer (Convolution + maxpooling) và 2 layer fully  connected  layer và output là softmax layer .  </p><center><img width="600" height="300" src="/assets/images/36833992_240414163222152_4178930615535534080_n-fe7855077a16b7a10a600b8a72e9bb66.png"></center><center>Hình 2. LeNet (Source CNN của Andrew Ng)</center><p>Chúng ta cùng tìm hiểu chi tiết architect của LeNet đối với dữ liệu mnist (accuracy lên đến 99%) :</p><ul><li>Input shape 28x28x3</li><li>Layer 1 :<ul><li>Convolution layer 1 : Kernel 5x5x3 , stride = 1,no padding, number filter = 6 ,output = 28x28x6.</li><li>Maxpooling layer : pooling size 2x2,stride = 2,padding = “same”,output = 14x14x6.</li></ul></li><li>Layer 2 :<ul><li>Convolution layer 2 : kernel 5x5x6,stride = 1, no padding, number filter = 16,output = 10x10x16.</li><li>Maxpooling layer : pooling size = 2x2, stride = 2, padding =”same”,output = 5x5x16.</li></ul></li><li>Flatten output = 5x5x16 = 400</li><li>Fully connection 1 : output = 120</li><li>Fully connection 2 : output = 84</li><li>Softmax layer, output = 10 (10 digits).</li></ul><p>Nhược điểm của LeNet là mạng còn rất đơn giản và sử dụng sigmoid (or tanh) ở mỗi convolution layer mạng tính toán rất chậm.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="2-alexnet2012">2. Alexnet(2012)<a class="hash-link" href="#2-alexnet2012" title="Direct link to heading">​</a></h2><p>AlexNet là một mạng CNN đã dành chiến thắng trong cuộc thi ImageNet LSVRC-2012 năm 2012 với large margin (15.3% VS 26.2% error rates). AlexNet là một mạng CNN traning với một số lượng parameter rất lớn (60 million) so với LeNet. Một số đặc điểm:</p><ul><li>Sử dụng relu thay cho sigmoid(or tanh) để xử lý với non-linearity. Tăng tốc độ tính toán lên 6 lần.</li><li>Sử dụng dropout như một phương pháp regularization mới cho CNN. Dropout không những giúp mô hình tránh được overfitting mà còn làm giảm thời gian huấn luyện mô hình </li><li>Overlap pooling để giảm size của network ( Traditionally pooling regions không overlap).</li><li>Sử dụng local response normalization để chuẩn hóa ở mỗi layer.</li><li>Sử dụng kỹ thuật data augmentation để tạo thêm data training bằng cách translations, horizontal reflections.</li><li>Alexnet training với 90 epochs trong 5 đến 6 ngày với 2 GTX 580 GPUs. Sử dụng SGD với learning rate 0.01, momentum 0.9 và weight decay 0.0005. </li></ul><center><img width="600" height="300" src="/assets/images/36767372_240415716555330_8179137527236526080_n-4b5f05ec55635c4875809140660b79f7.png"></center><center>Hình 3. AlexNet (Nguồn ImageNet Classification with Deep Convolutional Neural Networks)</center><p>Architect của Alexnet gồm 5 convolutional layer và 3 fully  connected  layer. Activation Relu được sử dụng sau mỗi convolution và fully connection layer. Detail architecture với dataset là imagenet size là 227x227x3 với 1000 class ( khác với trong hình trên size là 224x224). </p><p>Detail Architect:</p><ul><li>Input shape 227x227x3.</li><li>Layer 1 :<ul><li>Conv 1 : kernel : 11x11x3,stride = 4,no padding, number = 96,activation = relu,output = 55x55x96.</li><li>Maxpooling layer : pooling size = 3x3,stride = 2,padding =”same” ,output = 27x27x96.</li><li>Normalize layer.</li></ul></li><li>Layer 2 :<ul><li>Conv 2 : kernel :3x3x96,stride = 1, padding = “same”, number filter = 256,activation = relu,output = 27x27x256.</li><li>Maxpooling layer : pooling size = 3x3,stride=2, padding =”same”,output = 13x13x256.</li><li>Normalize layer.</li></ul></li><li>Layer 3:<ul><li>Conv 3 : kernel :3x3x256, stride = 1,padding=”same”, number filter = 384, activation = relu, output = 13x13x384.</li></ul></li><li>Layer 4:<ul><li>Conv 4 : kernel : 3x3x384 , stride = 1, padding = “same”, number filter = 384, activation= relu, output = 13x13x384</li></ul></li><li>Layer 5 :<ul><li>Conv 5 : kernel 3x3x384, stride = 1, padding = “same”, number filter = 256, activation = relu, output = 13x13x256.</li><li>Pooling layer : pooling size = 3x3,stride =2,padding =”same”,output = 6x6x256.</li></ul></li><li>Flatten 256x6x6 = 9216</li><li>Fully connection layer 1 : activation = relu , output = 4096 + dropout(0.5).</li><li>Fully connection layer 2 : activation = relu , output = 4096 + dropout(0.5).</li><li>Fully connection layer 3 : activation = softmax , output = 1000 (number class) </li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="3-zfnet2013">3. ZFNet(2013)<a class="hash-link" href="#3-zfnet2013" title="Direct link to heading">​</a></h2><p>ZFNet là một mạng cnn thắng trong ILSVRC 2013 với top-5 error rate của 14.8% . ZFNet có cấu trúc rất giống với AlexNet với 5 layer convolution , 2 fully connected layer và 1 output softmax layer. Khác biệt ở chỗ kernel size ở mỗi Conv layer .Một số đặc điểm chính :</p><ul><li>Tương tự AlexNet nhưng có một số điều chỉnh nhỏ.</li><li>Alexnet training trên 15m image trong khi ZF training chỉ có 1.3m image.</li><li>Sử dụng kernel 7x7 ở first layer (alexnet 11x11).Lý do là sử dụng kernel nhỏ hơn để giữ lại nhiều thông tin trên image hơn.</li><li>Tăng số lượng filter nhiều hơn so với alexnet</li><li>Training trên GTX 580 GPU trong 20 ngày<center><img width="600" height="300" src="/assets/images/cnn4-2828b3f608b90f11a278b7a8a37cb10b.jpg"></center></li></ul><center>Hình 4. ZFNet(2013).</center><ul><li>Input shape 224x224x3 .</li><li>Layer 1 :<ul><li>Conv 1 : kernel = 7x7x3, stride = 2, no padding, number filter = 96, output = 110x110x96.</li><li>Maxpooling 1 : pooling size = 3x3,stride=2, padding = “same”,output = 55x55x96</li><li>Normalize layer.</li></ul></li><li>Layer 2 :<ul><li>Conv 2 : kernel = 5x5x96, stride = 2, no padding, number filter = 256, output = 26x26x256.</li><li>Maxpooling 2 : pooling size = 3x3, stride=2,  padding = “same”,output = 13x13x256</li><li>Normalize layer.</li></ul></li><li>Layer 3:<ul><li>Conv 3 : kernel = 3x3x256, stride=1, padding=”same”, number filter = 384,output = 13x13x384.</li><li>Layer 4 :</li><li>Conv 4 : kernel = 3x3x384, stride=1, padding=”same”, number filter = 384,output = 13x13x384.</li></ul></li><li>Layer 5 :<ul><li>Conv 5 : kernel = 3x3x384, stride=1, padding=”same”, number filter = 256,output = 13x13x256.</li><li>Maxpooling  : pooling size = 3x3,stride =2,padding =”same”,output = 6x6x256.</li></ul></li><li>Flatten 6x6x256 = 9216</li><li>Fully connected 1 : activation = relu,output =4096</li><li>Fully connected 2 : activation = relu,output =4096</li><li>Softmax layer for classifier ouput = 1000</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="4-vggnet2014">4. VGGNet(2014).<a class="hash-link" href="#4-vggnet2014" title="Direct link to heading">​</a></h2><ul><li>Sau AlexNet thì VGG ra đời với một số cải thiện hơn , trước tiên là model VGG sẽ deeper hơn, tiếp theo là thay đổi trong thứ tự conv. Từ LeNet đến AlexNet đều sử dụng Conv-maxpooling còn VGG thì sử dụng 1 chuỗi Conv liên tiếp Conv-Conv-Conv ở middle và end của architect VGG. Việc này sẽ làm cho việc tính toán trở nên lâu hơn nhưng những feature sẽ vẫn được giữ lại nhiều hơn so với việc sử dụng maxpooling sau mỗi Conv. Hơn nữa hiện nay với sự ra đời của GPU giúp tốc độ tính toán trở nên nhanh hơn rất nhiều lần thì vấn đề này không còn đáng lo ngại. VGG cho small error hơn AlexNet trong ImageNet Large Scale Visual Recognition Challenge (ILSVRC) năm 2014. VGG có 2 phiên bản là VGG16 và VGG19.<center><img width="600" height="300" src="/assets/images/cnn10-b35d02ca5c1001b4fddbadcc247c2f86.jpg"></center><center> Hình 5. VGGNet(2014).</center></li><li>Architect của VGG16 bao gồm 16 layer :13 layer Conv (2 layer conv-conv,3 layer conv-conv-conv) đều có kernel 3x3, sau mỗi layer conv là maxpooling downsize xuống 0.5, và 3 layer fully connection. VGG19 tương tự như VGG16 nhưng có thêm 3 layer convolution ở 3 layer conv cuối ( thành 4 conv stack với nhau).
Detail parameter VGG16<center><img width="600" height="300" src="/assets/images/cnn15-f72ebbec5dad712981c35a80cf7b173b.jpg"></center><center> Hình 15. VGG16 </center></li><li>Sử dụng kernel 3x3 thay vì 11x11 ở alexnet(7x7 ZFNet). Kết hợp 2 conv 3x3 có hiểu quả hơn 1 cov 5x5 về receptive field giúp mạng deeper hơn  lại giảm tham số tính toán cho model.</li><li>3 Conv 3x3 có receptive field same 1 conv 7x7.</li><li>Input size giảm dần qua các conv nhưng tăng số chiều sâu.</li><li>Làm việc rất tốt cho task classifier và localizer ( rất hay được sử dụng trong object detection).</li><li>Sử dụng relu sau mỗi conv và training bằng batch gradient descent.</li><li>Có sử dụng data augmentation technique trong quá trình training.</li><li>Training với 4 Nvidia Titan Black GPUs trong 2-3 tuần.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="5-googlenet2014">5. GoogleNet(2014).<a class="hash-link" href="#5-googlenet2014" title="Direct link to heading">​</a></h2><p>Năm 2014, google publish một mạng neural do nhóm research của họ phát triển có tên là googleNet. Nó performance tốt hơn VGG, googleNet 6.7% error rate trong khi VGG là 7.3%  Ý tưởng chính là họ tạo ra một module mới có tên là inception giúp mạng traning sâu và nhanh hơn, chỉ có 5m tham số so với alexnet là 60m nhanh hơn gấp 12 lần.
Inception module là một mạng CNN giúp training wider(thay vì them nhiều layer hơn vì rất dễ xảy ra overfitting + tăng parameter người ta nghĩ ra tăng deeper ở mỗi tầng layer) so với mạng CNN bình thường. Mỗi layer trong CNN truyền thống sẽ extract các thông tin khác nhau. Output của 5x5 conv kernel sẽ khác với 3x3 kernel. Vậy để lấy những thông tin cần thiết cho bài toán của chúng ta thì nên dùng kernel size như thế nào ? Tại sao chúng sử dụng tất cả ta và sau đó để model tự chọn. Đó chính là ý tưởng của Inception module, nó  tính toán các kernel size khác nhau từ một input sau đó concatenate nó lại thành output. </p><center><img width="600" height="300" src="/assets/images/cnn5-32e573dcf8abba41d300b7c62574be8c.jpg"></center><center> Hình 6. Inception. </center><p>Trong inception người ta dùng conv kernel 1x1 với 2 mục đích là giảm tham số tính toán và dimensionality reduction . Dimensionality reduction có thể hiểu làm giảm depth của input (vd iput 28x28x100 qua kernel 1x1 với filter = 10 sẽ giảm depth về còn 28x28x10). Giảm chi phí tính toán có thể  hiểu qua ví dụ sau :</p><ul><li>Input shape 28x28x192 qua kernel 5x5 với 32 thì ouput là 28x28x32(padding same) thì  tham số tính toán là (5x5x192)*(28x28x32)=120 million</li><li>Input shape 28x28x192 qua kernel 1x1x192 filter = 16 , output = 28x28x16 tiếp tục với kernel 5x5x32 filter = 16 đươch output = 28x28x32. Tổng tham số tính toán : <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>28</mn><mi>x</mi><mn>28</mn><mi>x</mi><mn>16</mn><mo stretchy="false">)</mo><mo>∗</mo><mn>192</mn><mo>+</mo><mo stretchy="false">(</mo><mn>28</mn><mi>x</mi><mn>28</mn><mi>x</mi><mn>32</mn><mo stretchy="false">)</mo><mo>∗</mo><mo stretchy="false">(</mo><mn>5</mn><mi>x</mi><mn>5</mn><mi>x</mi><mn>16</mn><mo stretchy="false">)</mo><mo>=</mo><mn>2.4</mn><mo>+</mo><mn>10</mn><mo>=</mo><mn>12.4</mn><mi>m</mi><mi>i</mi><mi>l</mi><mi>l</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">(28x28x16)*192 + (28x28x32)*(5x5x16) = 2.4 + 10 = 12.4 million.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">28</span><span class="mord mathnormal">x</span><span class="mord">28</span><span class="mord mathnormal">x</span><span class="mord">16</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">192</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">28</span><span class="mord mathnormal">x</span><span class="mord">28</span><span class="mord mathnormal">x</span><span class="mord">32</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">5</span><span class="mord mathnormal">x</span><span class="mord">5</span><span class="mord mathnormal">x</span><span class="mord">16</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">2.4</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">10</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord">12.4</span><span class="mord mathnormal">mi</span><span class="mord mathnormal" style="margin-right:0.01968em">ll</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord">.</span></span></span></span></span>
Ta thấy với cùng output là 28x28x32 thì nếu dùng kernel 5x5x192 với 32 filter thì sẽ có tham số gấp 10 lần so với sử dụng kernel 1x1x192 sau đó dùng tiếp 1 kernel 5x5x16 với filter 32.
Inception hiện giờ có 4 version , ta sẽ cùng tìm hiểu sơ qua các version:</li><li>Inception v1 : có 2 dạng  là naïve và dimension reduction. Khác biệt chính đó là version dimension reduction nó dùng conv 1x1 ở mỗi layer để giảm depth của input giúp model có ít tham số hơn. Inception naïve có architect gồm 1x1 conv,3x3  conv, 5x5 conv và 3x3 maxpooling.<center><img width="600" height="300" src="/assets/images/cnn6-ebb444df634b449d04ce07dd8d35cdc4.jpg"></center></li></ul><center> Hình 7. Inception V1.</center><p>Inception v2 : Cải thiện version 1, thêm layer batchnormalize và giảm Internal Covariate Shift. Ouput của mỗi layer sẽ được normalize về Gaussian N(0,1). Conv 5x5 sẽ được thay thế bằng 2 conv 3x3 để giảm computation cost.</p><center><img width="600" height="300" src="/assets/images/cnn7-bd4a80868149bac6d7e935b1a122243b.jpg"></center><center> Hình 8. Inception V2. </center><p>Inception v3 : Điểm đáng chú ý ở version này là Factorization. Conv 7x7 sẽ được giảm về conv 1 dimesion là (1x7),(7x1). Tương tự conv 3x3 (3x1,1x3). Tăng tốc độ tính toán. Khi tách ra 2 conv thì làm model deeper hơn.</p><center><img width="600" height="300" src="/assets/images/cnn8-f46ac346cb6382ce2cc5d06985163d2a.jpg"></center><center> Hình 9. Inception V3. </center>Inception v4 : là sự kết hợp inception và resnet. Detail googleNet architect :<center><img width="600" height="300" src="/assets/images/cnn9-f72ebbec5dad712981c35a80cf7b173b.jpg"></center><center> Hình 10. GoogleNet.</center>GoogleNet gồm 22 layer, khởi đầu vẫn là những simple convolution layer, tiếp theo là những block của inception module với maxpooling theo sau mỗi block. Một số đặc điểm chính. * Sử dụng 9 Inception module trên toàn bộ architect. Làm model deeper hơn rất nhiều. * Không sử dụng fully connection layer mà thay vào đó là average pooling từ 7x7x1024 volume thành 1x1x1024 volume giảm thiểu được rất nhiều parameter. * Ít hơn 12x parameter so với Alexnet. * Auxiliary Loss được add vào total loss(weight =0.3). Nhưng được loại bỏ khi test.<h2 class="anchor anchorWithStickyNavbar_mojV" id="6-resnets2015">6. ResNets(2015).<a class="hash-link" href="#6-resnets2015" title="Direct link to heading">​</a></h2><p>ResNet được phát triển bởi microsoft năm 2015 với paper “ Deep residual learning for image recognition”. ResNet winer ImageNet ILSVRC competition 2015 với error rate 3.57% ,ResNet có cấu trúc gần giống VGG với nhiều stack layer làm cho model deeper hơn. Không giống VGG, resNet  có depth sâu hơn như 34,55,101 và 151 . Resnet giải quyết được vấn đề của deep learning truyền thống , nó có thể dễ dàng training model với hàng trăm layer. Để hiểu ResNet chúng ta cần hiểu vấn đề khi stack nhiều layer khi training, vấn đề đầu tiên khi tăng model deeper hơn gradient sẽ bị vanishing/explodes. Vấn đề này có thể giải quyết bằng cách thêm Batch Normalization nó giúp normalize output giúp các hệ số trở nên cân bằng hơn không quá nhỏ hoặc quá lớn nên sẽ giúp model dễ hội tụ hơn. Vấn đề thứ 2 là  degradation, Khi model deeper accuracy bắt đầu bão hòa(saturated) thậm chí là giảm. Như hình vẽ bên dưới khi stack nhiều layer hơn thì training error lại cao hơn ít layer như vậy vấn đề không phải là do overfitting. Vấn đề này là do model không dễ training khó học hơn, thử tượng tượng một training một shallow model, sau đó chúng ta stack thêm nhiều layer , các layer sau khi thêm vào sẽ không học thêm được gì cả (identity mapping) nên accuracy sẽ tương tự như shallow model mà không tăng. Resnet được ra đời để giải quyết vấn đề degradation này.</p><center><img width="600" height="300" src="/assets/images/cnn11-1abc704b895f11dae66cd54657e4a06f.jpg"></center><center> Hình 11. Compare accuracy with </center>ResNet có architecture gồm nhiều residual block, ý tưởng chính là skip layer bằng cách add connection với layer trước. Ý tưởng của residual block là feed foword x(input) qua một số layer conv-max-conv, ta thu được F(x) sau đó add thêm x vào H(x) = F(x) + x . Model sẽ dễ học hơn khi chúng ta thêm feature từ layer trước vào.<center><img width="600" height="300" src="/assets/images/cnn12-aa3a1b29e26280f747bdee931342a667.jpg"></center><center> Hình 12. ResNets block. </center>* Sử dụng batch Normalization sau mỗi Conv layer. * Initialization Xavier/2 * Training với SGD + momentum(0.9) * Learning rate 0.1, giảm 10 lần nếu error ko giảm * Mini batch size 256 * Weight decay 10^-5 * Không sử dụng dropout<center><img width="600" height="300" src="/assets/images/cnn13-d87a69dc04e947eb0e6f8892f297c2ef.jpg"></center><center> Hình 13. ResNets(2015). </center><h2 class="anchor anchorWithStickyNavbar_mojV" id="7-densenet2016">7. Densenet(2016)<a class="hash-link" href="#7-densenet2016" title="Direct link to heading">​</a></h2><p>Densenet(Dense connected convolutional network) là một trong những netwok mới nhất cho visual object recognition. Nó cũng gần giống Resnet nhưng có một vài điểm khác biệt. Densenet có cấu trúc gồm các dense block và các transition layers. Được stack dense block- transition layers-dense block- transition layers như hình vẽ. Với CNN truyền thống nếu chúng ta có L layer thì sẽ có L connection, còn trong densenet sẽ có L(L+1)/2 connection.</p><center><img width="600" height="300" src="/assets/images/cnn14-90b19af67a72e5b3889e99a34ef2f224.jpg"></center><center> Hình 14. Densenet(2016). </center>Hãy tưởng tượng ban đầu ta có 1 image size (28,28,3). Đầu tiên ta khởi tạo feature layer bằng Conv tạo ra 1 layer size (28,28,24). Sau mỗi layer tiếp theo (Trong dense block ) nó sẽ tạo thêm K= 12 feature giữa nguyên width và height. Khi đó output tiếp theo sẽ là (28,28,24 +12),(28,28,24 +12+12). Ở mỗi dense block sẽ có normalization, nonlinearity và dropout. Để giảm size và depth của feature thì transition layer được đặt giữa các dense block, nó gồm Conv kernel size =1, average pooling (2x2) với stride = 2 nó sẽ giảm output thành (14,14,48)<center><img width="600" height="300" src="/assets/images/cnn16-3ce428a688920ba5e5b38f2cbe5adc9a.jpg"></center>Detail parameter :<center><img width="600" height="300" src="/assets/images/cnn17-3506982af4a117fc86278b47dd6824e4.jpg"></center>Một số ưu điểm của Densenet: * Accuracy : Densenet training tham số ít hơn 1 nửa so với Resnet nhưng có same accuracy so trên ImageNet classification dataset. * Overfitting : DenseNet resistance overfitting rất hiệu quả. * Giảm được vashing gradient. * Sử dụng lại feature hiệu quả hơn. ## Kết bài : Trên đây chỉ là phần tóm lược sơ qua các architect nổi tiếng của CNN. Bên trong đó còn có nhiều cấu trúc phức tạp vì thời gian cũng như kiến thức có hạn nên vẫn chưa viết sâu hết được. Bên cạnh đó còn có nhiều sai sót rất mong bạn đọc góp ý để mình có thể hoàn thiện bài viết hơn.<p>Tham khảo : </p><ul><li>Deep learning course : Andrew Ng</li><li>An Intuitive Guide to Deep Network Architectures : Joyce Xu</li><li>A Simple Guide to the Versions of the Inception Network :Bharath Raj</li><li>CS231n: Convolutional Neural Networks for Visual Recognition</li><li>Paper : All paper about lenet,alexnet,vgg,googlenet,resnet,densenet</li><li>Notes on the Implementation of DenseNet in TensorFlow.</li><li>The Efficiency of Densenet</li></ul></div><footer class="row docusaurus-mt-lg blogPostDetailsFull_h6_j"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/deep-learning">Deep learning</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/blog/tags/cnn">CNN</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/ThorPham/blog/2018-10-8-Quá-trình-phát-triển-của-CNN-từ-LeNet-đến-DenseNet/index.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/Graph-convolution-network-cho-bài-toán-rút-trích-thông-tin"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Hướng tiếp cận Graph convolution network cho bài toán rút trích thông tin từ hóa đơn</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/Object-detection-từ-R-CNN-đến-Faster-R-CNN"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Object detection từ R-CNN đến Faster R-CNN</div></a></div></nav></main><div class="col col--2"><div class="tableOfContents_cNA8 thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#1-lenet1998" class="table-of-contents__link toc-highlight">1. LeNet(1998)</a></li><li><a href="#2-alexnet2012" class="table-of-contents__link toc-highlight">2. Alexnet(2012)</a></li><li><a href="#3-zfnet2013" class="table-of-contents__link toc-highlight">3. ZFNet(2013)</a></li><li><a href="#4-vggnet2014" class="table-of-contents__link toc-highlight">4. VGGNet(2014).</a></li><li><a href="#5-googlenet2014" class="table-of-contents__link toc-highlight">5. GoogleNet(2014).</a></li><li><a href="#6-resnets2015" class="table-of-contents__link toc-highlight">6. ResNets(2015).</a></li><li><a href="#7-densenet2016" class="table-of-contents__link toc-highlight">7. Densenet(2016)</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 Thorpham</div></div></div></footer></div>
<script src="/assets/js/runtime~main.375a46b9.js"></script>
<script src="/assets/js/main.fb378e59.js"></script>
</body>
</html>