"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1477],{10:function(n){n.exports=JSON.parse('{"blogPosts":[{"id":"Graph-convolution-network-cho-b\xe0i-to\xe1n-r\xfat-tr\xedch-th\xf4ng-tin","metadata":{"permalink":"/blog/Graph-convolution-network-cho-b\xe0i-to\xe1n-r\xfat-tr\xedch-th\xf4ng-tin","editUrl":"https://github.com/ThorPham/blog/2021-08-30-Graph-convolution-network-cho-b\xe0i-to\xe1n-r\xfat-tr\xedch-th\xf4ng-tin/index.mdx","source":"@site/blog/2021-08-30-Graph-convolution-network-cho-b\xe0i-to\xe1n-r\xfat-tr\xedch-th\xf4ng-tin/index.mdx","title":"H\u01b0\u1edbng ti\u1ebfp c\u1eadn Graph convolution network cho b\xe0i to\xe1n r\xfat tr\xedch th\xf4ng tin t\u1eeb h\xf3a \u0111\u01a1n","description":"The Mobile capture receipts Optical Character Recognition (MC-OCR) l\xe0 cu\u1ed9c thi v\u1ec1 \u1ea3nh receipt (h\xf3a \u0111\u01a1n) c\xf3 2 task v\xe0 team m\xecnh \u0111\xe3 tham gia task th\u1ee9 2 l\xe0 tr\xedch xu\u1ea5t c\xe1c th\xf4ng tin c\u01a1 b\u1ea3n bao g\u1ed3m SELLER, SELLER_ADDRESS, TIMESTAMP, TOTAL_COST (b\xean b\xe1n, \u0111\u1ecba \u0111i\u1ec3m, th\u1eddi gian v\xe0 t\u1ed5ng thanh to\xe1n) t\u1eeb \xe1nh c\xe1c h\xf3a \u0111\u01a1n \u0111\xe3 \u0111\u01b0\u1ee3c thu th\u1eadp t\u1eeb tr\u01b0\u1edbc b\u1eb1ng \u0111i\u1ec7n tho\u1ea1i.","date":"2021-08-30T00:00:00.000Z","formattedDate":"August 30, 2021","tags":[{"label":"NLP","permalink":"/blog/tags/nlp"},{"label":"CNN","permalink":"/blog/tags/cnn"},{"label":"Computer vision","permalink":"/blog/tags/computer-vision"},{"label":"graph","permalink":"/blog/tags/graph"}],"readingTime":9.01,"truncated":true,"authors":[{"name":"Thorpham","title":"Deep learning enthusiast","url":"https://github.com/ThorPham","imageURL":"https://github.com/ThorPham.png","key":"thorpham"}],"frontMatter":{"slug":"Graph-convolution-network-cho-b\xe0i-to\xe1n-r\xfat-tr\xedch-th\xf4ng-tin","title":"H\u01b0\u1edbng ti\u1ebfp c\u1eadn Graph convolution network cho b\xe0i to\xe1n r\xfat tr\xedch th\xf4ng tin t\u1eeb h\xf3a \u0111\u01a1n","authors":"thorpham","tags":["NLP","CNN","Computer vision","graph"]},"nextItem":{"title":"Qu\xe1 tr\xecnh ph\xe1t tri\u1ec3n c\u1ee7a CNN t\u1eeb LeNet \u0111\u1ebfn DenseNet.","permalink":"/blog/Qu\xe1-tr\xecnh-ph\xe1t-tri\u1ec3n-c\u1ee7a-CNN-t\u1eeb-LeNet-\u0111\u1ebfn-DenseNet"}},"content":"*[The Mobile capture receipts Optical Character Recognition (MC-OCR)](https://rivf2021-mc-ocr.vietnlp.com/) l\xe0 cu\u1ed9c thi v\u1ec1 \u1ea3nh receipt (h\xf3a \u0111\u01a1n) c\xf3 2 task v\xe0 team m\xecnh \u0111\xe3 tham gia task th\u1ee9 2 l\xe0 tr\xedch xu\u1ea5t c\xe1c th\xf4ng tin c\u01a1 b\u1ea3n bao g\u1ed3m **SELLER, SELLER_ADDRESS, TIMESTAMP, TOTAL_COST** (b\xean b\xe1n, \u0111\u1ecba \u0111i\u1ec3m, th\u1eddi gian v\xe0 t\u1ed5ng thanh to\xe1n) t\u1eeb \xe1nh c\xe1c h\xf3a \u0111\u01a1n \u0111\xe3 \u0111\u01b0\u1ee3c thu th\u1eadp t\u1eeb tr\u01b0\u1edbc b\u1eb1ng \u0111i\u1ec7n tho\u1ea1i. *\\n\x3c!--truncate--\x3e\\n## Ti\u1ec1n x\u1eed l\xfd (preprocessing)\\nC\xe1c \u1ea3nh h\xf3a \u0111\u01a1n do BTC cung c\u1ea5p c\xf3 ph\u1ea7n background (ngo\u1ea1i c\u1ea3nh) kh\xf4ng nh\u1ecf (th\u1eadm ch\xed h\u01a1n 50%), b\u1ecb nghi\xeang v\xe0 b\u1ecb quay theo r\u1ea5t nhi\u1ec1u h\u01b0\u1edbng kh\xe1c nhau. Do \u0111\xf3 \u0111\u1ec3 b\u01b0\u1edbc nh\u1eadn d\u1ea1ng text ch\xednh x\xe1c nh\u1ea5t c\u1ea7n lo\u1ea1i b\u1ecf ngo\u1ea1i c\u1ea3nh v\xe0 xoay ph\u1ea7n \u1ea3nh h\xf3a \u0111\u01a1n c\xf2n l\u1ea1i v\u1ec1 \u0111\xfang h\u01b0\u1edbng c\u1ee7a n\xf3.\\n\\n###  Segmentation v\xe0 rotation\\n\u0110\u1ec3 segment reciept ra kh\u1ecfi background b\u1ecdn m\xecnh x\xe0i 1 m\u1ea1ng c\xf3 t\xean l\xe0 **Basnet**  ( Boundary-aware  salient  object  detection) . \u0110\xe2y l\xe0 m\u1ed9t m\u1ea1ng  *salient  object  detection* - hi\u1ec3u \u0111\u01a1n gi\u1ea3n n\xf3 ch\u1ec9 quan t\xe2m  foreground/object v\xe0 background m\xe0 kh\xf4ng c\u1ea7n bi\u1ebft l\xe0 object \u0111\xf3 thu\u1ed9c class n\xe0o ( phi\xean b\u1ea3n n\xe2ng c\u1ea5p h\u01a1n **Basnet** c\u1ee7a c\xf9ng t\xe1c gi\u1ea3 l\xe0 $\\\\mathbf{U}^2$). M\xecnh s\u1eed d\u1ee5ng lu\xf4n pretrained model c\u1ee7a t\xe1c gi\u1ea3 x\xe0i lu\xf4n v\xe0 kh\xf4ng ti\u1ebfn h\xe0nh b\u01b0\u1edbc fine-tune n\xe0o.\\n\\n\\n\\n<div align=\\"center\\">\\n\\n![](https://images.viblo.asia/6a7f6ac0-9f2f-4091-a488-9bf266fa5869.jpg)\\n\\n</div>\\n\\n<div align=\\"center\\">\\n\\nH\xecnh 1 : K\u1ebft qu\u1ea3 t\u1eeb model segmentation\\n\\n</div>\\n\\nSau khi segment b\u1ecdn m\xecnh t\xednh g\xf3c nghi\xeang  gi\u1eefa tr\u1ee5c tr\xean-d\u01b0\u1edbi c\xfaa receipt v\xe0 tr\u1ee5c \u0111\u1ee9ng c\u1ee7a \u1ea3nh sau \u0111\xf3 xoay ph\u1ea7n receipt theo \u0111\xfang h\u01b0\u1edbng c\u1ee7a n\xf3.\\n### Image orientation ( x\xe1c \u0111\u1ecbnh h\u01b0\u1edbng c\u1ee7a receipt)\\nB\u1ecdn m\xecnh x\xe0i m\u1ed9t m\u1ea1ng self-supervised \u0111\u1ec3 x\xe1c \u0111\u1ecbnh h\u01b0\u1edbng c\u1ee7a receipt v\u1edbi \xfd t\u01b0\u1edfng nh\u01b0 trong paper *[Unsupervised Representation Learning by Predicting Image Rotations](https://arxiv.org/abs/1803.07728)*  c\u1ee7a t\xe1c gi\u1ea3 **Spyros Gidaris** . M\u1ed7i receipt c\xf3 th\u1ec3 \u1edf 1 trong 4 h\u01b0\u1edbng b\u1ecb xoay kh\xe1c nhau l\xe0 0, 90, 180, 270 \u0111\u1ed9 nh\u01b0 h\xecnh minh h\u1ecda \u1edf d\u01b0\u1edbi.\\n\\n![center](https://images.viblo.asia/3ef39706-a78b-4217-b8f4-5c34a54fe721.png)\\n\\n<div align=\\"center\\">\\n\\nH\xecnh 2 : Self-supervised cho b\xe0i to\xe1n rotation image\\n\\n</div>\\nB\u1ecdn m\xecnh s\u1eed d\u1ee5ng back-bone l\xe0 ResNet v\u1edbi k\u1ebft qu\u1ea3 tr\xean t\u1eadp test l\xe0 g\u1ea7n 96%.\\n(C\xe1i n\xe0y l\xe0m sau cu\u1ed9c thi \u0111\u1ec3 t\u0103ng time inference, c\xf2n trong cu\u1ed9c thi b\u1ecdn m\xecnh s\u1eed d\u1ee5ng OCR cho t\u1ea5t c\u1ea3 c\xe1c h\u01b0\u1edbng v\xe0 t\xednh s\u1ed1 t\u1eeb nhi\u1ec1u nh\u1ea5t \u0111\u1ec3 x\xe1c \u0111\u1ecbnh h\u01b0\u1edbng)\\n\\n### Text detection v\xe0 recognition\\nC\u0169ng gi\u1ed1ng nh\u01b0 c\xe1c team kh\xe1c team m\xecnh x\xe0i CRAFT cho text detection v\xe0 VietOCR cho text recognition. \u0110\xe3 c\xf3 r\u1ea5t nhi\u1ec1u b\xe0i vi\u1ebft v\u1ec1 c\xe1i n\xe0y m\xecnh s\u1ebd kh\xf4ng \u0111i s\xe2u v\xe0o n\xf3 n\u1eefa\\n\\n## Graph convolution network\\n\u0110\u1ec3 gi\u1ea3i quy\u1ebft b\xe0i to\xe1n key information extraction (tr\xedch xu\u1ea5t th\xf4ng tin c\u01a1 b\u1ea3n) c\xf3 r\u1ea5t nhi\u1ec1u h\u01b0\u1edbng ti\u1ebfp c\u1eadn nh\u01b0 text classification hay template matching nh\u01b0ng m\xecnh th\u1ea5y h\u01b0\u1edbng ti\u1ebfp c\u1eadn Graph l\xe0 hay nh\u1ea5t. M\u1ed7i receipt \u0111\u01b0\u1ee3c m\xf4 h\xecnh h\xf3a d\u01b0\u1edbi d\u1ea1ng graph $G(V, E)$ trong \u0111\xf3 $V$ (vertices/nodes) l\xe0 t\u1eadp c\xe1c \u0111\u1ec9nh t\u01b0\u01a1ng \u1ee9ng v\u1edbi bounding box m\u1ed7i v\xf9ng c\xf3 text (textbox/text bounding box) v\xe0 $E$ l\xe0 t\u1eadp c\xe1c c\u1ea1nh bi\u1ec3u di\u1ec5n cho m\u1ed1i quan h\u1ec7 gi\u1eefa c\xe1c \u0111\u1ec9nh. B\xe0i to\xe1n n\xe0y thu\u1ed9c l\u1edbp Node classification c\xf3 r\u1ea5t nhi\u1ec1u \xfd t\u01b0\u1edfng \u0111\u01b0\u1ee3c \u0111\u1ec1 xu\u1ea5t ra nh\u01b0 PICK (processing  keyinformation extraction from documents using improved graph learning-convolutional networks ) k\u1ebft h\u1ee3p gi\u1eefa vision feature v\xe0 text feature , \u0111i\u1ec3m m\xecnh kh\xf4ng th\xedch \u1edf b\xe0i n\xe0y l\xe0 s\u1ef1 k\u1ebft h\u1ee3p qu\xe1 c\u1ee9ng nh\u1eafc c\u1ee7a 2 feature n\xe0y. Team m\xecnh th\xedch c\xe1c ti\u1ebfp c\u1eadn d\u1ef1a tr\xean text feature v\xe0 v\u1ecb tr\xed box h\u01a1n n\xean s\u1eed d\u1ee5ng paper  *[Residual Gated Graph Convnets](https://arxiv.org/abs/1711.07553)* c\u1ee7a t\xe1c gi\u1ea3 *Xavier Bresson*, m\u1ed9t nh\xe2n v\u1eadt r\u1ea5t n\u1ed5i ti\u1ebfng v\u1edbi nhi\u1ec1u paper v\u1ec1 graph. *Xavier Bresson* c\u0169ng t\u1ea1o ra m\u1ed9t Benchmarking Graph Neural Networks v\u1edbi r\u1ea5t nhi\u1ec1u model \u0111\u01b0\u1ee3c vi\u1ebft tr\xean th\u01b0 vi\u1ec7n DGL .\\n\\n![center](https://images.viblo.asia/ae0f1811-0cdf-428c-bbed-8edc1468c455.png)\\n\\n<div align=\\"center\\">\\n\\nH\xecnh 3 : Graph  architecture cho b\xe0i to\xe1n node classification\\n\\n</div>\\n\\n### \u0110\u1ecbnh ngh\u0129a v\u1ec1 node feature v\xe0 edge feature.\\n**1.  Node features**\\n\\nNode featue \u0111\u01b0\u1ee3c t\u1ed5ng h\u1ee3p t\u1eeb t\u1ecda \u0111\u1ed9 textbox v\xe0 text do model c\u1ee7a OCR nh\u1eadn di\u1ec7n ra. M\u1ed7i textbox l\xe0 m\u1ed9t vector $L=(x_i,~y_i|~i \\\\in [1,4])$ trong \u0111\xf3 ($x_i,~y_i$ t\u1ecda \u0111\u1ed9 g\xf3c c\u1ee7a textbox). Text t\u1eeb OCR s\u1ebd \u0111\u01b0\u1ee3c embedding v\xe0 v\xe0o \u0111\u01b0a v\xe0o m\u1ed9t m\u1ea1ng LSTM. Sau \u0111\xf3 th\xf4ng tin v\u1ec1 t\u1ecda \u0111\u1ed9 textbox v\xe0 text \u0111\u01b0\u1ee3c k\u1ebft h\u1ee3p b\u1eb1ng c\xe1ch c\u1ed9ng theo t\u1eebng ph\u1ea7n t\u1eed (element-wise) v\u1edbi nhau t\u1ea1o th\xe0nh *node feature*. M\xecnh s\u1eed d\u1ee5ng embedding theo character m\xe0 kh\xf4ng d\xf9ng nh\u1eefng pretrained model nh\u01b0 *word2vec* hay *bert* v\xec c\xe1c l\xed do sau: a) h\xf3a \u0111\u01a1n c\xf3 c\u1ea3 ti\u1ebfng Vi\u1ec7t l\u1eabn ti\u1ebfng Anh l\u1eabn s\u1ed1, b) nhi\u1ec1u t\u1eeb ti\u1ebfng Vi\u1ec7t b\u1ecb nh\u1eadn di\u1ec7n sai d\u1ea5u/thanh v\xe0 cu\u1ed1i c\xf9ng  l\xe0 c) kh\xf4ng c\xf3 nhi\u1ec1u th\xf4ng tin v\u1ec1 ng\u1eef c\u1ea3nh.\\n\\n**2.  Edge  features**\\n\\nEdge (c\u1ea1nh) bi\u1ec3u di\u1ec5n s\u1ef1 li\xean k\u1ebft gi\u1eefa m\u1ed7i c\u1eb7p node trong graph. Tr\u01b0\u1edbc ti\xean ch\xfang ta \u0111\u1ecbnh ngh\u0129a li\xean k\u1ebft gi\u1eefa hai nodes b\u1ea5t k\xec. Gi\u1ea3 \u0111\u1ecbnh r\u1eb1ng text trong reciept \u0111\u01b0\u1ee3c s\u1eafp x\u1ebfp theo th\u1ee9 t\u1ef1 tr\xe1i-ph\u1ea3i tr\xean-d\u01b0\u1edbi, hai nodes \u0111\u01b0\u1ee3c g\u1ecdi l\xe0 c\xf3 li\xean k\u1ebft n\u1ebfu:\\n$$\\nd(v,~v_j) = abs(v_y - v_{j, y})  < 3\\\\times h_v\\n$$\\ntrong \u0111\xf3 $h$ l\xe0 chi\u1ec1u cao c\u1ee7a node hi\u1ec7n t\u1ea1i. N\xf3i m\u1ed9t c\xe1ch \u0111\u01a1n gi\u1ea3n, hai node \u0111\u01b0\u1ee3c coi l\xe0 c\xf3 li\xean k\u1ebft v\u1edbi nhau khi kho\u1ea3ng c\xe1ch theo tr\u1ee5c $y$ gi\u1eefa ch\xfang kh\xf4ng v\u01b0\u1ee3t qu\xe1 3 l\u1ea7n chi\u1ec1u cao c\u1ee7a node hi\u1ec7n t\u1ea1i.\\nTa \u0111\u1ecbnh ngh\u0129a *edge feature* c\u1ee7a 2 node c\xf3 li\xean k\u1ebft l\xe0 m\u1ed9t vector kho\u1ea3ng c\xe1ch theo tr\u1ee5c $x$  v\xe0 $y$ cho b\u1edfi c\xf4ng th\u1ee9c :\\n$$\\n distance(v_i,~v_j) = (abs(v_{i, x} - v_{j, x}), abs(v_{i, y} - v_{j, y}))\\n $$\\n\\n**3.  Network architecture (Ki\u1ebfn tr\xfac Graph model)**\\nM\xecnh s\u1eed d\u1ee5ng graph model c\xf3 t\xean l\xe0 *[Residual Gated Graph Convnets](https://arxiv.org/abs/1711.07553)* . Edge v\xe0 node features theo c\xe1c \u0111\u1ecbnh ngh\u0129a \u1edf tr\xean \u0111\u01b0\u1ee3c \u0111\u01b0a qua layer *RG-GCN*  (Residual Gated Graph Convnets).\\n\\n$$\\n\\\\mathbf{h}=\\\\mathbf{x}+\\\\left(\\\\mathbf{Ax}+\\\\sum_{v_j\\\\to v} \\\\eta(e_j) \\\\odot \\\\mathbf{Bx}_j \\\\right)^+,\\n$$\\n\\ntrong \u0111\xf3 $\\\\mathbf{x}$ l\xe0 Residual (hay skip connection nh\u01b0 trong Resnet), $\\\\mathbf{Ax}$ l\xe0  t\xe1c \u0111\u1ed9ng c\u1ee7a node hi\u1ec7n t\u1ea1i $\\\\sum ...$ l\xe0 t\xe1c \u0111\u1ed9ng c\u1ee7a c\xe1c node l\xe2n c\u1eadn, $^+$ l\xe0 h\xe0m ReLu). $\\\\eta$ l\xe0 t\u1ec9 tr\u1ecdng c\u1ee7a m\u1ed7i node l\xe2n c\u1eadn t\xe1c \u0111\u1ed9ng \u0111\u1ebfn node hi\u1ec7n t\u1ea1i v\xe0 \u0111\u01b0\u1ee3c t\xednh theo c\xf4ng th\u1ee9c:\\n\\n$$\\n\\t\\\\eta(e_j) = \\\\sigma(e_j) \\\\left(\\\\sum_{v_k\\\\to v}\\\\sigma(e_k)\\\\right)^{-1},\\n$$\\n\\n$\\\\sigma$ l\xe0 h\xe0m sigmod, $e_j$ v\xe0 $e_k$  l\xe0 features c\u1ee7a c\xe1c edges li\xean k\u1ebft v\u1edbi node hi\u1ec7n t\u1ea1i $v$ t\u1eeb c\xe1c nodes l\xe2n c\u1eadn $v_j$ v\xe0 $v_k$. $e_j$ theo th\u1ee9 t\u1ef1 \u0111\u01b0\u1ee3c t\xednh t\u1eeb c\xe1c c\xf4ng th\u1ee9c sau:\\n\\n$$\\ne_j = \\\\mathbf{C}e_j^{x} +\\\\mathbf{Dx}_j + \\\\mathbf{Ex}, \\n$$\\n$$\\ne_j^h = e_j^x + (e_j)^+,\\n$$\\n\\nv\u1edbi $e_j^x$ v\xe0 $e_j^h$ l\xe0 input v\xe0 outout c\u1ee7a hidden layer t\u1eeb feature vector c\u1ee7a c\u1ea1nh $e_j$ n\u1ed1i \u0111\u1ec9nh hi\u1ec7n t\u1ea1i $v$ v\u1edbi \u0111\u1ec9nh $v_j$. $\\\\mathbf{A,B,C,D,E}$ l\xe0 c\xe1c ma tr\u1eadn c\u1ee7a c\xe1c ph\xe9p quay \u0111\u01b0\u1ee3c h\u1ecdc t\u1eeb qu\xe1 tr\xecnh hu\u1ea5n luy\u1ec7n m\u1ea1ng.\\n\\nModel graph c\xf3 th\u1ec3 \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a theo class nh\u01b0 sau\\n\\n```python\\nclass GatedGCN_layer(nn.Module):\\n\\n    def __init__(self, input_dim, output_dim):\\n        super().__init__()\\n        self.A = nn.Linear(input_dim, output_dim)\\n        self.B = nn.Linear(input_dim, output_dim)\\n        self.C = nn.Linear(input_dim, output_dim)\\n        self.D = nn.Linear(input_dim, output_dim)\\n        self.E = nn.Linear(input_dim, output_dim)\\n        self.bn_node_h = nn.BatchNorm1d(output_dim)\\n        self.bn_node_e = nn.BatchNorm1d(output_dim)\\n\\n    def message_func(self, edges):\\n        Bh_j = edges.src[\'Bh\']\\n        # e_ij = Ce_ij + Dhi + Ehj\\n        e_ij = edges.data[\'Ce\'] + edges.src[\'Dh\'] + edges.dst[\'Eh\']\\n        edges.data[\'e\'] = e_ij\\n        return {\'Bh_j\' : Bh_j, \'e_ij\' : e_ij}\\n\\n     def reduce_func(self, nodes):\\n        Ah_i = nodes.data[\'Ah\']\\n        Bh_j = nodes.mailbox[\'Bh_j\']\\n        e = nodes.mailbox[\'e_ij\']\\n        # sigma_ij = sigmoid(e_ij)\\n        sigma_ij = torch.sigmoid(e)\\n        # hi = Ahi + sum_j eta_ij * Bhj\\n        h = Ah_i + torch.sum(sigma_ij * Bh_j, dim=1) / torch.sum(sigma_ij, dim=1)\\n        return {\'h\' : h}\\n\\n     def forward(self, g, h, e, snorm_n, snorm_e):\\n\\n        h_in = h # residual connection\\n        e_in = e # residual connection\\n\\n        g.ndata[\'h\']  = h\\n        g.ndata[\'Ah\'] = self.A(h)\\n        g.ndata[\'Bh\'] = self.B(h)\\n        g.ndata[\'Dh\'] = self.D(h)\\n        g.ndata[\'Eh\'] = self.E(h)\\n        g.edata[\'e\']  = e\\n        g.edata[\'Ce\'] = self.C(e)\\n\\n        g.update_all(self.message_func, self.reduce_func)\\n\\n        h = g.ndata[\'h\'] # result of graph convolution\\n        e = g.edata[\'e\'] # result of graph convolution\\n\\n        h = h * snorm_n # normalize activation w.r.t. graph node size\\n        e = e * snorm_e # normalize activation w.r.t. graph edge size\\n\\n        h = self.bn_node_h(h) # batch normalization\\n        e = self.bn_node_e(e) # batch normalization\\n\\n        h = torch.relu(h) # non-linear activation\\n        e = torch.relu(e) # non-linear activation\\n\\n        h = h_in + h # residual connection\\n        e = e_in + e # residual connection\\n\\n        return h, e\\n```\\n\\nSau khi stack (L=8) layers c\u1ee7a RG-GCN c\xe1c node-feature \u0111\u01b0\u1ee3c \u0111\u01b0a v\xe0o m\u1ed9t layer dense v\xe0 d\xf9ng chung weight cho t\u1ea5t c\u1ea3 c\xe1c node v\xe0 lay\u1ec3 cu\u1ed1i c\xf9ng s\u1eed d\u1ee5ng h\xe0m l\u1ed7i d\u1ea1ng cross entropy \u0111\u1ec3  ph\xe2n lo\u1ea1i node.\\n\\n## Chu\u1ea9n b\u1ecb dataset v\xe0 training\\n### Pseudo label - th\xeam nh\xe3n gi\u1ea3\\n\u0110\u1ec3 t\u1ea1o data training cho graph model, m\xecnh th\xeam ground truth (g\u1ed3m 4 \u0111\u1ec9nh polygon c\u1ee7a text, text v\xe0 class label) v\xe0o dataset \u0111\xe3 t\u1ea1o \u1edf b\u01b0\u1edbc tr\u01b0\u1edbc \u0111\xf3 (Text detection) v\xe0 lo\u1ea1i b\u1ecf nh\u1eefng textbox c\u1ee7a n\u1ebfu n\xf3 tr\xf9ng l\u1eb7p v\u1edbi textbox c\u1ee7a BTC b\u1edfi IoU > 0.2. Nh\u1eefng box c\xf2n l\u1ea1i s\u1ebd \u0111\u01b0\u1ee3c g\xe1n nh\xe3n l\xe0 Other.\\n\\n### Data Augmentation\\n\u0110\u1ec3 t\u0103ng \u0111\u1ed9 \u0111a d\u1ea1ng cho dataset m\xecnh l\xe0m gi\xe0u th\xeam b\u1eb1ng c\xe1ch thay th\u1ebf c\xe1c field SELLER v\xe0 ADDRESS d\u1ef1a tr\xean b\u1ed9 t\u1eeb \u0111i\u1ec3n \u0111\u01b0\u1ee3c t\u1ea1o ra t\u1eeb ground truth c\u1ee7a BTC v\xe0 l\u1ea5y ngh\u1eabu nhi\xean cho TIMESTAMP v\xe0 TOTAL. C\u1ea3 hai text detector l\xe0 CTPN v\xe0 CRAFT c\u0169ng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 l\xe0m gi\xe0u d\u1eef li\u1ec7u.\\n\\n###  Training v\xe0 accuracy\\nDataset sau khi l\xe0m gi\xe0u \u0111\u1ebfn kho\u1ea3ng 10k m\u1eabu v\xe0 \u0111\u01b0\u1ee3c chia theo t\u1ec9 l\u1ec7 80:20 cho train v\xe0 test. Qu\xe1 tr\xecnh tr\xean *GTX 1080TI* v\u1edbi 10 epochs cho \u0111\u1ed3 th\u1ecb nh\u01b0 sau:\\n<div align=\\"center\\">\\n\\n![](https://images.viblo.asia/2e190f53-08ba-4fa0-8e68-a8c20706628e.png)\\n\\n</div>\\n\\nK\u1ebft qu\u1ea3 accuracy tr\xean t\u1eebng field nh\u01b0 sau :\\n<div align=\\"center\\">\\n\\n![](https://images.viblo.asia/71f6eabb-4b39-459f-b664-298c4e9a69aa.png)\\n\\n</div>\\n\\n## Post processing\\n**1. Spelling correction**\\n\\nM\xecnh s\u1eed d\u1ee5ng grounth truth \u0111\u1ec3 s\u1eeda l\u1ea1i text trong tr\u01b0\u1eddng h\u1ee3p b\u1ecb nh\u1eadn di\u1ec7n sai. V\xed d\u1ee5 v\u1edbi \u0111\u1ecba ch\u1ec9 v\xe0 t\xean c\xf4ng ty, shop, market th\u01b0\u1eddng l\xe0 t\xean ri\xeang n\xean m\xecnh t\u1ea1o m\u1ed9t dictionary theo c\u1eb7p v\u1edbi key l\xe0 gi\xe1 tr\u1ecb text nh\u1eadn di\u1ec7n \u0111\u01b0\u1ee3c v\xe0 value l\xe0 ground truth c\u1ee7a BTC. Khi inference n\u1ebfu company/address tr\xf9ng v\u1edbi key trong dictionary th\xec companty/address \u0111\xf3 \u0111\u01b0\u1ee3c thay th\u1ebf b\u1eb1ng value trong dictionary.\\n\\n**2. Regular expression**\\n\\nM\u1ed9t s\u1ed1 tr\u01b0\u1eddng h\u1ee3p ng\xe0y th\xe1ng/timestamp b\u1ecb sai do l\xfac l\xe0m pseudo label th\xec gi\xe1 tr\u1ecb gi\u1eefa grouth truth v\xe0 OCR kh\xf4ng kh\u1edbp nhau. Trong tr\u01b0\u1eddng h\u1ee3p n\xe0y khi inference m\u1ed9t s\u1ed1 tr\u01b0\u1eddng h\u1ee3p v\u1ec1 date b\u1ecb b\u1ecf s\xf3t, regular expression \u0111\u01b0\u1ee3c d\xf9ng trong tr\u01b0\u1eddng h\u1ee3p n\xe0y \u0111\u1ec3 tr\xedch xu\u1ea5t \u0111\xfang ph\u1ea7n datetime/timestamp b\u1ed5 tr\u1ee3 cho model graph.\\n\\n### K\u1ebft qu\u1ea3\\n<div align=\\"center\\">\\n\\n![](https://images.viblo.asia/143aab75-eea8-477b-8c66-4e1aab0bdf8a.png)\\n\\n</div>\\n\\n\\nReferences :\\n* An invoice reading system using agraph convolutional network\\n* Information extractionfrom  text  intensive  and  visually  rich  banking  documents\\n* Learning graph nor-malization for graph neural networks\\n* Unsupervised Representation Learning by Predicting Image Rotations\\n* Residual Gated Graph ConvNets\\n* https://atcold.github.io/pytorch-Deep-Learning/\\n* https://github.com/graphdeeplearning/benchmarking-gnns\\n* https://github.com/pbcquoc/vietocr\\n* https://github.com/clovaai/CRAFT-pytorch"},{"id":"Qu\xe1-tr\xecnh-ph\xe1t-tri\u1ec3n-c\u1ee7a-CNN-t\u1eeb-LeNet-\u0111\u1ebfn-DenseNet","metadata":{"permalink":"/blog/Qu\xe1-tr\xecnh-ph\xe1t-tri\u1ec3n-c\u1ee7a-CNN-t\u1eeb-LeNet-\u0111\u1ebfn-DenseNet","editUrl":"https://github.com/ThorPham/blog/2018-10-8-Qu\xe1-tr\xecnh-ph\xe1t-tri\u1ec3n-c\u1ee7a-CNN-t\u1eeb-LeNet-\u0111\u1ebfn-DenseNet/index.md","source":"@site/blog/2018-10-8-Qu\xe1-tr\xecnh-ph\xe1t-tri\u1ec3n-c\u1ee7a-CNN-t\u1eeb-LeNet-\u0111\u1ebfn-DenseNet/index.md","title":"Qu\xe1 tr\xecnh ph\xe1t tri\u1ec3n c\u1ee7a CNN t\u1eeb LeNet \u0111\u1ebfn DenseNet.","description":"Convolutional neural network l\xe0 m\u1ed9t m\u1ea1ng neural \u0111\u01b0\u1ee3c \u1ee9ng d\u1ee5ng r\u1ea5t nhi\u1ec1u trong deep learning trong computer vision cho classifier v\xe0 localizer . T\u1eeb m\u1ea1ng CNN c\u01a1 b\u1ea3n ng\u01b0\u1eddi ta c\xf3 th\u1ec3 t\u1ea1o ra r\u1ea5t nhi\u1ec1u architect kh\xe1c nhau, t\u1eeb nh\u1eefng m\u1ea1ng neural c\u01a1 b\u1ea3n 1 \u0111\u1ebfn 2 layer \u0111\u1ebfn 100 layer. \u0110\xe3 bao gi\u1edd b\u1ea1n t\u1ef1 h\u1ecfi n\xean s\u1eed d\u1ee5ng bao nhi\xeau layer, n\xean k\u1ebft h\u1ee3p conv v\u1edbi maxpooling th\u1ebf n\xe0o? conv-maxpooling hay conv-conv-maxplooling ? hay n\xean s\u1eed d\u1ee5ng kernel 3x3 hay 5x5 th\u1eadm ch\xed 7x7 \u0111i\u1ec3m kh\xe1c bi\u1ec7t l\xe0 g\xec ? L\xe0m g\xec khi model b\u1ecb vanishing/exploding gradient, hay t\u1ea1i sao thi th\xeam nhi\u1ec1u layer h\u01a1n th\xec theo l\xfd thuy\u1ebft accuarcy ph\u1ea3i cao h\u01a1n so v\u1edbi shallow model, nh\u01b0ng th\u1ef1c t\u1ebf l\u1ea1i kh\xf4ng ph\u1ea3i accuarcy kh\xf4ng t\u0103ng th\u1eadm ch\xed l\xe0 gi\u1ea3m \u0111\xf3 c\xf3 ph\u1ea3i nguy\xean nh\xe2n do overfitting .Trong b\xe0i vi\u1ebft n\xe0y ta s\u1ebd t\xecm hi\u1ec3u c\xe1c architure n\u1ed5i ti\u1ebfng \u0111\u1ec3 xem c\u1ea5u tr\xfac c\u1ee7a n\xf3 nh\u01b0 th\u1ebf n\xe0o, c\xe1c \xfd t\u01b0\u1edfng v\u1ec1 CNN m\u1edbi nh\u1ea5t hi\u1ec7n nay  t\u1eeb \u0111\xf3 ta c\xf3 th\u1ec3 tr\u1ea3 l\u1eddi \u0111\u01b0\u1ee3c m\u1ea5y c\xe2u h\u1ecfi tr\xean","date":"2018-10-08T00:00:00.000Z","formattedDate":"October 8, 2018","tags":[{"label":"Deep learning","permalink":"/blog/tags/deep-learning"},{"label":"CNN","permalink":"/blog/tags/cnn"}],"readingTime":15.165,"truncated":true,"authors":[{"name":"Thorpham","title":"Deep learning enthusiast","url":"https://github.com/ThorPham","imageURL":"https://github.com/ThorPham.png","key":"thorpham"}],"frontMatter":{"slug":"Qu\xe1-tr\xecnh-ph\xe1t-tri\u1ec3n-c\u1ee7a-CNN-t\u1eeb-LeNet-\u0111\u1ebfn-DenseNet","title":"Qu\xe1 tr\xecnh ph\xe1t tri\u1ec3n c\u1ee7a CNN t\u1eeb LeNet \u0111\u1ebfn DenseNet.","authors":"thorpham","tags":["Deep learning","CNN"]},"prevItem":{"title":"H\u01b0\u1edbng ti\u1ebfp c\u1eadn Graph convolution network cho b\xe0i to\xe1n r\xfat tr\xedch th\xf4ng tin t\u1eeb h\xf3a \u0111\u01a1n","permalink":"/blog/Graph-convolution-network-cho-b\xe0i-to\xe1n-r\xfat-tr\xedch-th\xf4ng-tin"},"nextItem":{"title":"Object detection t\u1eeb R-CNN \u0111\u1ebfn Faster R-CNN","permalink":"/blog/Object-detection-t\u1eeb-R-CNN-\u0111\u1ebfn-Faster-R-CNN"}},"content":"*Convolutional neural network l\xe0 m\u1ed9t m\u1ea1ng neural \u0111\u01b0\u1ee3c \u1ee9ng d\u1ee5ng r\u1ea5t nhi\u1ec1u trong deep learning trong computer vision cho classifier v\xe0 localizer . T\u1eeb m\u1ea1ng CNN c\u01a1 b\u1ea3n ng\u01b0\u1eddi ta c\xf3 th\u1ec3 t\u1ea1o ra r\u1ea5t nhi\u1ec1u architect kh\xe1c nhau, t\u1eeb nh\u1eefng m\u1ea1ng neural c\u01a1 b\u1ea3n 1 \u0111\u1ebfn 2 layer \u0111\u1ebfn 100 layer. \u0110\xe3 bao gi\u1edd b\u1ea1n t\u1ef1 h\u1ecfi n\xean s\u1eed d\u1ee5ng bao nhi\xeau layer, n\xean k\u1ebft h\u1ee3p conv v\u1edbi maxpooling th\u1ebf n\xe0o? conv-maxpooling hay conv-conv-maxplooling ? hay n\xean s\u1eed d\u1ee5ng kernel 3x3 hay 5x5 th\u1eadm ch\xed 7x7 \u0111i\u1ec3m kh\xe1c bi\u1ec7t l\xe0 g\xec ? L\xe0m g\xec khi model b\u1ecb vanishing/exploding gradient, hay t\u1ea1i sao thi th\xeam nhi\u1ec1u layer h\u01a1n th\xec theo l\xfd thuy\u1ebft accuarcy ph\u1ea3i cao h\u01a1n so v\u1edbi shallow model, nh\u01b0ng th\u1ef1c t\u1ebf l\u1ea1i kh\xf4ng ph\u1ea3i accuarcy kh\xf4ng t\u0103ng th\u1eadm ch\xed l\xe0 gi\u1ea3m \u0111\xf3 c\xf3 ph\u1ea3i nguy\xean nh\xe2n do overfitting .Trong b\xe0i vi\u1ebft n\xe0y ta s\u1ebd t\xecm hi\u1ec3u c\xe1c architure n\u1ed5i ti\u1ebfng \u0111\u1ec3 xem c\u1ea5u tr\xfac c\u1ee7a n\xf3 nh\u01b0 th\u1ebf n\xe0o, c\xe1c \xfd t\u01b0\u1edfng v\u1ec1 CNN m\u1edbi nh\u1ea5t hi\u1ec7n nay  t\u1eeb \u0111\xf3 ta c\xf3 th\u1ec3 tr\u1ea3 l\u1eddi \u0111\u01b0\u1ee3c m\u1ea5y c\xe2u h\u1ecfi tr\xean*\\n\x3c!--truncate--\x3e\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./36774671_240413323222236_1459661677975830528_n.png\').default} />\\n</center>\\n\\n<center>\\nH\xecnh 1. Qu\xe1 tr\xecnh ph\xe1t tri\u1ec3n c\u1ee7a CNN\\n</center>\\n\\n## 1. LeNet(1998)\\nLeNet l\xe0 m\u1ed9t trong nh\u1eefng m\u1ea1ng CNN l\xe2u \u0111\u1eddi n\u1ed5i ti\u1ebfng nh\u1ea5t \u0111\u01b0\u1ee3c Yann LeCUn ph\xe1t tri\u1ec3n v\xe0o nh\u1eefng n\u0103m 1998s. C\u1ea5u tr\xfac c\u1ee7a LeNet g\u1ed3m 2 layer (Convolution + maxpooling) v\xe0 2 layer fully  connected  layer v\xe0 output l\xe0 softmax layer .  \\n\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./36833992_240414163222152_4178930615535534080_n.png\').default} />\\n</center>\\n\\n<center>\\nH\xecnh 2. LeNet (Source CNN c\u1ee7a Andrew Ng)\\n</center>\\n\\nCh\xfang ta c\xf9ng t\xecm hi\u1ec3u chi ti\u1ebft architect c\u1ee7a LeNet \u0111\u1ed1i v\u1edbi d\u1eef li\u1ec7u mnist (accuracy l\xean \u0111\u1ebfn 99%) :\\n\\n* Input shape 28x28x3\\n* Layer 1 :\\n  * Convolution layer 1 : Kernel 5x5x3 , stride = 1,no padding, number filter = 6 ,output = 28x28x6.\\n  * Maxpooling layer : pooling size 2x2,stride = 2,padding = \u201csame\u201d,output = 14x14x6.\\n* Layer 2 :\\n  * Convolution layer 2 : kernel 5x5x6,stride = 1, no padding, number filter = 16,output = 10x10x16.\\n  * Maxpooling layer : pooling size = 2x2, stride = 2, padding =\u201dsame\u201d,output = 5x5x16.\\n* Flatten output = 5x5x16 = 400\\n* Fully connection 1 : output = 120\\n* Fully connection 2 : output = 84\\n* Softmax layer, output = 10 (10 digits).\\n \\nNh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a LeNet l\xe0 m\u1ea1ng c\xf2n r\u1ea5t \u0111\u01a1n gi\u1ea3n v\xe0 s\u1eed d\u1ee5ng sigmoid (or tanh) \u1edf m\u1ed7i convolution layer m\u1ea1ng t\xednh to\xe1n r\u1ea5t ch\u1eadm.\\n\\n## 2. Alexnet(2012)\\nAlexNet l\xe0 m\u1ed9t m\u1ea1ng CNN \u0111\xe3 d\xe0nh chi\u1ebfn th\u1eafng trong cu\u1ed9c thi ImageNet LSVRC-2012 n\u0103m 2012 v\u1edbi large margin (15.3% VS 26.2% error rates). AlexNet l\xe0 m\u1ed9t m\u1ea1ng CNN traning v\u1edbi m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng parameter r\u1ea5t l\u1edbn (60 million) so v\u1edbi LeNet. M\u1ed9t s\u1ed1 \u0111\u1eb7c \u0111i\u1ec3m:\\n\\n* S\u1eed d\u1ee5ng relu thay cho sigmoid(or tanh) \u0111\u1ec3 x\u1eed l\xfd v\u1edbi non-linearity. T\u0103ng t\u1ed1c \u0111\u1ed9 t\xednh to\xe1n l\xean 6 l\u1ea7n.\\n* S\u1eed d\u1ee5ng dropout nh\u01b0 m\u1ed9t ph\u01b0\u01a1ng ph\xe1p regularization m\u1edbi cho CNN. Dropout kh\xf4ng nh\u1eefng gi\xfap m\xf4 h\xecnh tr\xe1nh \u0111\u01b0\u1ee3c overfitting m\xe0 c\xf2n l\xe0m gi\u1ea3m th\u1eddi gian hu\u1ea5n luy\u1ec7n m\xf4 h\xecnh \\n* Overlap pooling \u0111\u1ec3 gi\u1ea3m size c\u1ee7a network ( Traditionally pooling regions kh\xf4ng overlap).\\n* S\u1eed d\u1ee5ng local response normalization \u0111\u1ec3 chu\u1ea9n h\xf3a \u1edf m\u1ed7i layer.\\n* S\u1eed d\u1ee5ng k\u1ef9 thu\u1eadt data augmentation \u0111\u1ec3 t\u1ea1o th\xeam data training b\u1eb1ng c\xe1ch translations, horizontal reflections.\\n* Alexnet training v\u1edbi 90 epochs trong 5 \u0111\u1ebfn 6 ng\xe0y v\u1edbi 2 GTX 580 GPUs. S\u1eed d\u1ee5ng SGD v\u1edbi learning rate 0.01, momentum 0.9 v\xe0 weight decay 0.0005. \\n\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./36767372_240415716555330_8179137527236526080_n.png\').default} />\\n</center>\\n\\n<center>\\nH\xecnh 3. AlexNet (Ngu\u1ed3n ImageNet Classification with Deep Convolutional Neural Networks)\\n</center>\\n\\nArchitect c\u1ee7a Alexnet g\u1ed3m 5 convolutional layer v\xe0 3 fully  connected  layer. Activation Relu \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng sau m\u1ed7i convolution v\xe0 fully connection layer. Detail architecture v\u1edbi dataset l\xe0 imagenet size l\xe0 227x227x3 v\u1edbi 1000 class ( kh\xe1c v\u1edbi trong h\xecnh tr\xean size l\xe0 224x224). \\n\\nDetail Architect:\\n* Input shape 227x227x3.\\n* Layer 1 :\\n  * Conv 1 : kernel : 11x11x3,stride = 4,no padding, number = 96,activation = relu,output = 55x55x96.\\n  * Maxpooling layer : pooling size = 3x3,stride = 2,padding =\u201dsame\u201d ,output = 27x27x96.\\n  * Normalize layer.\\n* Layer 2 :\\n  * Conv 2 : kernel :3x3x96,stride = 1, padding = \u201csame\u201d, number filter = 256,activation = relu,output = 27x27x256.\\n  * Maxpooling layer : pooling size = 3x3,stride=2, padding =\u201dsame\u201d,output = 13x13x256.\\n  * Normalize layer.\\n* Layer 3:\\n  * Conv 3 : kernel :3x3x256, stride = 1,padding=\u201dsame\u201d, number filter = 384, activation = relu, output = 13x13x384.\\n* Layer 4:\\n  * Conv 4 : kernel : 3x3x384 , stride = 1, padding = \u201csame\u201d, number filter = 384, activation= relu, output = 13x13x384\\n* Layer 5 :\\n  * Conv 5 : kernel 3x3x384, stride = 1, padding = \u201csame\u201d, number filter = 256, activation = relu, output = 13x13x256.\\n  * Pooling layer : pooling size = 3x3,stride =2,padding =\u201dsame\u201d,output = 6x6x256.\\n* Flatten 256x6x6 = 9216\\n* Fully connection layer 1 : activation = relu , output = 4096 + dropout(0.5).\\n* Fully connection layer 2 : activation = relu , output = 4096 + dropout(0.5).\\n* Fully connection layer 3 : activation = softmax , output = 1000 (number class) \\n\\n## 3. ZFNet(2013)\\nZFNet l\xe0 m\u1ed9t m\u1ea1ng cnn th\u1eafng trong ILSVRC 2013 v\u1edbi top-5 error rate c\u1ee7a 14.8% . ZFNet c\xf3 c\u1ea5u tr\xfac r\u1ea5t gi\u1ed1ng v\u1edbi AlexNet v\u1edbi 5 layer convolution , 2 fully connected layer v\xe0 1 output softmax layer. Kh\xe1c bi\u1ec7t \u1edf ch\u1ed7 kernel size \u1edf m\u1ed7i Conv layer .M\u1ed9t s\u1ed1 \u0111\u1eb7c \u0111i\u1ec3m ch\xednh :\\n*\\tT\u01b0\u01a1ng t\u1ef1 AlexNet nh\u01b0ng c\xf3 m\u1ed9t s\u1ed1 \u0111i\u1ec1u ch\u1ec9nh nh\u1ecf.\\n*\\tAlexnet training tr\xean 15m image trong khi ZF training ch\u1ec9 c\xf3 1.3m image.\\n*\\tS\u1eed d\u1ee5ng kernel 7x7 \u1edf first layer (alexnet 11x11).L\xfd do l\xe0 s\u1eed d\u1ee5ng kernel nh\u1ecf h\u01a1n \u0111\u1ec3 gi\u1eef l\u1ea1i nhi\u1ec1u th\xf4ng tin tr\xean image h\u01a1n.\\n*\\tT\u0103ng s\u1ed1 l\u01b0\u1ee3ng filter nhi\u1ec1u h\u01a1n so v\u1edbi alexnet\\n*\\tTraining tr\xean GTX 580 GPU trong 20 ng\xe0y\\n\x3c!-- ![cnn4](/img/20180707/cnn4.jpg)<br/>\\n --\x3e\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn4.jpg\').default} />\\n</center>\\n\\n<center>\\n H\xecnh 4. ZFNet(2013).\\n</center>\\n\\n* Input shape 224x224x3 .\\n* Layer 1 :\\n   * Conv 1 : kernel = 7x7x3, stride = 2, no padding, number filter = 96, output = 110x110x96.\\n   * Maxpooling 1 : pooling size = 3x3,stride=2, padding = \u201csame\u201d,output = 55x55x96\\n   * Normalize layer.\\n* Layer 2 :\\n   * Conv 2 : kernel = 5x5x96, stride = 2, no padding, number filter = 256, output = 26x26x256.\\n   * Maxpooling 2 : pooling size = 3x3, stride=2,  padding = \u201csame\u201d,output = 13x13x256\\n   * Normalize layer.\\n* Layer 3:\\n   * Conv 3 : kernel = 3x3x256, stride=1, padding=\u201dsame\u201d, number filter = 384,output = 13x13x384.\\n   * Layer 4 :\\n   * Conv 4 : kernel = 3x3x384, stride=1, padding=\u201dsame\u201d, number filter = 384,output = 13x13x384.\\n* Layer 5 :\\n   * Conv 5 : kernel = 3x3x384, stride=1, padding=\u201dsame\u201d, number filter = 256,output = 13x13x256.\\n   * Maxpooling  : pooling size = 3x3,stride =2,padding =\u201dsame\u201d,output = 6x6x256.\\n* Flatten 6x6x256 = 9216\\n* Fully connected 1 : activation = relu,output =4096\\n* Fully connected 2 : activation = relu,output =4096\\n* Softmax layer for classifier ouput = 1000\\n\\n## 4. VGGNet(2014).\\n* Sau AlexNet th\xec VGG ra \u0111\u1eddi v\u1edbi m\u1ed9t s\u1ed1 c\u1ea3i thi\u1ec7n h\u01a1n , tr\u01b0\u1edbc ti\xean l\xe0 model VGG s\u1ebd deeper h\u01a1n, ti\u1ebfp theo l\xe0 thay \u0111\u1ed5i trong th\u1ee9 t\u1ef1 conv. T\u1eeb LeNet \u0111\u1ebfn AlexNet \u0111\u1ec1u s\u1eed d\u1ee5ng Conv-maxpooling c\xf2n VGG th\xec s\u1eed d\u1ee5ng 1 chu\u1ed7i Conv li\xean ti\u1ebfp Conv-Conv-Conv \u1edf middle v\xe0 end c\u1ee7a architect VGG. Vi\u1ec7c n\xe0y s\u1ebd l\xe0m cho vi\u1ec7c t\xednh to\xe1n tr\u1edf n\xean l\xe2u h\u01a1n nh\u01b0ng nh\u1eefng feature s\u1ebd v\u1eabn \u0111\u01b0\u1ee3c gi\u1eef l\u1ea1i nhi\u1ec1u h\u01a1n so v\u1edbi vi\u1ec7c s\u1eed d\u1ee5ng maxpooling sau m\u1ed7i Conv. H\u01a1n n\u1eefa hi\u1ec7n nay v\u1edbi s\u1ef1 ra \u0111\u1eddi c\u1ee7a GPU gi\xfap t\u1ed1c \u0111\u1ed9 t\xednh to\xe1n tr\u1edf n\xean nhanh h\u01a1n r\u1ea5t nhi\u1ec1u l\u1ea7n th\xec v\u1ea5n \u0111\u1ec1 n\xe0y kh\xf4ng c\xf2n \u0111\xe1ng lo ng\u1ea1i. VGG cho small error h\u01a1n AlexNet trong ImageNet Large Scale Visual Recognition Challenge (ILSVRC) n\u0103m 2014. VGG c\xf3 2 phi\xean b\u1ea3n l\xe0 VGG16 v\xe0 VGG19.\\n\x3c!-- ![cnn5](/img/20180707/cnn10.jpg)<br/> --\x3e\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn10.jpg\').default} />\\n</center>\\n <center> H\xecnh 5. VGGNet(2014).</center>\\n* Architect c\u1ee7a VGG16 bao g\u1ed3m 16 layer :13 layer Conv (2 layer conv-conv,3 layer conv-conv-conv) \u0111\u1ec1u c\xf3 kernel 3x3, sau m\u1ed7i layer conv l\xe0 maxpooling downsize xu\u1ed1ng 0.5, v\xe0 3 layer fully connection. VGG19 t\u01b0\u01a1ng t\u1ef1 nh\u01b0 VGG16 nh\u01b0ng c\xf3 th\xeam 3 layer convolution \u1edf 3 layer conv cu\u1ed1i ( th\xe0nh 4 conv stack v\u1edbi nhau).\\nDetail parameter VGG16\\n\x3c!-- ![cnn15](/img/20180707/cnn15.jpg)<br/> --\x3e\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn15.jpg\').default} />\\n</center>\\n <center> H\xecnh 15. VGG16 </center>\\n* S\u1eed d\u1ee5ng kernel 3x3 thay v\xec 11x11 \u1edf alexnet(7x7 ZFNet). K\u1ebft h\u1ee3p 2 conv 3x3 c\xf3 hi\u1ec3u qu\u1ea3 h\u01a1n 1 cov 5x5 v\u1ec1 receptive field gi\xfap m\u1ea1ng deeper h\u01a1n  l\u1ea1i gi\u1ea3m tham s\u1ed1 t\xednh to\xe1n cho model.\\n* 3 Conv 3x3 c\xf3 receptive field same 1 conv 7x7.\\n* Input size gi\u1ea3m d\u1ea7n qua c\xe1c conv nh\u01b0ng t\u0103ng s\u1ed1 chi\u1ec1u s\xe2u.\\n* L\xe0m vi\u1ec7c r\u1ea5t t\u1ed1t cho task classifier v\xe0 localizer ( r\u1ea5t hay \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong object detection).\\n* S\u1eed d\u1ee5ng relu sau m\u1ed7i conv v\xe0 training b\u1eb1ng batch gradient descent.\\n* C\xf3 s\u1eed d\u1ee5ng data augmentation technique trong qu\xe1 tr\xecnh training.\\n* Training v\u1edbi 4 Nvidia Titan Black GPUs trong 2-3 tu\u1ea7n.\\n\\n## 5. GoogleNet(2014). \\n\\nN\u0103m 2014, google publish m\u1ed9t m\u1ea1ng neural do nh\xf3m research c\u1ee7a h\u1ecd ph\xe1t tri\u1ec3n c\xf3 t\xean l\xe0 googleNet. N\xf3 performance t\u1ed1t h\u01a1n VGG, googleNet 6.7% error rate trong khi VGG l\xe0 7.3%  \xdd t\u01b0\u1edfng ch\xednh l\xe0 h\u1ecd t\u1ea1o ra m\u1ed9t module m\u1edbi c\xf3 t\xean l\xe0 inception gi\xfap m\u1ea1ng traning s\xe2u v\xe0 nhanh h\u01a1n, ch\u1ec9 c\xf3 5m tham s\u1ed1 so v\u1edbi alexnet l\xe0 60m nhanh h\u01a1n g\u1ea5p 12 l\u1ea7n.\\nInception module l\xe0 m\u1ed9t m\u1ea1ng CNN gi\xfap training wider(thay v\xec them nhi\u1ec1u layer h\u01a1n v\xec r\u1ea5t d\u1ec5 x\u1ea3y ra overfitting + t\u0103ng parameter ng\u01b0\u1eddi ta ngh\u0129 ra t\u0103ng deeper \u1edf m\u1ed7i t\u1ea7ng layer) so v\u1edbi m\u1ea1ng CNN b\xecnh th\u01b0\u1eddng. M\u1ed7i layer trong CNN truy\u1ec1n th\u1ed1ng s\u1ebd extract c\xe1c th\xf4ng tin kh\xe1c nhau. Output c\u1ee7a 5x5 conv kernel s\u1ebd kh\xe1c v\u1edbi 3x3 kernel. V\u1eady \u0111\u1ec3 l\u1ea5y nh\u1eefng th\xf4ng tin c\u1ea7n thi\u1ebft cho b\xe0i to\xe1n c\u1ee7a ch\xfang ta th\xec n\xean d\xf9ng kernel size nh\u01b0 th\u1ebf n\xe0o ? T\u1ea1i sao ch\xfang s\u1eed d\u1ee5ng t\u1ea5t c\u1ea3 ta v\xe0 sau \u0111\xf3 \u0111\u1ec3 model t\u1ef1 ch\u1ecdn. \u0110\xf3 ch\xednh l\xe0 \xfd t\u01b0\u1edfng c\u1ee7a Inception module, n\xf3  t\xednh to\xe1n c\xe1c kernel size kh\xe1c nhau t\u1eeb m\u1ed9t input sau \u0111\xf3 concatenate n\xf3 l\u1ea1i th\xe0nh output. \\n\x3c!-- ![cnn6](/img/20180707/cnn5.jpg)<br/> --\x3e\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn5.jpg\').default} />\\n</center>\\n\\n <center> H\xecnh 6. Inception. </center>\\n\\nTrong inception ng\u01b0\u1eddi ta d\xf9ng conv kernel 1x1 v\u1edbi 2 m\u1ee5c \u0111\xedch l\xe0 gi\u1ea3m tham s\u1ed1 t\xednh to\xe1n v\xe0 dimensionality reduction . Dimensionality reduction c\xf3 th\u1ec3 hi\u1ec3u l\xe0m gi\u1ea3m depth c\u1ee7a input (vd iput 28x28x100 qua kernel 1x1 v\u1edbi filter = 10 s\u1ebd gi\u1ea3m depth v\u1ec1 c\xf2n 28x28x10). Gi\u1ea3m chi ph\xed t\xednh to\xe1n c\xf3 th\u1ec3  hi\u1ec3u qua v\xed d\u1ee5 sau :\\n* Input shape 28x28x192 qua kernel 5x5 v\u1edbi 32 th\xec ouput l\xe0 28x28x32(padding same) th\xec  tham s\u1ed1 t\xednh to\xe1n l\xe0 (5x5x192)*(28x28x32)=120 million\\n* Input shape 28x28x192 qua kernel 1x1x192 filter = 16 , output = 28x28x16 ti\u1ebfp t\u1ee5c v\u1edbi kernel 5x5x32 filter = 16 \u0111\u01b0\u01a1ch output = 28x28x32. T\u1ed5ng tham s\u1ed1 t\xednh to\xe1n : $(28x28x16)*192 + (28x28x32)*(5x5x16) = 2.4 + 10 = 12.4 million.$\\nTa th\u1ea5y v\u1edbi c\xf9ng output l\xe0 28x28x32 th\xec n\u1ebfu d\xf9ng kernel 5x5x192 v\u1edbi 32 filter th\xec s\u1ebd c\xf3 tham s\u1ed1 g\u1ea5p 10 l\u1ea7n so v\u1edbi s\u1eed d\u1ee5ng kernel 1x1x192 sau \u0111\xf3 d\xf9ng ti\u1ebfp 1 kernel 5x5x16 v\u1edbi filter 32.\\nInception hi\u1ec7n gi\u1edd c\xf3 4 version , ta s\u1ebd c\xf9ng t\xecm hi\u1ec3u s\u01a1 qua c\xe1c version:\\n* Inception v1 : c\xf3 2 d\u1ea1ng  l\xe0 na\xefve v\xe0 dimension reduction. Kh\xe1c bi\u1ec7t ch\xednh \u0111\xf3 l\xe0 version dimension reduction n\xf3 d\xf9ng conv 1x1 \u1edf m\u1ed7i layer \u0111\u1ec3 gi\u1ea3m depth c\u1ee7a input gi\xfap model c\xf3 \xedt tham s\u1ed1 h\u01a1n. Inception na\xefve c\xf3 architect g\u1ed3m 1x1 conv,3x3  conv, 5x5 conv v\xe0 3x3 maxpooling.\\n\x3c!-- ![cnn7](/img/20180707/cnn6.jpg)<br/> --\x3e\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn6.jpg\').default} />\\n</center>\\n\\n<center> H\xecnh 7. Inception V1.</center>\\n\\nInception v2 : C\u1ea3i thi\u1ec7n version 1, th\xeam layer batchnormalize v\xe0 gi\u1ea3m Internal Covariate Shift. Ouput c\u1ee7a m\u1ed7i layer s\u1ebd \u0111\u01b0\u1ee3c normalize v\u1ec1 Gaussian N(0,1). Conv 5x5 s\u1ebd \u0111\u01b0\u1ee3c thay th\u1ebf b\u1eb1ng 2 conv 3x3 \u0111\u1ec3 gi\u1ea3m computation cost.\\n\x3c!-- ![cnn8](/img/20180707/cnn7.jpg)<br/> --\x3e\\n\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn7.jpg\').default} />\\n</center>\\n\\n<center> H\xecnh 8. Inception V2. </center>\\n\\nInception v3 : \u0110i\u1ec3m \u0111\xe1ng ch\xfa \xfd \u1edf version n\xe0y l\xe0 Factorization. Conv 7x7 s\u1ebd \u0111\u01b0\u1ee3c gi\u1ea3m v\u1ec1 conv 1 dimesion l\xe0 (1x7),(7x1). T\u01b0\u01a1ng t\u1ef1 conv 3x3 (3x1,1x3). T\u0103ng t\u1ed1c \u0111\u1ed9 t\xednh to\xe1n. Khi t\xe1ch ra 2 conv th\xec l\xe0m model deeper h\u01a1n.\\n\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn8.jpg\').default} />\\n</center>\\n\\n<center> H\xecnh 9. Inception V3. </center>\\nInception v4 : l\xe0 s\u1ef1 k\u1ebft h\u1ee3p inception v\xe0 resnet.\\nDetail googleNet architect :\\n\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn9.jpg\').default} />\\n</center>\\n<center> H\xecnh 10. GoogleNet.</center>\\nGoogleNet g\u1ed3m 22 layer, kh\u1edfi \u0111\u1ea7u v\u1eabn l\xe0 nh\u1eefng simple convolution layer, ti\u1ebfp theo l\xe0 nh\u1eefng block c\u1ee7a inception module v\u1edbi maxpooling theo sau m\u1ed7i block. M\u1ed9t s\u1ed1 \u0111\u1eb7c \u0111i\u1ec3m ch\xednh.\\n* S\u1eed d\u1ee5ng 9 Inception module tr\xean to\xe0n b\u1ed9 architect. L\xe0m model deeper h\u01a1n r\u1ea5t nhi\u1ec1u.\\n* Kh\xf4ng s\u1eed d\u1ee5ng fully connection layer m\xe0 thay v\xe0o \u0111\xf3 l\xe0 average pooling t\u1eeb 7x7x1024 volume th\xe0nh 1x1x1024 volume gi\u1ea3m thi\u1ec3u \u0111\u01b0\u1ee3c r\u1ea5t nhi\u1ec1u parameter.\\n* \xcdt h\u01a1n 12x parameter so v\u1edbi Alexnet.\\n* Auxiliary Loss \u0111\u01b0\u1ee3c add v\xe0o total loss(weight =0.3). Nh\u01b0ng \u0111\u01b0\u1ee3c lo\u1ea1i b\u1ecf khi test.\\n\\n## 6. ResNets(2015).\\nResNet \u0111\u01b0\u1ee3c ph\xe1t tri\u1ec3n b\u1edfi microsoft n\u0103m 2015 v\u1edbi paper \u201c Deep residual learning for image recognition\u201d. ResNet winer ImageNet ILSVRC competition 2015 v\u1edbi error rate 3.57% ,ResNet c\xf3 c\u1ea5u tr\xfac g\u1ea7n gi\u1ed1ng VGG v\u1edbi nhi\u1ec1u stack layer l\xe0m cho model deeper h\u01a1n. Kh\xf4ng gi\u1ed1ng VGG, resNet  c\xf3 depth s\xe2u h\u01a1n nh\u01b0 34,55,101 v\xe0 151 . Resnet gi\u1ea3i quy\u1ebft \u0111\u01b0\u1ee3c v\u1ea5n \u0111\u1ec1 c\u1ee7a deep learning truy\u1ec1n th\u1ed1ng , n\xf3 c\xf3 th\u1ec3 d\u1ec5 d\xe0ng training model v\u1edbi h\xe0ng tr\u0103m layer. \u0110\u1ec3 hi\u1ec3u ResNet ch\xfang ta c\u1ea7n hi\u1ec3u v\u1ea5n \u0111\u1ec1 khi stack nhi\u1ec1u layer khi training, v\u1ea5n \u0111\u1ec1 \u0111\u1ea7u ti\xean khi t\u0103ng model deeper h\u01a1n gradient s\u1ebd b\u1ecb vanishing/explodes. V\u1ea5n \u0111\u1ec1 n\xe0y c\xf3 th\u1ec3 gi\u1ea3i quy\u1ebft b\u1eb1ng c\xe1ch th\xeam Batch Normalization n\xf3 gi\xfap normalize output gi\xfap c\xe1c h\u1ec7 s\u1ed1 tr\u1edf n\xean c\xe2n b\u1eb1ng h\u01a1n kh\xf4ng qu\xe1 nh\u1ecf ho\u1eb7c qu\xe1 l\u1edbn n\xean s\u1ebd gi\xfap model d\u1ec5 h\u1ed9i t\u1ee5 h\u01a1n. V\u1ea5n \u0111\u1ec1 th\u1ee9 2 l\xe0  degradation, Khi model deeper accuracy b\u1eaft \u0111\u1ea7u b\xe3o h\xf2a(saturated) th\u1eadm ch\xed l\xe0 gi\u1ea3m. Nh\u01b0 h\xecnh v\u1ebd b\xean d\u01b0\u1edbi khi stack nhi\u1ec1u layer h\u01a1n th\xec training error l\u1ea1i cao h\u01a1n \xedt layer nh\u01b0 v\u1eady v\u1ea5n \u0111\u1ec1 kh\xf4ng ph\u1ea3i l\xe0 do overfitting. V\u1ea5n \u0111\u1ec1 n\xe0y l\xe0 do model kh\xf4ng d\u1ec5 training kh\xf3 h\u1ecdc h\u01a1n, th\u1eed t\u01b0\u1ee3ng t\u01b0\u1ee3ng m\u1ed9t training m\u1ed9t shallow model, sau \u0111\xf3 ch\xfang ta stack th\xeam nhi\u1ec1u layer , c\xe1c layer sau khi th\xeam v\xe0o s\u1ebd kh\xf4ng h\u1ecdc th\xeam \u0111\u01b0\u1ee3c g\xec c\u1ea3 (identity mapping) n\xean accuracy s\u1ebd t\u01b0\u01a1ng t\u1ef1 nh\u01b0 shallow model m\xe0 kh\xf4ng t\u0103ng. Resnet \u0111\u01b0\u1ee3c ra \u0111\u1eddi \u0111\u1ec3 gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 degradation n\xe0y.\\n\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn11.jpg\').default} />\\n</center>\\n<center> H\xecnh 11. Compare accuracy with </center>\\nResNet c\xf3 architecture g\u1ed3m nhi\u1ec1u residual block, \xfd t\u01b0\u1edfng ch\xednh l\xe0 skip layer b\u1eb1ng c\xe1ch add connection v\u1edbi layer tr\u01b0\u1edbc. \xdd t\u01b0\u1edfng c\u1ee7a residual block l\xe0 feed foword x(input) qua m\u1ed9t s\u1ed1 layer conv-max-conv, ta thu \u0111\u01b0\u1ee3c F(x) sau \u0111\xf3 add th\xeam x v\xe0o H(x) = F(x) + x . Model s\u1ebd d\u1ec5 h\u1ecdc h\u01a1n khi ch\xfang ta th\xeam feature t\u1eeb layer tr\u01b0\u1edbc v\xe0o.\\n\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn12.jpg\').default} />\\n</center>\\n<center> H\xecnh 12. ResNets block. </center>\\n* S\u1eed d\u1ee5ng batch Normalization sau m\u1ed7i Conv layer.\\n* Initialization Xavier/2\\n* Training v\u1edbi SGD + momentum(0.9)\\n* Learning rate 0.1, gi\u1ea3m 10 l\u1ea7n n\u1ebfu error ko gi\u1ea3m\\n* Mini batch size 256\\n* Weight decay 10^-5\\n* Kh\xf4ng s\u1eed d\u1ee5ng dropout\\n\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn13.jpg\').default} />\\n</center>\\n<center> H\xecnh 13. ResNets(2015). </center>\\n\\n## 7. Densenet(2016)\\nDensenet(Dense connected convolutional network) l\xe0 m\u1ed9t trong nh\u1eefng netwok m\u1edbi nh\u1ea5t cho visual object recognition. N\xf3 c\u0169ng g\u1ea7n gi\u1ed1ng Resnet nh\u01b0ng c\xf3 m\u1ed9t v\xe0i \u0111i\u1ec3m kh\xe1c bi\u1ec7t. Densenet c\xf3 c\u1ea5u tr\xfac g\u1ed3m c\xe1c dense block v\xe0 c\xe1c transition layers. \u0110\u01b0\u1ee3c stack dense block- transition layers-dense block- transition layers nh\u01b0 h\xecnh v\u1ebd. V\u1edbi CNN truy\u1ec1n th\u1ed1ng n\u1ebfu ch\xfang ta c\xf3 L layer th\xec s\u1ebd c\xf3 L connection, c\xf2n trong densenet s\u1ebd c\xf3 L(L+1)/2 connection.\\n\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn14.jpg\').default} />\\n</center>\\n<center> H\xecnh 14. Densenet(2016). </center>\\nH\xe3y t\u01b0\u1edfng t\u01b0\u1ee3ng ban \u0111\u1ea7u ta c\xf3 1 image size (28,28,3). \u0110\u1ea7u ti\xean ta kh\u1edfi t\u1ea1o feature layer b\u1eb1ng Conv t\u1ea1o ra 1 layer size (28,28,24). Sau m\u1ed7i layer ti\u1ebfp theo (Trong dense block ) n\xf3 s\u1ebd t\u1ea1o th\xeam K= 12 feature gi\u1eefa nguy\xean width v\xe0 height. Khi \u0111\xf3 output ti\u1ebfp theo s\u1ebd l\xe0 (28,28,24 +12),(28,28,24 +12+12). \u1ede m\u1ed7i dense block s\u1ebd c\xf3 normalization, nonlinearity v\xe0 dropout. \u0110\u1ec3 gi\u1ea3m size v\xe0 depth c\u1ee7a feature th\xec transition layer \u0111\u01b0\u1ee3c \u0111\u1eb7t gi\u1eefa c\xe1c dense block, n\xf3 g\u1ed3m Conv kernel size =1, average pooling (2x2) v\u1edbi stride = 2 n\xf3 s\u1ebd gi\u1ea3m output th\xe0nh (14,14,48)\\n\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn16.jpg\').default} />\\n</center>\\nDetail parameter :\\n\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./cnn17.jpg\').default} />\\n</center>\\nM\u1ed9t s\u1ed1 \u01b0u \u0111i\u1ec3m c\u1ee7a Densenet:\\n* Accuracy : Densenet training tham s\u1ed1 \xedt h\u01a1n 1 n\u1eeda so v\u1edbi Resnet nh\u01b0ng c\xf3 same accuracy so tr\xean ImageNet classification dataset.\\n* Overfitting : DenseNet resistance overfitting r\u1ea5t hi\u1ec7u qu\u1ea3.\\n* Gi\u1ea3m \u0111\u01b0\u1ee3c vashing gradient.\\n* S\u1eed d\u1ee5ng l\u1ea1i feature hi\u1ec7u qu\u1ea3 h\u01a1n. \\n  \\n## K\u1ebft b\xe0i : \\nTr\xean \u0111\xe2y ch\u1ec9 l\xe0 ph\u1ea7n t\xf3m l\u01b0\u1ee3c s\u01a1 qua c\xe1c architect n\u1ed5i ti\u1ebfng c\u1ee7a CNN. B\xean trong \u0111\xf3 c\xf2n c\xf3 nhi\u1ec1u c\u1ea5u tr\xfac ph\u1ee9c t\u1ea1p v\xec th\u1eddi gian c\u0169ng nh\u01b0 ki\u1ebfn th\u1ee9c c\xf3 h\u1ea1n n\xean v\u1eabn ch\u01b0a vi\u1ebft s\xe2u h\u1ebft \u0111\u01b0\u1ee3c. B\xean c\u1ea1nh \u0111\xf3 c\xf2n c\xf3 nhi\u1ec1u sai s\xf3t r\u1ea5t mong b\u1ea1n \u0111\u1ecdc g\xf3p \xfd \u0111\u1ec3 m\xecnh c\xf3 th\u1ec3 ho\xe0n thi\u1ec7n b\xe0i vi\u1ebft h\u01a1n.\\n\\nTham kh\u1ea3o : \\n* Deep learning course : Andrew Ng\\n* An Intuitive Guide to Deep Network Architectures : Joyce Xu\\n* A Simple Guide to the Versions of the Inception Network :Bharath Raj\\n* CS231n: Convolutional Neural Networks for Visual Recognition\\n* Paper : All paper about lenet,alexnet,vgg,googlenet,resnet,densenet\\n* Notes on the Implementation of DenseNet in TensorFlow.\\n* The Efficiency of Densenet"},{"id":"Object-detection-t\u1eeb-R-CNN-\u0111\u1ebfn-Faster-R-CNN","metadata":{"permalink":"/blog/Object-detection-t\u1eeb-R-CNN-\u0111\u1ebfn-Faster-R-CNN","editUrl":"https://github.com/ThorPham/blog/2018-8-8-Object-detection-t\u1eeb-R-CNN-\u0111\u1ebfn-Faster-R-CNN/index.md","source":"@site/blog/2018-8-8-Object-detection-t\u1eeb-R-CNN-\u0111\u1ebfn-Faster-R-CNN/index.md","title":"Object detection t\u1eeb R-CNN \u0111\u1ebfn Faster R-CNN","description":"Nh\u01b0 ch\xfang ta \u0111\xe3 bi\u1ebft object detection bao g\u1ed3m 2 nhi\u1ec7m v\u1ee5 ch\xednh l\xe0 Classifier v\xe0 Localization. Trong \u0111\xf3 nhi\u1ec7m v\u1ee5 c\xf3 v\u1ebb kh\xf3 kh\u0103n h\u01a1n l\xe0 Localization. Tr\u01b0\u1edbc khi deep learning ph\xe1t tri\u1ec3n nh\u01b0 hi\u1ec7n nay, trong computer vision ng\u01b0\u1eddi ta detection object qua 2 giai \u0111o\u1ea1n. \u0110\u1ea7u ti\xean l\xe0 tr\xedch xu\u1ea5t feature t\u1eeb hog,lbp,sift sau \u0111\xf3 d\xf9ng c\xe1c thu\u1eadt to\xe1n trong machine learning nh\u01b0 SVM \u0111\u1ec3 classifier. B\u01b0\u1edbc ti\u1ebfp theo l\xe0 detection object tr\xean \u1ea3nh l\u1edbn th\xec ng\u01b0\u1eddi ta s\u1ebd d\xf9ng 1 window search tr\xean to\xe0n b\u1ed9 b\u1ee9c \u1ea3nh sau \u0111\xf3 d\xf9ng model \u0111\xe3 classifier \u0111\u1ec3 ph\xe2n l\u1edbp object. C\xe1c model n\xe0y c\xf3 \u01b0u \u0111i\u1ec3m l\xe0 th\u1eddi gian build model t\u01b0\u01a1ng \u0111\u1ed1i nhanh, c\u1ea7n \xedt d\u1eef li\u1ec7u . Nh\u01b0\u1ee3c \u0111i\u1ec3m l\xe0 \u0111\u1ed9 ch\xednh x\xe1c kh\xf4ng cao v\xe0 th\u1eddi gian predict r\u1ea5t l\xe2u n\xean kh\xf3 c\xf3 th\u1ec3 d\xf9ng trong real time.","date":"2018-08-08T00:00:00.000Z","formattedDate":"August 8, 2018","tags":[{"label":"Object detection","permalink":"/blog/tags/object-detection"},{"label":"CNN","permalink":"/blog/tags/cnn"}],"readingTime":6.795,"truncated":true,"authors":[{"name":"Thorpham","title":"Deep learning enthusiast","url":"https://github.com/ThorPham","imageURL":"https://github.com/ThorPham.png","key":"thorpham"}],"frontMatter":{"slug":"Object-detection-t\u1eeb-R-CNN-\u0111\u1ebfn-Faster-R-CNN","title":"Object detection t\u1eeb R-CNN \u0111\u1ebfn Faster R-CNN","authors":"thorpham","tags":["Object detection","CNN"]},"prevItem":{"title":"Qu\xe1 tr\xecnh ph\xe1t tri\u1ec3n c\u1ee7a CNN t\u1eeb LeNet \u0111\u1ebfn DenseNet.","permalink":"/blog/Qu\xe1-tr\xecnh-ph\xe1t-tri\u1ec3n-c\u1ee7a-CNN-t\u1eeb-LeNet-\u0111\u1ebfn-DenseNet"},"nextItem":{"title":"Sentiment Analysis s\u1eed d\u1ee5ng Tf-Idf \xe1p d\u1ee5ng cho ng\xf4n ng\u1eef ti\u1ebfng vi\u1ec7t","permalink":"/blog/Sentiment-Analysis-s\u1eed-d\u1ee5ng-Tf-Idf"}},"content":"*Nh\u01b0 ch\xfang ta \u0111\xe3 bi\u1ebft object detection bao g\u1ed3m 2 nhi\u1ec7m v\u1ee5 ch\xednh l\xe0 Classifier v\xe0 Localization. Trong \u0111\xf3 nhi\u1ec7m v\u1ee5 c\xf3 v\u1ebb kh\xf3 kh\u0103n h\u01a1n l\xe0 Localization. Tr\u01b0\u1edbc khi deep learning ph\xe1t tri\u1ec3n nh\u01b0 hi\u1ec7n nay, trong computer vision ng\u01b0\u1eddi ta detection object qua 2 giai \u0111o\u1ea1n. \u0110\u1ea7u ti\xean l\xe0 tr\xedch xu\u1ea5t feature t\u1eeb hog,lbp,sift sau \u0111\xf3 d\xf9ng c\xe1c thu\u1eadt to\xe1n trong machine learning nh\u01b0 SVM \u0111\u1ec3 classifier. B\u01b0\u1edbc ti\u1ebfp theo l\xe0 detection object tr\xean \u1ea3nh l\u1edbn th\xec ng\u01b0\u1eddi ta s\u1ebd d\xf9ng 1 window search tr\xean to\xe0n b\u1ed9 b\u1ee9c \u1ea3nh sau \u0111\xf3 d\xf9ng model \u0111\xe3 classifier \u0111\u1ec3 ph\xe2n l\u1edbp object. C\xe1c model n\xe0y c\xf3 \u01b0u \u0111i\u1ec3m l\xe0 th\u1eddi gian build model t\u01b0\u01a1ng \u0111\u1ed1i nhanh, c\u1ea7n \xedt d\u1eef li\u1ec7u . Nh\u01b0\u1ee3c \u0111i\u1ec3m l\xe0 \u0111\u1ed9 ch\xednh x\xe1c kh\xf4ng cao v\xe0 th\u1eddi gian predict r\u1ea5t l\xe2u n\xean kh\xf3 c\xf3 th\u1ec3 d\xf9ng trong real time.*\\n\x3c!--truncate--\x3e\\nV\u1edbi t\u1ed9c \u0111\u1ed9 ph\xe1t tri\u1ec3n nh\u01b0 hi\u1ec7n nay , d\u1eef li\u1ec7u c\u1ee7a ch\xfang ta ng\xe0y c\xe0ng nhi\u1ec1u v\xe0 c\xe1c b\xe0i to\xe1n b\u1eaft \u0111\u1ea7u kh\xf3 d\u1ea7n l\xean n\xean nh\u1eefng model truy\u1ec1n th\u1ed1ng t\u1ecf ra k\xe9m hi\u1ec7u qu\u1ea3. C\xe1c feature l\u1ea5y ra t\u1eeb computer vision truy\u1ec1n th\u1ed1ng nh\u01b0 hog,sift,lbp l\xe0 nh\u1eefng shadow feature n\xf3 ch\u1ec9 l\u1ea5y \u0111\u01b0\u1ee3c nh\u1eefng feature tr\xean b\u1ec1 m\u1eb7t n\u1ed5i image m\xe0 th\xf4i, do \u0111\xf3 nh\u1eefng b\xe0i to\xe1n nh\u01b0 classifer con ch\xf3 hay con m\xe8o th\xec nh\u1eefng feature n\xe0y l\xe0m vi\u1ec7c t\u01b0\u01a1ng \u0111\u1ed1i hi\u1ec7u qu\u1ea3 , nh\u01b0ng n\xe2ng c\u1ea5p b\xe0i to\xe1n l\xean \u0111\xf3 l\xe0 classifier con bull dog hay con b\xe9c r\xea th\xec nh\u1eefng feature n\xe0y l\xe0m vi\u1ec7c k\xe9m hi\u1ec7u qu\u1ea3. C\u0169ng d\u1ec5 hi\u1ec3u v\xec c\xf9ng 1 class l\xe0 dog th\xec nh\u1eefng feature n\xe0y t\u01b0\u01a1ng \u0111\u1ed1i gi\u1ed1ng nhau n\xean kh\xf3 c\xf3 th\u1ec3 ph\xe2n bi\u1ec7t \u0111\u01b0\u1ee3c con n\xe0y con kia. Ch\xednh v\xec th\u1ebf ta c\u1ea7n nh\u1eefng feature s\xe2u h\u01a1n, nh\u1eefng feature m\xe0 n\xf3 \u1ea9n \u1edf trong image m\xe0 ta kh\xf3 c\xf3 th\u1ec3 quan s\xe1t \u0111\u01b0\u1ee3c b\u1eb1ng m\u1eaft th\u01b0\u1eddng \u0111\u1ec3 ph\xe2n bi\u1ec7t dog n\xe0y hay dog kia. Trong deep learning , object detection n\u1ec1n t\u1ea3ng c\u01a1 b\u1ea3n l\xe0 d\u1ef1a tr\xean m\u1ea1ng CNN \u0111\u1ec3 l\u1ea5y nh\u1eefng deep feature b\u1eb1ng c\xe1ch \u0111\u01b0a qua nhi\u1ec1u layer kh\xe1c nhau feature \u0111\u01b0\u1ee3c extract s\xe2u h\u01a1n sau \u0111\xf3 \u0111\u01b0\u1ee3c \u0111\u01b0a v\xe0o classifier v\xe0 regression box. Hai h\u01b0\u1edbng ti\u1ebfp c\u1eadn ch\xednh trong deep learning l\xe0 :\\n\\n* Chia image ra th\xe0nh nh\u1eefng grid cell SxS . M\u1ed7i cell \u0111\u01b0\u1ee3c coi nh\u01b0 region proposal gi\xfap gi\u1ea3m th\u1eddi gian v\xe0 chi ph\xed t\xednh to\xe1n thay v\xec s\u1eed d\u1ee5ng tr\u1ef1c ti\u1ebfp image ( model SSD,YOLO)\\n* T\xecm nh\u1eefng region proposal c\xf3 nhi\u1ec1u kh\u1eb3n n\u0103ng ch\u1ee9a object nh\u1ea5t s\u1eed d\u1ee5ng selective search hay RPN ( model R-CNN,Fast R-CNN, Faster R-CNN)\\n \\nTrong b\xe0i n\xe0y ch\xfang ta s\u1ebd t\xecm hi\u1ec3u v\u1ec1 c\xe1c h\u1ecd nh\xe0 CNN cho object detection.\\n\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./35474711_223243181605917_1003697740695207936_n.png\').default} />\\n</center>\\n\\n## 1. R-CNN\\n\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./35329075_223246958272206_8520772647733166080_n.png\').default} />\\n</center>\\n\\nR-CNN l\xe0 vi\u1ebft t\u1eaft c\u1ee7a \u201cRegion-based Convolutional Neural Networks\u201d. \xdd t\u01b0\u1edfng ch\xednh c\u1ee7a n\xf3 g\u1ed3m 2 ph\u1ea7n : \u0110\u1ea7u ti\xean l\xe0 d\xf9ng selective search t\xecm c\xe1c region of interest (roi) tr\xean image \u0111\u1ec3 t\u1ea1o ra c\xe1c bounding box m\xe0 c\xf3 x\xe1c su\u1ea5t cao l\xe0 c\xf3 object. Sau \u0111\xf3 d\xf9ng CNN \u0111\u1ec3 l\u1ea5y feature t\u1eeb c\xe1c region n\xe0y \u0111\u1ec3 classifier v\xe0 regression  box. C\xe1c b\u01b0\u1edbc th\u1ef1c hi\u1ec7n nh\u01b0 sau :\\n* T\u1eeb input image ta d\xf9ng selective search \u0111\u1ec3 l\u1ea5y c\xe1i region proposal .Selective Search n\xf3 ho\u1ea1t \u0111\u1ed9ng b\u1eb1ng c\xe1ch l\xe0 \u0111\u1ea7u ti\xean t\u1ea1o ra c\xe1c seed l\xe0 c\xe1c region segementation tr\xean image(trong skimage h\u1ecd d\xf9ng Felsenszwalb\u2019s efficient graph based image segmentation) C\xe1c region sau \u0111\xf3 s\u1ebd \u0111\u01b0\u1ee3c merger l\u1ea1i v\u1edbi nhau b\u1eb1ng c\xe1ch t\xednh \u0111\u1ed9 t\u01b0\u01a1ng \u0111\u1ed3ng v\u1ec1 color,shape,textuture\u2026 Cu\u1ed1i c\xf9ng ta v\u1ebd bounding box cho t\u1eebng region.M\u1ed7i image ng\u01b0\u1eddi ta s\u1ebd l\u1ea5y t\u1ea7m 2k region proposal.\\n* Ti\u1ebfp \u0111\u1ebfn c\xe1c region proposal s\u1ebd \u0111\u01b0\u1ee3c swarped(crop) \u0111\u1ec3 fix size(v\xec m\u1ed9t s\u1ed1 m\u1ea1ng nh\u01b0 VGG y\xeau c\u1ea7u size input l\xe0 c\u1ed1 \u0111\u1ecbnh,h\u01a1n n\u1eefa ta c\u1ea7n feature output same size). Sau \u0111\xf3 d\xf9ng CNN( VGG,Alexnet) \u0111\u1ec3 l\u1ea5y feature.\\n* Cu\u1ed1i c\xf9ng l\xe0 d\xf9ng SVM \u0111\u1ec3 classifier v\xe0 regression bounding box\\nNh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a ph\u01b0\u01a1ng ph\xe1p n\xe0y l\xe0 training r\u1ea5t l\xe2u v\xec 2k image qua CNN \u0111\u1ec3 l\u1ea5y feature m\u1ea5t r\u1ea5t nhi\u1ec1u th\u1eddi gian (52 s tr\xean cpu) . \\n\\n## 2. SPP net\\n\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./35346991_223247254938843_4269602627899097088_n.png\').default} />\\n</center>\\n\\nThay v\xec feed forword 2k image qua CNN th\xec ng\u01b0\u1eddi ta feed forwork qua CNN m\u1ed9t l\u1ea7n \u0111\u1ec3 l\u1ea5y feature map (feature map = feature + location). Sau \u0111\xf3 d\xf9ng selective search \u0111\u1ec3 t\xecm region proposal, r\u1ed3i project tr\xean feature map \u0111\u1ec3 l\u1ea5y feature t\u01b0\u01a1ng \u1ee9ng. C\xf3 m\u1ed9t v\u1ea5n \u0111\u1ec1 \u1edf \u0111\xe2y l\xe0 c\xe1c feature map c\u1ee7a region proposal c\xf3 size kh\xe1c nhau n\xean khi \u0111\u01b0a qua CNN s\u1ebd c\xf3 length output kh\xe1c nhau. V\xec v\u1eady ng\u01b0\u1eddi ta s\u1eed d\u1ee5ng Spatial paramy pooling layer \u0111\u1ec3  fix size feature.(spp layer ho\u1ea1t \u0111\u1ed9ng  c\u0169ng t\u01b0\u01a1ng t\u1ef1 bag of word trong image processing n\xf3 s\u1ebd chia feature theo Spatial pyramid v\xe0 \xe1p d\u1ee5ng max pooling theo t\u1eebng spatial gi\xfap c\xe1c feature c\xf3 size kh\xe1c nhau th\xe0nh same size). Ph\u1ea7n sau c\xf2n l\u1ea1i t\u01b0\u01a1ng t\u1ef1 nh\u01b0 R-CNN\\n\\n## 3. Fast R-CNN\\n\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./35348615_223247421605493_4297776049992761344_n.png\').default} />\\n</center>\\n\\nFast R-CNN c\u1ea3i thi\u1ec7n \u0111\u01b0\u1ee3c c\xe1c nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a R-CNN b\u1eb1ng c\xe1ch h\u1ee3p nh\u1ea5t 3 model \u0111\u1ed9c l\u1eadp v\u1ed1n r\u1ea5t ch\u1eadm ch\u1ea1p. N\xf3 c\u0169ng c\xf3 ph\u1ea7n gi\u1ed1ng SPP-net l\xe0 d\xf9ng CNN \u0111\u1ec3 l\u1ea5y feature map m\u1ed9t l\u1ea7n thay v\xec d\xf9ng ri\xeang cho m\u1ed7i region proposal. Sau \u0111\xf3 nh\u1eefng feature n\xe0y s\u1ebd \u0111\u01b0\u1ee3c \u0111\u01b0a qua m\u1ed9t Fully connection layer \u0111\u1ec3 classifier v\xe0 regression bounding box. Model n\xe0y t\u01b0\u01a1ng \u0111\u1ed1i nhanh ch\u1ea1y 2s/image.\\nC\xe1c b\u01b0\u1edbc th\u1ee9c hi\u1ec7n thu\u1eadt to\xe1n :\\n* D\xf9ng pretraining model (VGG,ZF\u2026) \u0111\u1ec3 l\u1ea5y feature map.\\n* S\u1eed d\u1ee5ng selective search \u0111\u1ec3 l\u1ea5y region proposal (~2k image). Sau \u0111\xf3 project l\xean feature map \u0111\u1ec3 l\u1ea5y feature t\u01b0\u01a1ng \u1ee9ng\\n* Feature s\u1ebd \u0111\u01b0\u1ee3c \u0111\u01b0a qua ROI pooling \u0111\u1ec3 fix size.\\n* Cu\u1ed1i c\xf9ng Fully connection layer \u0111\u1ec3 classifier v\xe0 regression box\\n  \\n## 4. Faster R-CNN\\n\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./35326994_223247604938808_7752372882168086528_n.png\').default} />\\n</center>\\n\\nFaster R-CNN g\u1ed3m region proposal network ( n\xf3 thay th\u1ebf cho selective search) v\xe0 ph\u1ea7n c\xf2n l\u1ea1i t\u01b0\u01a1ng t\u1ef1 nh\u01b0 Fast R-CNN. Region proposal network(RPN) n\xf3 d\xf9ng 1 sliding window search tr\xean feature map \u0111\u1ec3 t\u1ea1o c\xe1c anchor box. Sau \u0111\xf3 ch\xfang ta chu\u1ea9n b\u1ecb data training cho RPN b\u1eb1ng c\xe1ch g\xe1n nh\xe3n cho m\u1ed7i anchor box d\u1ef1a v\xe0o iou v\u1edbi ground truth. Cu\u1ed1i c\xf9ng d\xf9ng data n\xe0y \u0111\u1ec3 classifier v\xe0 regression bounding box. Ta thu \u0111\u01b0\u1ee3c r\u1ea5t nhi\u1ec1u bounding box v\xe0 d\xf9ng non maximum suppression(NMS) \u0111\u1ec3 lo\u1ea1i b\u1ecf b\u1edbt \u0111i nh\u1eefng box kh\xf4ng c\xf3 nhi\u1ec1u kh\u1eb3n n\u0103ng ch\u1ee9a object. Sau \u0111\xf3 nh\u1eefng bounding box n\xe0y s\u1ebd t\u01b0\u01a1ng t\u1ef1 nh\u01b0 selective search \u1edf fast R-CNN n\xf3 \u0111\u01b0\u1ee3c \u0111\u01b0a qua ROI pooling \u0111\u1ec3 fix size v\xe0 cu\u1ed1i c\xf9ng \u0111\u01b0a v\xe0o fully connection layer \u0111\u1ec3 classifier (x\xe1c \u0111\u1ecbnh object c\u1ee5 th\u1ec3) v\xe0 regression box. Model n\xe0y t\u01b0\u01a1ng \u0111\u1ed1i nhanh predict 0.2s/image(gpu)\\nModel g\u1ed3m c\xe1c b\u01b0\u1edbc sau :\\nPretrain model CNN \u0111\u1ec3 l\u1ea5y feature map.\\n* Training RPN \u0111\u1ec3 t\xecm bouding box v\xe0 classifier (ch\u1ec9 x\xe1c \u0111\u1ecbnh l\xe0 object v\xe0 non-object kh\xf4ng classifier c\u1ee5 th\u1ec3 object). M\u1ed9t siliding window size NxM search tr\xean feature map. T\u1ea1i m\u1ed7i center c\u1ee7a window, ta predict mutil region v\u1edbi scale v\xe0 ratio kh\xe1c nhau. Th\xf4ng th\u01b0\u1eddng l\xe0 3 scale v\xe0 3 ratio n\xean t\u1ea1o ra 9 anchor box. Positive sample IOU > 0.7, negative sample IOU < 0.3.\\n* D\xf9ng data n\xe0y training \u0111\u1ec3 classifier v\xe0 regression . V\xec s\u1ed1 background nhi\u1ec1u n\xean \u0111\u1ec3 h\u1ea1n ch\u1ebf bias ng\u01b0\u1eddi ta d\xf9ng mini bath \u0111\u1ec3 training m\u1ed7i l\u1ea7n \u0111\u01b0a v\xe0o t\u1ec9 l\u1ec7 m\u1ed9t pos v\xe0 neg nh\u1ea5t \u0111\u1ecbnh. Sau \u0111\xf3 lo\u1ea1i b\u1edbt bounding box c\xf3 \xedt kh\u1eb3n n\u0103ng ch\u1ee9a object b\u1eb1ng NMS.\\n* Ta project bounding box l\xean feature map \u0111\u1ec3 l\u1ea5y feature t\u01b0\u01a1ng \u1ee9ng sau \u0111\xf3 \u0111\u01b0a v\xe0o ROI pooling layer fix size \u0111\u1ec3 \u0111\u01b0a v\xe0o Fully connection layer \u0111\u1ec3 classifier t\u1eebng object v\xe0 regression box.\\n\\n## 5. K\u1ebft lu\u1eadn \\nTr\xean \u0111\xe2y m\u1edbi ch\u1ec9 l\xe0 m\u1ed9t b\xe0i gi\u1edbi thi\u1ec7u s\u01a1 qua v\u1ec1 c\xe1ch ho\u1ea1t \u0111\u1ed9ng c\u1ee7a R-CNN \u0111\u1ebfn Faster R-CNN. B\xean trong c\u1ea5u tr\xfac c\u1ee7a m\u1ed7i model n\xe0y t\u01b0\u01a1ng \u0111\u1ed1i ph\u1ee9c t\u1ea1p. N\u1ebfu c\xf3 th\u1ec3 m\xecnh s\u1ebd vi\u1ebft chi ti\u1ebft c\xe1ch ho\u1ea1t \u0111\u1ed9ng c\u1ee7a t\u1eebng model qua c\xe1c b\xe0i sau."},{"id":"Sentiment-Analysis-s\u1eed-d\u1ee5ng-Tf-Idf","metadata":{"permalink":"/blog/Sentiment-Analysis-s\u1eed-d\u1ee5ng-Tf-Idf","editUrl":"https://github.com/ThorPham/blog/2018-7-5-Sentiment-Analysis-s\u1eed-d\u1ee5ng-Tf-Idf /index.mdx","source":"@site/blog/2018-7-5-Sentiment-Analysis-s\u1eed-d\u1ee5ng-Tf-Idf /index.mdx","title":"Sentiment Analysis s\u1eed d\u1ee5ng Tf-Idf \xe1p d\u1ee5ng cho ng\xf4n ng\u1eef ti\u1ebfng vi\u1ec7t","description":"Text mining ( l\u1ea5y th\xf4ng tin t\u1eeb text) l\xe0 m\u1ed9t l\u0129nh v\u1ef1ng r\u1ed9ng v\xe0 \xe1p d\u1ee5ng trong nhi\u1ec1u l\u0129nh v\u1ef1c kh\xe1c nhau. M\u1ed9t s\u1ed1 \u1ee9ng d\u1ee5ng c\xf3 th\u1ec3 k\u1ec3 \u0111\u1ebfn l\xe0 : sentiment analysis, document classification, topic classification, text summarization, machine translation. Trong b\xe0i h\xf4m nay ta s\u1ebd t\xecm hi\u1ec3u v\u1ec1 sentiment analysis.Ph\xe2n t\xedch c\u1ea3m x\xfac(sentiment analysis) \u0111\u01b0\u1ee3c hi\u1ec3u \u0111\u01a1n gi\u1ea3n l\xe0 \u0111\xe1nh gi\xe1 1 c\xe2u n\xf3i, tweet l\xe0 t\xedch c\u1ef1c (pos) hay ti\xeau c\u01b0c(neg). Ch\u1eb3ng h\u1ea1n l\u1ea5y m\u1ed9t v\xed d\u1ee5, b\u1ea1n m\u1edf m\u1ed9t c\u1eeda h\xe0ng b\xe1n \u0111\u1ed3 \u0103n m\xe0 mu\u1ed1n bi\u1ebft tr\xean m\u1ea1ng x\xe3 h\u1ed9i ng\u01b0\u1eddi ta n\xf3i g\xec v\u1ec1 qu\xe1n \u0103n c\u1ee7a b\u1ea1n. B\u1ea1n b\u1eaft \u0111\u1ea7u v\xe0o face, instagram hay tweeter \u0111\u1ec3 thu th\u1eadp c\xe1c commnent li\xean quan \u0111\u1ebfn qu\xe1n \u0103n c\u1ee7a b\u1ea1n. B\u1ea1n b\u1eaft \u0111\u1ea7u \u0111oc th\xec c\xf3 ng\u01b0\u1eddi khen ng\u01b0\u1eddi ch\xea, v\u1ea5n \u0111\u1ec1 x\u1ea3y ra l\xe0 b\xe2y gi\u1edd s\u1ed1 comment n\xf3 t\u0103ng l\xean 1000 hay 10000 b\u1ea1n c\xf3 \u0111\u1ee7 s\u1ee9c \u0111\u1ecdc c\xe1c comment \u0111\xf3 hay kh\xf4ng.B\u1ea1n b\u1eaft \u0111\u1ea7u ngh\u0129 ra s\u1ebd build m\u1ed9t model l\xe0m vi\u1ec7c \u0111\xf3 cho b\u1ea1n. Ta b\u1eaft tay v\xe0o c\xf4ng vi\u1ec7c.","date":"2018-07-05T00:00:00.000Z","formattedDate":"July 5, 2018","tags":[{"label":"NLP","permalink":"/blog/tags/nlp"},{"label":"python","permalink":"/blog/tags/python"},{"label":"sklearn","permalink":"/blog/tags/sklearn"}],"readingTime":8.215,"truncated":true,"authors":[{"name":"Thorpham","title":"Deep learning enthusiast","url":"https://github.com/ThorPham","imageURL":"https://github.com/ThorPham.png","key":"thorpham"}],"frontMatter":{"slug":"Sentiment-Analysis-s\u1eed-d\u1ee5ng-Tf-Idf","title":"Sentiment Analysis s\u1eed d\u1ee5ng Tf-Idf \xe1p d\u1ee5ng cho ng\xf4n ng\u1eef ti\u1ebfng vi\u1ec7t","authors":"thorpham","tags":["NLP","python","sklearn"]},"prevItem":{"title":"Object detection t\u1eeb R-CNN \u0111\u1ebfn Faster R-CNN","permalink":"/blog/Object-detection-t\u1eeb-R-CNN-\u0111\u1ebfn-Faster-R-CNN"},"nextItem":{"title":"Nh\u1eadn d\u1ea1ng ch\u1eef s\u1ed1 vi\u1ebft tay v\u1edbi sklearn v\xe0 opencv","permalink":"/blog/Nh\u1eadn-d\u1ea1ng-ch\u1eef-s\u1ed1-vi\u1ebft-tay"}},"content":"*Text mining ( l\u1ea5y th\xf4ng tin t\u1eeb text) l\xe0 m\u1ed9t l\u0129nh v\u1ef1ng r\u1ed9ng v\xe0 \xe1p d\u1ee5ng trong nhi\u1ec1u l\u0129nh v\u1ef1c kh\xe1c nhau. M\u1ed9t s\u1ed1 \u1ee9ng d\u1ee5ng c\xf3 th\u1ec3 k\u1ec3 \u0111\u1ebfn l\xe0 : sentiment analysis, document classification, topic classification, text summarization, machine translation. Trong b\xe0i h\xf4m nay ta s\u1ebd t\xecm hi\u1ec3u v\u1ec1 sentiment analysis.Ph\xe2n t\xedch c\u1ea3m x\xfac(sentiment analysis) \u0111\u01b0\u1ee3c hi\u1ec3u \u0111\u01a1n gi\u1ea3n l\xe0 \u0111\xe1nh gi\xe1 1 c\xe2u n\xf3i, tweet l\xe0 t\xedch c\u1ef1c (pos) hay ti\xeau c\u01b0c(neg). Ch\u1eb3ng h\u1ea1n l\u1ea5y m\u1ed9t v\xed d\u1ee5, b\u1ea1n m\u1edf m\u1ed9t c\u1eeda h\xe0ng b\xe1n \u0111\u1ed3 \u0103n m\xe0 mu\u1ed1n bi\u1ebft tr\xean m\u1ea1ng x\xe3 h\u1ed9i ng\u01b0\u1eddi ta n\xf3i g\xec v\u1ec1 qu\xe1n \u0103n c\u1ee7a b\u1ea1n. B\u1ea1n b\u1eaft \u0111\u1ea7u v\xe0o face, instagram hay tweeter \u0111\u1ec3 thu th\u1eadp c\xe1c commnent li\xean quan \u0111\u1ebfn qu\xe1n \u0103n c\u1ee7a b\u1ea1n. B\u1ea1n b\u1eaft \u0111\u1ea7u \u0111oc th\xec c\xf3 ng\u01b0\u1eddi khen ng\u01b0\u1eddi ch\xea, v\u1ea5n \u0111\u1ec1 x\u1ea3y ra l\xe0 b\xe2y gi\u1edd s\u1ed1 comment n\xf3 t\u0103ng l\xean 1000 hay 10000 b\u1ea1n c\xf3 \u0111\u1ee7 s\u1ee9c \u0111\u1ecdc c\xe1c comment \u0111\xf3 hay kh\xf4ng.B\u1ea1n b\u1eaft \u0111\u1ea7u ngh\u0129 ra s\u1ebd build m\u1ed9t model l\xe0m vi\u1ec7c \u0111\xf3 cho b\u1ea1n. Ta b\u1eaft tay v\xe0o c\xf4ng vi\u1ec7c.*\\n\x3c!--truncate--\x3e\\nThu\u1eadt to\xe1n s\u1eed dung : m\xecnh s\u1ebd s\u1eed d\u1ee5ng logistic regression k\u1ebft h\u1ee3p v\u1edbi k\u1ef9 thu\u1eadt tf-idf, Library : pyvi(m\u1ed9t th\u01b0 vi\u1ec7n x\u1eed l\xfd ti\u1ebfng vi\u1ec7t), sklearn\\n\\nC\xe1c b\u01b0\u1edbc th\u1ef1c hi\u1ec7n :\\n    1.  Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u\\n    2.  Ti\u1ec1n x\u1eed l\xfd d\u1eef li\u1ec7u\\n    3.  Build model\\n    4.  Funny m\u1ed9t t\xed\\n\\n## Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u\\nD\u1eef li\u1ec7u text c\xf3 \u1edf kh\u1eafp m\u1ecdi n\u01a1i t\u1eeb facebook \u0111\u1ebfn c\xe1c website \u0111\xe2u \u0111\xe2u c\u0169ng c\xf3.M\xecnh s\u1ebd l\u1ea5y d\u1eef li\u1ec7u t\u1eeb trang tripnow.vn m\u1ed9t trang web con c\u1ee7a foody.vn chuy\xean v\u1ec1 \u0111\xe1nh gi\xe1 c\xe1c c\u1eeda h\xe0ng. \u0110\u1ec3 \u0111\u01a1n gi\u1ea3n m\xecnh ch\u1ec9 l\u1ea5y comment \u1edf c\xe1c c\u1eeda h\xe0ng \u1edf TP.hcm v\xe0 tr\xean m\u1ed7i comment \u0111\xf3 c\xf3 star thang \u0111o t\u1eeb 1-10 m\xecnh s\u1ebd l\u1ea5y n\xf3 l\xe0m c\u0103n c\u1ee9 \u0111\xe1nh gi\xe1 comment l\xe0 pos or neg.\\nB\u1eaft \u0111\u1ea7u chi\u1ebfn d\u1ecbch c\xe0 web n\xe0o (crawler). M\xecnh s\u1ebd s\u1eed d\u1ee5ng selenium \u0111\u1ec3 c\xe0 d\u1eef li\u1ec7u text.\\n\\n```python\\n#load thu vien\\nimport numpy as np\\nimport selenium\\nfrom selenium import webdriver\\nimport time\\nfrom selenium.webdriver.common.keys import Keys\\n#open list name\\nwith open(\\"name.txt\\") as f:\\n  names = f.read()\\nlist_name = names.split(\\"\\\\n\\")\\n#crawler data\\ndriver = webdriver.Chrome()\\npath = \\"https://www.tripnow.vn\\"\\ntexts = []\\nscores = []\\nfor name in range(len(list_name)):\\n  path_link = path + list_name[name] + \\"/binh-luan\\"\\n  driver.get(path_link)\\n  actions = webdriver.ActionChains(driver)\\n  count = 0\\n  while (count<30):\\n      load_more = driver.find_element_by_xpath(\\"//div/*[@ng-click=\'LoadMore()\']\\")\\n      actions.move_to_element(load_more).perform()\\n      load_more.click()\\n      driver.execute_script(\\"window.scrollTo(0, document.body.scrollHeight);\\")\\n      count += 1   \\n      time.sleep(3)\\n  text =  driver.find_elements_by_xpath(\\"//div/span[@ng-bind-html=\'Model.Description\']\\")\\n  score = driver.find_elements_by_xpath(\\"//li/div/div/div/span[@class=\'ng-binding\']\\")\\n\\n  for tx,sc in zip(text,score):\\n      comment = tx.text\\n      scoring = sc.text\\n      texts.append(comment)\\n      scores.append(scoring)\\n```\\nGi\u1ea3i th\xedch code m\u1ed9t t\xed :\\n* \u0110\u1ec3 tr\xe1nh code d\xe0i m\xecnh l\u01b0u c\xe1c t\xean c\u1eeda h\xe0ng \u1edf file name.txt\\n* M\u1ed7i c\u1eeda h\xe0ng c\xf3 r\u1ea5t nhi\u1ec1u comment (c\xf3 th\u1ec3 v\xe0i tr\u0103m) nh\u01b0ng m\xecnh ch\u1ec9 l\u1ea5y 300 comment \u1edf m\u1ed7i c\u1eeda h\xe0ng v\xec m\xe1y m\xecnh t\u01b0\u01a1ng \u0111\u1ed1i y\u1ebfu load nhi\u1ec1u m\xe1y ch\u1ea1y kh\xf4ng n\u1ed5i.\\n* Comment \u0111\u01b0\u1ee3c l\u1ea5y t\u1eeb //div/span[@ng-bind-html=\'Model.Description] v\xe0 l\u01b0u v\xe0o bi\u1ebfn texts\\n* Score \u0111\u01b0\u1ee3c l\u1ea5y t\u1eeb //li/div/div/div/span[@class=\'ng-binding\'] v\xe0 l\u01b0u v\xe0o bi\u1ebfn scores\\n* M\xecnh cho n\xf3 ch\u1ea1y t\u1ea7m 2 ti\u1ebfng thu \u0111\u01b0\u1ee3c t\u1ea7m 6000 comment v\xe0 \u0111\u01b0\u1ee3c l\u01b0u d\u01b0\u1edbi d\u1ea1ng text\\n  \\n\x3c!-- ![](https://images.viblo.asia/469fcc5f-6858-4c9e-a42a-642e165a49ed.jpg)\\n --\x3e\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src=\\"https://images.viblo.asia/469fcc5f-6858-4c9e-a42a-642e165a49ed.jpg\\" />\\n</center>\\n\\n\x3c!-- ![](https://images.viblo.asia/46576b53-7631-4664-9e27-c20530c92f7e.jpg) --\x3e\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src=\\"https://images.viblo.asia/46576b53-7631-4664-9e27-c20530c92f7e.jpg\\" />\\n</center>\\n\\n## Ti\u1ec1n x\u1eed l\xfd d\u1eef li\u1ec7u\\nTr\u01b0\u1edbc ti\xean ta t\xecm hi\u1ec3u k\u1ef9 thu\u1eadt TF-IDF n\xf3 l\xe0 vi\u1ebft t\u1eaft c\u1ee7a t\u1eeb Term frequency invert document frequency.N\xf3 l\xe0 m\u1ed9t k\u1ef9 thu\u1eadt feature extraction d\xf9ng trong text mining v\xe0 information retrieval. Tr\u01b0\u1edbc khi c\xf3 tf-idf ng\u01b0\u1eddi ta d\xf9ng one-hot-encoding \u0111\u1ec3 embedding words sang vector. Nh\u01b0ng k\u1ef9 thu\u1eadt n\xe0y g\u1eb7p m\u1ed9t s\u1ed1 h\u1ea1n ch\u1ebf l\xe0 :\\n* Nh\u1eefng t\u1eeb th\u01b0\u1eddng xuy\xean xu\u1ea5t hi\u1ec7n s\u1ebd kh\xf4ng c\xf3 nhi\u1ec1u th\xf4ng tin nh\u01b0ng v\u1eabn c\xf3 t\u1ec9 tr\u1ecdng(weight) ngang v\u1edbi c\xe1c t\u1eeb kh\xe1c.vd : stop word ch\u1eb3ng h\u1ea1n hay ch\xfang ta ph\xe2n t\xedch v\u1ec1qu\xe1n \u0103n n\xe0o \u0111\xf3 th\xec t\u1eeb \\"qu\xe1n \u0103n\\" xu\u1ea5t hi\u1ec7n \u1edf t\u1ea5t c\u1ea3 document.Ch\xfang ta c\u1ea7n gi\u1ea3m t\u1ec9 tr\u1ecdng v\u1ec1 m\u1eb7t th\xf4ng tin n\xf3 xu\u1ed1ng v\xec th\xf4ng tin kh\xf4ng mang nhi\u1ec1u gi\xe1 tr\u1ecb.\\n* Nh\u1eefng t\u1eeb hi\u1ebfm(rare word) or key word kh\xf4ng c\xf3 s\u1ef1 kh\xe1c bi\u1ec7t v\u1ec1 t\u1ec9 tr\u1ecdng th\xf4ng tin\\n\u0110\u1ec3 kh\u1eafc ph\u1ee5c h\u1ea1n ch\u1ebf n\xe0y tf-idf \u0111\xe3 ra \u0111\u1eddi.Tf-idf bao g\u1ed3m 2 th\xe0nh ph\u1ea7n l\xe0 tf(term frequency) v\xe0 idf(inverse document frequency).\\n\\n$$\\ntf(w,d) = \\\\frac{\\\\text{number of word w in document d}}{\\\\text{total word in document}}\\n$$\\n\\n* tf \u0111o l\u01b0\u1eddng t\u1ec9 tr\u1ecdng t\u1ea7n su\u1ea5t t\u1eeb w c\xf3 trong document d.V\xec document th\u01b0\u1eddng c\xf3 lenght kh\xe1c nhau n\xean \u0111\u1ec3 normalization ta chia n\xf3 cho number word trong document d.\\n  \\n$$\\nidf = tf* \\\\frac{N}{\\\\text{documnet in word w appear}}\\n$$\\n\\n* N l\xe0 t\u1ed5ng s\u1ed1 document trong dataset.T\u1ec9 s\u1ed1 $\\\\frac{N}{\\\\text{documnet in word w appear}}$  \u0111\u01b0\u1ee3c xem l\xe0 inverse document frequency. N\u1ebfu m\u1ed9t t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u \u1edf c\xe1c document th\xec t\u1ec9 s\u1ed1 n\xe0y s\u1ebd g\u1ea7n 1.V\xe0 ng\u01b0\u1ee3c l\u1ea1i m\u1ed9t t\u1eeb \xedt xu\u1ea5t hi\u1ec7n h\u01a1n t\u1ec9 s\u1ed1 n\xe0y s\u1ebd cao h\u01a1n 1. \u0110i\u1ec1u n\xe0y gi\xfap gi\u1ea3m t\u1ec9 tr\u1ecdng c\u1ee7a nh\u1eefng t\u1eeb th\u01b0\u1eddng xuy\xean su\u1ea5t hi\u1ec7n v\xe0 t\u0103ng t\u1ec9 tr\u1ecdng nh\u1eefng t\u1eeb \xedt xu\u1ea5t hi\u1ec7n trong document h\u01a1n (l\u01b0u \xfd N lu\xf4n l\u1edbn h\u01a1n ho\u1eb7c b\u1eb1ng documnet in word w appear).\\n* M\u1ed9t v\u1ea5n \u0111\u1ec1 l\xe0 khi N r\u1ea5t l\u1edbn m\xe0 documnet in word w appear  r\u1ea5t nh\u1ecf th\xec t\u1ec9 s\u1ed1 n\xe0y r\u1ea5t l\u01a1n cho n\xean l\xe0 ng\u01b0\u1eddi d\xf9ng log transform \u0111\u1ec3 gi\u1ea3m gi\xe1 tr\u1ecb t\u1ec9 s\u1ed1  $\\\\frac{N}{documnet in word w appear}$ r\xe1nh g\xe2y kh\xf3 kh\u0103n trong vi\u1ec7c t\xednh to\xe1n ( l\u01b0u \xfd log n\xf3 l\xe0m gi\u1ea3m gi\xe1 tr\u1ecb theo c\u1ea5p l\u0169y th\u1eeba). Khi \u0111\xf3 c\xf4ng th\u1ee9c idf cu\u1ed1i c\xf9ng s\u1ebd l\xe0 :\\n  \\n$$\\nidf = tf* log(\\\\frac{N}{\\\\text{documnet in word w appear}})\\n$$\\n\\n* V\xed d\u1ee5 : M\u1ed9t document 100 word ch\u1ee9a word cat 3 l\u1ea7n.$tf = \\\\frac{3}{100} = 0.03$ . Gi\u1ea3 s\u1eed c\xf3 10000 document m\xe0 word cat xu\u1ea5t hi\u1ec7n trong 1000 document. $idf(cat) = 0.03* log(\\\\frac{10000}{1000}) = 0.06$.\\n*  Ta b\u1eaft \u0111\u1ea7u x\u1eed l\xfd d\u1eef li\u1ec7u. \u0110\u1ea7u ti\xean l\xe0 load d\u1eef li\u1ec7u\\n```python\\n#import library\\nimport numpy as np\\nimport pandas as pd\\nfrom pyvi import ViTokenizer\\nimport glob\\nfrom collections import Counter\\nfrom string import punctuation\\n#load data\\npaths = glob.glob(\\"./comment/*.txt\\")\\ncomments = []\\nfor path in paths :\\n  with open(path,encoding=\\"utf-8\\") as file:\\n      text= file.read()\\n      text_lower = text.lower()\\n      text_token = ViTokenizer.tokenize(text_lower)\\n      comments.append(text_token)\\n  file.close()\\n```\\nD\u1eef li\u1ec7u s\u1ebd \u0111\u01b0\u1ee3c t\xe1ch t\u1eeb b\u1eb1ng ViTokenizer.tokenize sau \u0111\xf3 \u0111\u01b0\u1ee3c l\u01b0u d\u01b0\u1edbi bi\u1ebfn comment.\\n```python\\nstop_word = []\\nwith open(\\"stop_word.txt\\",encoding=\\"utf-8\\") as f :\\n  text = f.read()\\n  for word in text.split() :\\n      stop_word.append(word)\\n  f.close()\\n punc = list(punctuation)\\nstop_word = stop_word + punc\\nprint(stop_word)\\n```\\nTi\u1ebfp theo l\xe0 x\xe2y d\u1ef1ng stop_word v\xe0 punctuation\\n\x3c!-- ![](https://images.viblo.asia/15136ee2-802b-4526-a8ac-b1ede543a4bf.jpg)\\n --\x3e\\n<center>\\n   <img width=\\"500\\" height=\\"300\\" src=\\"https://images.viblo.asia/15136ee2-802b-4526-a8ac-b1ede543a4bf.jpg\\" />\\n</center>\\n\\n```python\\nsentences = []\\nfor comment in comments:\\n  sent = []\\n  for word in comment.split(\\" \\") :\\n          if (word not in stop_word) :\\n              if (\\"_\\" in word) or (word.isalpha() == True):\\n                  sent.append(word)\\n  sentences.append(\\" \\".join(sent)) \\n```\\nL\xe0m s\u1ea1ch data lo\u1ea1i b\u1ecf stop_word , nh\u1eefng t\u1eeb kh\xf4ng ph\u1ea3i alphabet \u0111\u01b0\u1ee3c remove\\nTi\u1ebfp theo ta embedding text th\xe0nh vector s\u1eed d\u1ee5ng if-idf v\u1edbi function TfidfVectorizer trong `sklearn\'\\n```python\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\ntf = TfidfVectorizer(min_df=5,max_df= 0.8,max_features=3000,sublinear_tf=True)\\ntf.fit(sentences)\\nX = tf.transform(sentences)\\n```\\nH\xe0m TfidfVectorizer c\xf3 c\xe1c tham s\u1ed1 ch\xfang ta c\u1ea7n ch\xfa \xfd l\xe0:\\n* min_df : lo\u1ea1i b\u1ecf nh\u1eefng t\u1eeb n\xe0o t\u1eeb vocabulary c\xf3 t\u1ea7n su\u1ea5t su\u1ea5t hi\u1ec7n nh\u1ecf h\u01a1n min_df ( t\xednh theo count)\\n* max_df \\" lo\u1ea1i b\u1ecf nh\u1eefng t\u1eeb n\xe0o t\u1eeb vocabulary c\xf3 t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n l\u1edbn h\u01a1n max_df ( t\xednh theo %)\\n* sublinear_tf: Scale term frequency b\u1eb1ng logarithmic scale\\n* stop_words lo\u1ea1i b\u1ecf stop word, ch\xfang ta \u0111\xe3 l\xe0m tr\u01b0\u1edbc \u0111\xf3 n\xean kh\xf4ng c\u1ea7n tham s\u1ed1 n\xe0y\\n* max_features l\u1ef1a ch\u1ecdn s\u1ed1 character v\xe0o vocabulary\\n* vocabulary n\u1ebfu ch\xfang ta \u0111\xe3 x\xe2y d\u1ef1ng vocabulury tr\u01b0\u1edbc \u0111\xf3 th\xec kh\xf4ng c\u1ea7n max_features\\n* token_pattern l\xe0 regular expression \u0111\u1ec3 ch\u1ecdn word v\xe0o vocabulary\\n\\nX\u1eed l\xfd label : ta s\u1ebd \u0111\u01b0a ra m\u1ed9t threshold \u0111\u1ec3 quy\u1ebft \u0111\u1ecbnh 1 comment l\xe0 pos hay neg. Ta ch\u1ecdn threshold l\xe0 6, khi score < 6 th\xec comment \u0111\u01b0\u1ee3c xem l\xe0 neg v\xe0 ng\u01b0\u1ee3c l\u1ea1i l\xe0 pos\\n```python\\nfrom sklearn.preprocessing import Binarizer\\nbinaray = Binarizer(threshold=6)\\ny = binaray.fit_transform(y_score)\\ny = np.array(y).flatten()\\n```\\nNh\u1eadn x\xe9t d\u1eef li\u1ec7u c\u1ee7a ch\xfang ta l\xe0 kh\xf4ng t\u1ed1t l\u1eafm v\xec s\u1ed1 neg = 691 tr\xean t\u1ed5ng s\u1ed1 comment l\xe0 6000. Nh\u01b0 v\u1eady ch\u1ec9 c\xf3 10% l\xe0 neg khi \u0111\xf3 d\u1eef li\u1ec7u s\u1ebd unbalance . C\u0169ng d\u1ec5 hi\u1ec3u v\xec \u0111a s\u1ed1 qu\xe1n \u0103n tr\xean trang tripnow.vn l\xe0 ngon ho\u1eb7c l\xe0 foody.vn thu\xea ng\u01b0\u1eddi comment ch\u1eb3ng h\u1ea1n. V\xec c\u1ea3 2 label neg v\xe0 pos c\xf3 th\u1ec3 xem l\xe0 quan tr\u1ecdng nh\u01b0 nhau. kh\xf4ng c\xf3 bi\u1ebfn n\xe0o tr\u1ed9i h\u01a1n n\xean model c\u1ee7a ch\xfang ta tr\xean data n\xe0y c\xf3 l\u1ebd s\u1ebd kh\xf4ng t\u1ed1t. H\u01a1n n\u1eefa score n\xe0y ch\u1ec9 mang t\xednh ch\u1ea5t t\u01b0\u1ee3ng tr\u01b0ng n\xean kh\xf4ng d\xe1m ch\u1eafc n\xf3 l\xe0 ti\xeau ch\xed ph\xe2n lo\u1ea1i neg v\xe0 pos t\u1ed1t.\\nChia d\u1eef li\u1ec7u \u0111\u1ec3 training v\xe0 testing t\u1ef7 l\u1ec7 test l\xe0 30%\\n```python\\nfrom sklearn.model_selection import train_test_split\\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=10,shuffle=True)\\n```\\n## Build model\\nTa d\xf9ng logistic regression \u0111\u1ec3 training model.\\n```python\\nmodel = LogisticRegression()\\nmodel.fit(X_train,y_train)\\ny_pre = model.predict(X_test)\\nprint(classification_report(y_test,y_pre))\\n```\\n\x3c!-- ![](https://images.viblo.asia/69381bdf-fcee-4c52-8643-b5b1792adc83.jpg) --\x3e\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src=\\"https://images.viblo.asia/69381bdf-fcee-4c52-8643-b5b1792adc83.jpg\\" />\\n</center>\\n\\nAccuracy l\xe0 91% nh\u01b0ng recall ch\u1ec9 c\xf3 23% t\u01b0\u01a1ng \u0111\u1ed1i th\u1ea5p. C\xf3 ngh\u0129a l\xe0 trong 191 comment neg ta ch\u1ec9 d\u1ef1 \u0111o\xe1n ch\xednh x\xe1c kho\u1ea3ng 44 comment\\nB\xe2y gi\u1edd ta th\u1eed predict m\u1ed9t s\u1ed1 c\xe2u.\\n```python\\ntext =[[\\"qu\xe1n n\u1ea5u d\u1edf qu\xe1\\"],[\\"\u0111\u1ed3 \u0103n b\xecnh_th\u01b0\u1eddng\\"],[\\"qu\xe1n n\u1ea5u ngon\\"]]\\nfor i in text:\\n  test = tf.transform(i)\\n  print(model.predict(test))\\n==>> [0] [1] [1]\\n```\\nKh\xf4ng  t\u1ed1t l\u1eafm ha do d\u1eef li\u1ec7u unbalance qu\xe1 v\u1edbi l\u1ea1i score ch\u01b0a chu\u1ea9n \u0111\u1ec3 ph\xe2n lo\u1ea1i\\n## Funny m\u1ed9t t\xed\\nTa s\u1ebd xem nh\u1eefng t\u1eeb n\xe0o \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng nhi\u1ec1u nh\u1ea5t trong document v\xe0 x\xe2y d\u1ef1ng wordcloud c\u1ee7a n\xf3.\\n```python\\nimport wordcloud\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\ncloud = np.array(sentences).flatten()\\nplt.figure(figsize=(20,10))\\nword_cloud = wordcloud.WordCloud(max_words=100,background_color =\\"black\\",\\n                               width=2000,height=1000,mode=\\"RGB\\").generate(str(cloud))\\nplt.axis(\\"off\\")\\nplt.imshow(word_cloud)\\n```\\n\x3c!-- ![](https://images.viblo.asia/fb9394db-32ea-49dc-9580-c76103f50f32.jpg)\\n --\x3e\\n\\n<center>\\n   <img width=\\"800\\" height=\\"400\\" src=\\"https://images.viblo.asia/fb9394db-32ea-49dc-9580-c76103f50f32.jpg\\" />\\n</center>"},{"id":"Nh\u1eadn-d\u1ea1ng-ch\u1eef-s\u1ed1-vi\u1ebft-tay","metadata":{"permalink":"/blog/Nh\u1eadn-d\u1ea1ng-ch\u1eef-s\u1ed1-vi\u1ebft-tay","editUrl":"https://github.com/ThorPham/blog/2018-06-4-Nh\u1eadn-d\u1ea1ng-ch\u1eef-s\u1ed1-vi\u1ebft-tay/index.md","source":"@site/blog/2018-06-4-Nh\u1eadn-d\u1ea1ng-ch\u1eef-s\u1ed1-vi\u1ebft-tay/index.md","title":"Nh\u1eadn d\u1ea1ng ch\u1eef s\u1ed1 vi\u1ebft tay v\u1edbi sklearn v\xe0 opencv","description":"C\xf3 r\u1ea5t nhi\u1ec1u thu\u1eadt to\xe1n \u0111\u1ec3 nh\u1eadn di\u1ec7n ch\u1eef s\u1ed1 vi\u1ebft tay (hand writen digit) nh\u01b0 neural network,CNN v\u1edbi \u0111\u1ed9 ch\xednh x\xe1c r\u1ea5t cao l\xean t\u1edbi 99%. Nh\u01b0ng c\u0169ng kh\xf4ng n\xean ph\u1ee7 nh\u1eadn c\xe1c thu\u1eadt to\xe1n \xe1p d\u1ee5ng theo ki\u1ec3u truy\u1ec1n th\u1ed1ng tuy v\u1eabn c\xf3 nhi\u1ec1u \u01b0u \u0111i\u1ec3m l\xe0 \u0111\u01a1n gi\u1ea3n v\xe0 chi ph\xed t\xednh to\xe1n th\xe2p.Trong b\xe0i vi\u1ebft n\xe0y ch\xfang ta c\xf9ng t\xecm hi\u1ec3u Hog(histogram of oriented gradient) v\xe0 svm( support vector machine) \u0111\u1ec3 nh\u1eadn d\u1ea1ng ch\u1eef s\u1ed1 vi\u1ebft tay tr\xean b\u1ed9 d\u1eef li\u1ec7u MNIST.","date":"2018-06-04T00:00:00.000Z","formattedDate":"June 4, 2018","tags":[{"label":"opencv","permalink":"/blog/tags/opencv"},{"label":"sklearn","permalink":"/blog/tags/sklearn"}],"readingTime":3.135,"truncated":true,"authors":[{"name":"Thorpham","title":"Deep learning enthusiast","url":"https://github.com/ThorPham","imageURL":"https://github.com/ThorPham.png","key":"thorpham"}],"frontMatter":{"slug":"Nh\u1eadn-d\u1ea1ng-ch\u1eef-s\u1ed1-vi\u1ebft-tay","title":"Nh\u1eadn d\u1ea1ng ch\u1eef s\u1ed1 vi\u1ebft tay v\u1edbi sklearn v\xe0 opencv","authors":"thorpham","tags":["opencv","sklearn"]},"prevItem":{"title":"Sentiment Analysis s\u1eed d\u1ee5ng Tf-Idf \xe1p d\u1ee5ng cho ng\xf4n ng\u1eef ti\u1ebfng vi\u1ec7t","permalink":"/blog/Sentiment-Analysis-s\u1eed-d\u1ee5ng-Tf-Idf"},"nextItem":{"title":"T\xecm hi\u1ec3u v\u1ec1 th\u01b0 vi\u1ec7n keras trong deep learning","permalink":"/blog/T\xecm-hi\u1ec3u-v\u1ec1-th\u01b0-vi\u1ec7n-keras-trong-deep-learning"}},"content":"*C\xf3 r\u1ea5t nhi\u1ec1u thu\u1eadt to\xe1n \u0111\u1ec3 nh\u1eadn di\u1ec7n ch\u1eef s\u1ed1 vi\u1ebft tay (hand writen digit) nh\u01b0 neural network,CNN v\u1edbi \u0111\u1ed9 ch\xednh x\xe1c r\u1ea5t cao l\xean t\u1edbi 99%. Nh\u01b0ng c\u0169ng kh\xf4ng n\xean ph\u1ee7 nh\u1eadn c\xe1c thu\u1eadt to\xe1n \xe1p d\u1ee5ng theo ki\u1ec3u truy\u1ec1n th\u1ed1ng tuy v\u1eabn c\xf3 nhi\u1ec1u \u01b0u \u0111i\u1ec3m l\xe0 \u0111\u01a1n gi\u1ea3n v\xe0 chi ph\xed t\xednh to\xe1n th\xe2p.Trong b\xe0i vi\u1ebft n\xe0y ch\xfang ta c\xf9ng t\xecm hi\u1ec3u Hog(histogram of oriented gradient) v\xe0 svm( support vector machine) \u0111\u1ec3 nh\u1eadn d\u1ea1ng ch\u1eef s\u1ed1 vi\u1ebft tay tr\xean b\u1ed9 d\u1eef li\u1ec7u MNIST.*\\n\x3c!--truncate--\x3e\\nC\u1ea5u tr\xfac b\xe0i vi\u1ebft:\\n  - X\xe2y d\u1ef1ng model nh\u1eadn di\u1ec7n digit.\\n  - Predict tr\xean \u1ea3nh c\xf3 nhi\u1ec1u ditgit\\n## X\xe2y d\u1ef1ng model nh\u1eadn di\u1ec7n digit.\\nD\u1eef li\u1ec7u m\xe0 ch\xfang ta training model l\xe0 mnist. \u0110\xe2y c\xf3 th\u1ec3 coi l\xe0 \\"hello word\\" trong machine learning. Tr\u01b0\u1edbc h\u1ebft ta t\xecm hi\u1ec3u s\u01a1 qua v\u1ec1 d\u1eef li\u1ec7u, mnist l\xe0 t\u1eadp h\u1ee3p c\xe1c \u1ea3nh x\xe1m v\u1ec1 digit c\xf3 chi\u1ec1u l\xe0 28x28 bao g\u1ed3m 70.000 ng\xe0n \u1ea3nh, trong \u0111\xf3 c\xf3 60.000 \u1ea3nh \u0111\u1ec3 training v\xe0 10.000 \u1ea3nh \u0111\u1ec3 testing. \xdd t\u01b0\u1edfng c\u1ee7a model l\xe0 d\xf9ng HOG(histogram oriented of gradient) \u0111\u1ec3 extract feature( c\xe1c b\u1ea1n c\xf3 th\u1ec3 coi c\xe1c b\xe0i tr\u01b0\u1edbc \u0111\u1ec3 bi\u1ebft th\xeam hog l\xe0 g\xec). Sau khi c\xf3 feature ta s\u1ebd \u0111\u01b0a v\xe0o model SVM \u0111\u1ec3 ph\xe2n lo\u1ea1i. Cu\u1ed1i c\xf9ng d\xf9ng opencv \u0111\u1ebf segmentation digit v\xe0 d\xf9ng model ch\xfang ta v\u1eeba build \u0111\u1ec3 predict. B\u1eaft tay v\xe0o model n\xe0o.\\n\\n\u0110\u1ea7u ti\xean load dataset v\xe0 load c\xe1c th\u01b0 vi\u1ec7n c\u1ea7n d\xf9ng:\\n```py\\nimport cv2\\nimport numpy as np\\nfrom skimage.feature import hog\\nfrom sklearn.svm import LinearSVC\\nfrom keras.datasets import mnist\\nfrom sklearn.metrics import accuracy_score\\n#load data\\n(X_train,y_train),(X_test,y_test) = mnist.load_data()\\n```\\nTi\u1ebfp theo ta s\u1ebd t\xednh hog. Ta d\xf9ng orientations=9,pixels_per_cell=(14,14),cells_per_block=(1,1).\\n```py\\n#cho x_train\\nX_train_feature = []\\nfor i in range(len(X_train)):\\n    feature = hog(X_train[i],orientations=9,pixels_per_cell=(14,14),cells_per_block=(1,1),block_norm=\\"L2\\")\\n    X_train_feature.append(feature)\\nX_train_feature = np.array(X_train_feature,dtype = np.float32)\\n\\n#cho x_test\\nX_test_feature = []\\nfor i in range(len(X_test)):\\n    feature = hog(X_test[i],orientations=9,pixels_per_cell=(14,14),cells_per_block=(1,1),block_norm=\\"L2\\")\\n    X_test_feature.append(feature)\\nX_test_feature = np.array(X_test_feature,dtype=np.float32)\\n```\\nTi\u1ebfp theo ta build model v\xe0o predict\\n```py\\nmodel = LinearSVC(C=10)\\nmodel.fit(X_train_feature,y_train)\\ny_pre = model.predict(X_test_feature)\\nprint(accuracy_score(y_test,y_pre))\\n```\\nAccuracy l\xe0 88% kh\xf4ng cao l\u1eafm. Ta c\xf3 th\u1ec3 \u0111i\u1ec1u ch\u1ec9nh c\xe1c tham s\u1ed1 \u0111\u1ec3 t\u0103ng \u0111\u1ed9 ch\xednh x\xe1c c\u1ee7a model.\\n## Predict tr\xean \u1ea3nh c\xf3 nhi\u1ec1u ditgit\\n\u0110\u1ebfn b\u01b0\u1edbc n\xe0y ta s\u1ebd d\xf9ng opencv, \u0111\u1ea7u ti\xean ta s\u1ebd x\u1eed l\xfd \u1ea3nh v\xe0 t\xecm contours c\u1ee7a digit tr\xean image.\\nImage n\xe0y l\u01b0\u1ee3m tr\xean m\u1ea1ng ha.\\n\\n<center>\\n   <img width=\\"600\\" height=\\"200\\" src={require(\'./digit.jpg\').default} />\\n</center>\\n\\n```py\\nimage = cv2.imread(\\"digit.jpg\\")\\nim_gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\\nim_blur = cv2.GaussianBlur(im_gray,(5,5),0)\\nim,thre = cv2.threshold(im_blur,90,255,cv2.THRESH_BINARY_INV)\\n_,contours,hierachy = cv2.findContours(thre,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\\nrects = [cv2.boundingRect(cnt) for cnt in contours]\\n```\\nG\u1ea3i th\xedch code m\u1ed9t t\xed hem:\\n  - \u0110\u1ea7u ti\xean convert color sang gray color\\n  - Ti\u1ebfp theo gi\u1ea3m nhi\u1ec5u b\u1eb1ng Gaussian( t\xf9y thu\u1ed9c image m\xe0 ta x\u1eed l\xfd kh\xe1c nhau)\\n  - Ti\u1ebfp theo d\xf9ng threshold chuy\u1ec3n v\u1ec1 \u1ea3nh binary\\n  - Cu\u1ed1i c\xf9ng l\xe0 t\xecm contour v\xe0 v\u1ebd bouding box \\nSau \u0111\xf3 predict ditgit c\u1ee7a m\u1ed7i box.\\n\\n```py\\nfor i in contours:\\n    (x,y,w,h) = cv2.boundingRect(i)\\n    cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),3)\\n    roi = thre[y:y+h,x:x+w]\\n    roi = np.pad(roi,(20,20),\'constant\',constant_values=(0,0))\\n    roi = cv2.resize(roi, (28, 28), interpolation=cv2.INTER_AREA)\\n    roi = cv2.dilate(roi, (3, 3))\\n    \\n    # Calculate the HOG features\\n    roi_hog_fd = hog(roi, orientations=9, pixels_per_cell=(14, 14), cells_per_block=(1, 1),block_norm=\\"L2\\")\\n    nbr = model.predict(np.array([roi_hog_fd], np.float32))\\n    cv2.putText(image, str(int(nbr[0])), (x, y),cv2.FONT_HERSHEY_DUPLEX, 2, (0, 255, 255), 3)\\n    cv2.imshow(\\"image\\",image)\\ncv2.imwrite(\\"image_pand.jpg\\",image)\\ncv2.waitKey()\\ncv2.destroyAllWindows()\\n```\\n\x3c!-- ![digit_predict](./ditgit-predict.jpg) --\x3e\\n\\n<center>\\n   <img width=\\"600\\" height=\\"200\\" src={require(\'./ditgit-predict.jpg\').default} />\\n</center>\\n\\nM\u1ed9t s\u1ed1 l\u01b0u \xfd l\xe0 : Ta n\xean padding cho m\u1ed7i digit m\u1ed9t kho\u1ea3ng n\xe0o \u0111\xf3 tr\xe1nh tr\u01b0\u1eddng h\u1ee3p digit ko c\xf3 background s\u1ebd kh\xf3 predict. Tuy thu\u1eadt to\xe1n hog + svm n\xe0y c\xf3 \u0111\u1ed9 ch\xednh x\xe1c kh\xf4ng cao b\u1eb1ng c\xe1c thu\u1eadt to\xe1n trong deep learning nh\u01b0ng n\xf3 v\u1eabn t\u1ea1m ch\u1ea5p nh\u1eadn \u0111\u01b0\u1ee3c.M\xecnh vi\u1ebft b\xe0i n\xe0y \u0111\u1ec3 m\u1ecdi ng\u01b0\u1eddi h\xecnh dung \u0111\u01b0\u1ee3c c\xe1c b\u01b0\u1edbc th\u1ef1c hi\u1ec7n thu\u1eadt to\xe1n v\xe0 c\xe1c predict khi detection multi digit.\\n\\nTham Kh\u1ea3o : \\n* http://hanzratech.in/\\n* https://pyimagesearch.com\\n* http://learnopencv.com"},{"id":"T\xecm-hi\u1ec3u-v\u1ec1-th\u01b0-vi\u1ec7n-keras-trong-deep-learning","metadata":{"permalink":"/blog/T\xecm-hi\u1ec3u-v\u1ec1-th\u01b0-vi\u1ec7n-keras-trong-deep-learning","editUrl":"https://github.com/ThorPham/blog/2018-5-25-T\xecm-hi\u1ec3u-v\u1ec1-th\u01b0-vi\u1ec7n-keras-trong-deep-learning/index.md","source":"@site/blog/2018-5-25-T\xecm-hi\u1ec3u-v\u1ec1-th\u01b0-vi\u1ec7n-keras-trong-deep-learning/index.md","title":"T\xecm hi\u1ec3u v\u1ec1 th\u01b0 vi\u1ec7n keras trong deep learning","description":"Keras l\xe0 m\u1ed9t library \u0111\u01b0\u1ee3c ph\xe1t tri\u1ec3n v\xe0o n\u0103m 2015 b\u1edfi Fran\xe7ois Chollet, l\xe0 m\u1ed9t k\u1ef9 s\u01b0 nghi\xean c\u1ee9u deep learning t\u1ea1i google. N\xf3 l\xe0 m\u1ed9t open source cho neural network \u0111\u01b0\u1ee3c vi\u1ebft b\u1edfi ng\xf4n ng\u1eef python. keras l\xe0 m\u1ed9t API b\u1eadc cao c\xf3 th\u1ec3 s\u1eed d\u1ee5ng chung v\u1edbi c\xe1c th\u01b0 vi\u1ec7n deep learning n\u1ed5i ti\u1ebfng nh\u01b0 tensorflow(\u0111\u01b0\u1ee3c ph\xe1t tri\u1ec3n b\u1edfi gg), CNTK(\u0111\u01b0\u1ee3c ph\xe1t tri\u1ec3n b\u1edfi microsoft),theano(ng\u01b0\u1eddi ph\xe1t tri\u1ec3n ch\xednh Yoshua Bengio). keras c\xf3 m\u1ed9t s\u1ed1 \u01b0u \u0111i\u1ec3m nh\u01b0 :","date":"2018-05-25T00:00:00.000Z","formattedDate":"May 25, 2018","tags":[{"label":"python","permalink":"/blog/tags/python"},{"label":"Keras","permalink":"/blog/tags/keras"}],"readingTime":9.31,"truncated":true,"authors":[{"name":"Thorpham","title":"Deep learning enthusiast","url":"https://github.com/ThorPham","imageURL":"https://github.com/ThorPham.png","key":"thorpham"}],"frontMatter":{"slug":"T\xecm-hi\u1ec3u-v\u1ec1-th\u01b0-vi\u1ec7n-keras-trong-deep-learning","title":"T\xecm hi\u1ec3u v\u1ec1 th\u01b0 vi\u1ec7n keras trong deep learning","authors":"thorpham","tags":["python","Keras"]},"prevItem":{"title":"Nh\u1eadn d\u1ea1ng ch\u1eef s\u1ed1 vi\u1ebft tay v\u1edbi sklearn v\xe0 opencv","permalink":"/blog/Nh\u1eadn-d\u1ea1ng-ch\u1eef-s\u1ed1-vi\u1ebft-tay"},"nextItem":{"title":"Feature extraction trong computer vision","permalink":"/blog/Feature-extraction-trong-computer-vision"}},"content":"Keras l\xe0 m\u1ed9t library \u0111\u01b0\u1ee3c ph\xe1t tri\u1ec3n v\xe0o n\u0103m 2015 b\u1edfi Fran\xe7ois Chollet, l\xe0 m\u1ed9t k\u1ef9 s\u01b0 nghi\xean c\u1ee9u deep learning t\u1ea1i google. N\xf3 l\xe0 m\u1ed9t open source cho neural network \u0111\u01b0\u1ee3c vi\u1ebft b\u1edfi ng\xf4n ng\u1eef python. keras l\xe0 m\u1ed9t API b\u1eadc cao c\xf3 th\u1ec3 s\u1eed d\u1ee5ng chung v\u1edbi c\xe1c th\u01b0 vi\u1ec7n deep learning n\u1ed5i ti\u1ebfng nh\u01b0 tensorflow(\u0111\u01b0\u1ee3c ph\xe1t tri\u1ec3n b\u1edfi gg), CNTK(\u0111\u01b0\u1ee3c ph\xe1t tri\u1ec3n b\u1edfi microsoft),theano(ng\u01b0\u1eddi ph\xe1t tri\u1ec3n ch\xednh Yoshua Bengio). keras c\xf3 m\u1ed9t s\u1ed1 \u01b0u \u0111i\u1ec3m nh\u01b0 :\\n* D\u1ec5 s\u1eed d\u1ee5ng,x\xe2y d\u1ef1ng model nhanh.\\n* C\xf3 th\u1ec3 run tr\xean c\u1ea3 cpu v\xe0 gpu\\n* H\u1ed7 tr\u1ee3 x\xe2y d\u1ef1ng CNN , RNN v\xe0 c\xf3 th\u1ec3 k\u1ebft h\u1ee3p c\u1ea3 2.*\\n\x3c!--truncate--\x3e\\n## C\xe1ch c\xe0i \u0111\u1eb7t :\\nTr\u01b0\u1edbc khi c\xe0i \u0111\u1eb7t keras b\u1ea1n ph\u1ea3i c\xe0i \u0111\u1eb7t m\u1ed9t trong s\u1ed1 c\xe1c th\u01b0 vi\u1ec7n sau tensorflow,CNTK,theano. Sau \u0111\xf3 b\u1ea1n c\xf3 th\u1ec3 c\xe0i \u0111\u1eb7t b\u1eb1ng 1 s\u1ed1 l\u1ec7nh sau \u0111\u1ed1i v\u1edbi window:\\n* pip install keras\\n* conda install keras\\n  \\n## T\xecm hi\u1ec3u c\u1ea5u tr\xfac c\u1ee7a Keras\\nC\u1ea5u tr\xfac c\u1ee7a keras ch\xfang ta c\xf3 th\u1ec3 chia ra th\xe0nh 3 ph\u1ea7n ch\xednh :\\n\x3c!-- ![keras0](/assets/images/keras0.jpg) --\x3e\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./keras0.jpg\').default} />\\n</center>\\n\\n### \u0110\u1ea7u ti\u1ec1n l\xe0 c\xe1c module d\xf9ng \u0111\u1ec3 x\xe2y d\u1ef1ng b\u1ed9 x\u01b0\u01a1ng cho model :\\n   \\n\x3c!-- ![keras1](/assets/images/keras1.jpg) --\x3e\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./keras1.jpg\').default} />\\n</center>\\n\\n\u0110\u1ea7u ti\xean ta t\xecm hi\u1ec3u sub-module : Models trong keras. \u0110\u1ec3 kh\u1edfi t\u1ea1o m\u1ed9t model trong keras ta c\xf3 th\u1ec3 d\xf9ng 2 c\xe1ch:\\n1. C\xe1ch 1 : Th\xf4ng qua  Sequential nh\u01b0 v\xed d\u1ee5 d\u01b0\u1edbi. Ch\xfang ta kh\u1edfi t\u1ea1o model b\u1eb1ng `Sequential` sau \u0111\xf3 d\xf9ng method add \u0111\u1ec3 th\xeam c\xe1c layer.\\n```py\\nimport numpy as np\\nfrom keras.models import Sequential,Model\\n\\nmodel = Sequential()\\nmodel.add(Conv2D(32, kernel_size=(3, 3),\\n                 activation=\'relu\',\\n                 input_shape=input_shape))\\nmodel.add(Flatten())\\nmodel.add(Dense(128, activation=\'relu\'))\\nmodel.add(Dense(num_classes, activation=\'softmax\'))\\n\\nmodel.compile(loss=keras.losses.categorical_crossentropy,\\n              optimizer=keras.optimizers.Adadelta(),\\n              metrics=[\'accuracy\'])\\n\\nmodel.fit(x_train, y_train,\\n          batch_size=batch_size,\\n          epochs=epochs,\\n          verbose=1,\\n          validation_data=(x_test, y_test))\\n```\\n\\n2. C\xe1ch th\u1ee9 2 \u0111\u1ec3 kh\u1edfi t\u1ea1o model l\xe0 d\xf9ng function API . Nh\u01b0 v\xed d\u1ee5 d\u01b0\u1edbi\\n```py\\nfrom keras.models import Model\\nfrom keras.layers import Input, Dense\\n\\na = Input(shape=(32,))\\nb = Dense(32)(a)\\nmodel = Model(inputs=a, outputs=b)\\n```\\n\\n* N\xf3 c\u0169ng t\u01b0\u01a1ng t\u1ef1 nh\u01b0 computation graph, ch\xfang ta xem input c\u0169ng l\xe0 m\u1ed9t layer sau \u0111\xf3 build t\u1eeb input t\u1edbi output sau \u0111\xf3 k\u1ebft h\u1ee3p l\u1ea1i b\u1eb1ng h\xe0m  `Model`. \u01afu \u0111i\u1ec3m c\u1ee7a ph\u01b0\u01a1ng ph\xe1p n\xe0y c\xf3 th\u1ec3 t\xf9y bi\u1ebfn nhi\u1ec1u h\u01a1n,gi\xfap ta x\xe2y d\u1ef1ng c\xe1c model ph\u1ee9c t\u1ea1p nhi\u1ec1u input v\xe0 output.\\n* Khi ch\xfang ta kh\u1edfi t\u1ea1o m\u1ed9t model th\xec c\xf3 c\xe1c method ta c\u1ea7n l\u01b0u \xfd l\xe0 :\\n* `compile` : Sau khi build model xong th\xec compile n\xf3 c\xf3 t\xe1c d\u1ee5ng bi\xean t\u1eadp l\u1ea1i to\xe0n b\u1ed9 model c\u1ee7a ch\xfang ta \u0111\xe3 build. \u1ede \u0111\xe2y ch\xfang ta c\xf3 th\u1ec3 ch\u1ecdn c\xe1c tham s\u1ed1 \u0111\u1ec3 training model nh\u01b0 : thu\u1eadt to\xe1n training th\xf4ng qua tham s\u1ed1 `optimizer`, function loss c\u1ee7a model ch\xfang ta c\xf3 th\u1ec3 s\u1eed d\u1ee5ng m\u1eb7c \u0111\u1ecbnh ho\u1eb7c t\u1ef1 build th\xf4ng qua tham s\u1ed1 `loss`, ch\u1ecdn metrics hi\u1ec7n th\u1ecb khi model \u0111\u01b0\u1ee3c training\\n* `summary` method n\xe0y gi\xfap ch\xfang ta t\u1ed5ng h\u1ee3p l\u1ea1i model xem model c\xf3 bao nhi\xeau layer, t\u1ed5ng s\u1ed1 tham s\u1ed1 bao nhi\xeau,shape c\u1ee7a m\u1ed7i layer..\\n* `fit` d\xf9ng \u0111\u1ec3 \u0111\u01b0a data v\xe0o training \u0111\u1ec3 t\xecm tham s\u1ed1 model(t\u01b0\u01a1ng t\u1ef1 nh\u01b0 sklearn)\\n* `predict` d\xf9ng \u0111\u1ec3 predict c\xe1c new instance\\n* `evaluate` \u0111\u1ec3 t\xednh to\xe1n \u0111\u1ed9 ch\xednh x\xe1c c\u1ee7a model\\n* `history` d\xf9ng \u0111\u1ec3 xem accuracy,loss qua t\u1eebng epochs . Th\u01b0\u1eddng d\xf9ng v\u1edbi matplotlib \u0111\u1ec3 v\u1ebd chart.\\n  \\nTi\u1ebfp theo ch\xfang ta t\xecm hi\u1ec3u \u0111\xean sub-module Layers : N\xf3 ch\u1ee9a c\xe1c layers chuy\xean d\u1ee5ng \u0111\u1ec3 ta build c\xe1c model nh\u01b0 CNN,RNN,GANs..C\xf3 r\u1ea5t nhi\u1ec1u layers n\xean ta ch\u1ec9 quan t\xe2m \u0111\u1ebfn m\u1ed9t s\u1ed1 layer th\u01b0\u1eddng s\u1eed d\u1ee5ng.\\n\x3c!-- ![keras2](/assets/images/keras2.jpg) --\x3e\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./keras2.jpg\').default} />\\n</center>\\n\\nCore layer : ch\u1ee9a c\xe1c layer m\xe0 h\u1ea7u nh\u01b0 model n\xe0o c\u0169ng s\u1eed d\u1ee5ng \u0111\u1ebfn n\xf3.\\n* `Dense` layer n\xe0y s\u1eed d\u1ee5ng nh\u01b0 m\u1ed9t layer neural network b\xecnh th\u01b0\u1eddng. C\xe1c tham s\u1ed1 quan t\xe2m.\\n  * `units` chi\u1ec1u output\\n  * `activation` d\xf9ng \u0111\u1ec3 ch\u1ecdn activation.\\n  * `input_dim` chi\u1ec1u input n\u1ebfu l\xe0 layer \u0111\u1ea7u ti\xean\\n  * `use_bias` c\xf3 s\u1eed d\u1ee5ng bias ko,true or false\\n  * `kernel_initializer` kh\u1edfi t\u1ea1o gi\xe1 tr\u1ecb \u0111\u1ea7u cho tham s\u1ed1 trong layer tr\u1eeb bias\\n  * `bias_initializer` kh\u1edfi t\u1ea1o gi\xe1 tr\u1ecb \u0111\u1ea7u cho bias\\n  * `kernel_regularizer` regularizer cho coeff\\n  * `bias_regularizer` regularizer cho bias\\n  * `activity_regularizer` c\xf3 s\u1eed d\u1ee5ng regularizer cho output ko\\n  * `kernel_constraint`,`bias_constraint` c\xf3 r\xe0ng bu\u1ed9c v\u1ec1 weight ko\\n* `Activation` d\xf9ng \u0111\u1ec3 ch\u1ecdn activation trong layer(c\xf3 th\u1ec3 d\xf9ng tham s\u1ed1 activation thay th\u1ebf).Xem ph\u1ea7n sau.\\n* `Dropout` layer n\xe0y d\xf9ng nh\u01b0 regularization cho c\xe1c layer h\u1ea1n ch\u1ebf overfiting. Tham s\u1ed1 c\u1ea7n ch\xfa \xfd :\\n  * `rate` t\u1ec9 l\u1ec7 dropout\\n  * `noise_shape` c\xe1i n\xe0y ch\u01b0a t\xecm hi\u1ec3u\\n  * `seed` random seed b\xecnh th\u01b0\u1eddng\\n* `Flatten` d\xf9ng \u0111\u1ec3 l\xe1t ph\u1eb1ng layer \u0111\u1ec3 fully connection, vd : shape : 20x20 qua layer n\xe0y s\u1ebd l\xe0 400x1\\n* `Input` layer n\xe0y s\u1eed d\u1ee5ng input nh\u01b0 1 layer nh\u01b0 vd tr\u01b0\u1edbc ta \u0111\xe3 n\xf3i.\\n* `Reshape` gi\u1ed1ng nh\u01b0 t\xean g\u1ecdi c\u1ee7a n\xf3, d\xf9ng \u0111\u1ec3 reshape\\n* `Lambda` d\xf9ng nh\u01b0 lambda trong python th\xf4i ha\\n* Convolutional Layers: ch\u1ee9a c\xe1c layer trong m\u1ea1ng n\u01a1 ron t\xedch ch\u1eadp . `Conv1D`,`Conv2D` l\xe0 convolution layer d\xf9ng \u0111\u1ec3 l\u1ea5y feature t\u1eeb image. tham s\u1ed1 c\u1ea7n ch\xfa \xfd:\\n  * `filters` s\u1ed1 filter c\u1ee7a convolution layer\\n  * `kernel_size` size window search tr\xean image\\n  * `strides` b\u01b0\u1edbc nh\u1ea3y m\u1ed7i window search\\n  * `padding` same l\xe0 d\xf9ng padding,valid l\xe0 ko\\n  * `data_format` format channel \u1edf \u0111\u1ea7u hay cu\u1ed1i\\n* `UpSampling1D`,`UpSampling2D` Ng\u01b0\u1ee3c l\u1ea1i v\u1edbi convolution layer\\n  * `size` vd (2,2) c\xf3 ngh\u0129a m\u1ed7i pixel ban \u0111\u1ea7u s\u1ebd th\xe0nh 4 pixel\\n  * `ZeroPadding1D`,`ZeroPadding2D` d\xf9ng \u0111\u1ec3 padding tr\xean image.\\n  * `padding` s\u1ed1 pixel padding \\n* Pooling Layers : Ch\u1ee9a c\xe1c layer d\xf9ng trong m\u1ea1ng CNN.\\n  * `MaxPooling1D`,`MaxPooling2D` d\xf9ng \u0111\u1ec3 l\u1ea5y feature n\u1ed5i b\u1eadt(d\xf9ng max) v\xe0 gi\xfap gi\u1ea3m parameter khi training\\n  * `pool_size` size pooling\\n  * `AveragePooling1D`,`AveragePooling2D` gi\u1ed1ng nh\u01b0 maxpooling nh\u01b0ng d\xf9ng Average\\n* `GlobalMaxPooling1D`,`GlobalMaxPooling2D` ch\u01b0a d\xf9ng bao gi\u1edd n\xean ch\u01b0a hi\u1ec3u n\xf3 l\xe0m g\xec\\n* Recurrent Layers ch\u1ee9a c\xe1c layers d\xf9ng trong m\u1ea1ng RNN\\n  * `RNN` layer RNN c\u01a1 b\u1ea3n\\n  * `GRU` kh\u1eafc ph\u1ee5c h\u1ea1n ch\u1ebf RNN tr\xe1nh vanish gradient.\\n  * `LSTM` Long Short-Term Memory layer\\n* Embedding layer : `Embedding` d\xf9ng trong nhi\u1ec1u trong nlp m\u1ee5c \u0111\xedch embbding sang m\u1ed9t kh\xf4ng gian m\u1edbi c\xf3 chi\u1ec1u nh\u1ecf h\u01a1n, v\xe0 dc learning from data thay cho one-hot lad hard code.\\n  * `input_dim` size c\u1ee7a vocabulary\\n  * `output_dim` size c\u1ee7a word embbding\\n  * `input_length` chi\u1ec1u d\xe0i m\u1ed7i sequence\\n* Merge Layers ch\u1ee9a c\xe1c layers gi\xfap ch\xfang ta c\u1ed9ng,tr\u1eeb,ho\u1eb7c n\u1ed1i c\xe1c layer nh\u01b0 c\xe1c vector v\u1eady :\\n  * `Add` c\u1ed9ng c\xe1c layers\\n  * `Subtract`tr\u1eeb c\xe1c layers\\n  * `Multiply`nh\xe2n c\xe1c layer\\n  * `Average` t\xednh trung b\xecnh c\xe1c layers\\n  * `Maximum` l\u1ea5y maximun gi\u1eefa c\xe1c layers\\n  * `Concatenate` n\u1ed1i c\xe1c layer\\n* `Dot` Nh\xe2n matrix gi\u1eef 2 layers\\nOwn Keras layers : Gi\xfap ch\xfang ta c\xf3 th\u1ec3 x\xe2y d\u1ef1ng layer nh\u01b0 theo \xfd mu\u1ed1n, g\u1ed3m 3 method ch\xfang ta c\u1ea7n ch\xfa \xfd l\xe0 `build`,`call` v\xe0 `compute_output_shape`\\n  \\n```py\\nfrom keras import backend as K\\nfrom keras.engine.topology import Layer\\nimport numpy as np\\n\\nclass MyLayer(Layer):\\n\\n    def __init__(self, output_dim, **kwargs):\\n        self.output_dim = output_dim\\n        super(MyLayer, self).__init__(**kwargs)\\n\\n    def build(self, input_shape):\\n        # Create a trainable weight variable for this layer.\\n        self.kernel = self.add_weight(name=\'kernel\', \\n                                      shape=(input_shape[1], self.output_dim),\\n                                      initializer=\'uniform\',\\n                                      trainable=True)\\n        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\\n\\n    def call(self, x):\\n        return K.dot(x, self.kernel)\\n\\n    def compute_output_shape(self, input_shape):\\n        return (input_shape[0], self.output_dim)\\n```\\n\\n### Ti\u1ebfp theo ch\xfang ta t\xecm hi\u1ec3u \u0111\u1ebfn ti\u1ec1n x\u1eed l\xfd d\u1eef li\u1ec7u trong keras, n\xf3 \u0111\u01b0\u1ee3c chia ra l\xe0m 3 ph\u1ea7n :\\n* `Sequence Preprocessing` ti\u1ec1n x\u1eed l\xfd chu\u1ed7i .\\n    * `TimeseriesGenerator` c\xe1i n\xe0y d\xf9ng \u0111\u1ec3 t\u1ea1o d\u1eef li\u1ec7u cho time series\\n    * `pad_sequences` d\xf9ng \u0111\u1ec3 padding gi\xfap c\xe1c chu\u1ed7i c\xf3 \u0111\u1ed9 d\xe0i b\u1eb1ng nhau\\n    * `skipgrams` t\u1ea1o data trong model skip gram,k\u1ebft qu\u1ea3 tr\u1ea3 v\u1ec1 2 tuple n\u1ebfu word xu\u1ea5t hi\u1ec7n c\xf9ng nhau l\xe0 1 n\u1ebfu ko l\xe0 0.\\n* `Text Preprocessing` ti\u1ec1n x\u1eed l\xfd text\\n    * `Tokenizer` gi\u1ed1ng k\u1ef9 thu\u1eadt tokenizer trong nlp, t\u1ea1o tokenizer t\u1eeb documment\\n    * `one_hot` t\u1ea1o data d\u1ea1ng one hot encoding\\n    * `text_to_word_sequence` covert text th\xe0nh sequence\\n* `Image Preprocessing` ti\u1ec1n x\u1eed l\xfd image\\n    * `ImageDataGenerator` t\u1ea1o th\xeam data b\u1eb1ng c\xe1ch scale,rotation...\\n    \\n### C\xe1c function trong b\u1ed9 x\u01b0\u01a1ng c\u1ee7a model\\nC\xe1c h\xe0m `loss functions` th\u01b0\u1eddng d\xf9ng :\\n* `mean_squared_error` th\u01b0\u1eddng d\xf9ng trong regression t\xednh theo eculic\\n* `mean_absolute_error` t\xednh theo tr\u1ecb tuy\u1ec7t \u0111\u1ed1i\\n* `categorical_crossentropy` d\xf9ng trong classifier nhi\u1ec1u class\\n* `binary_crossentropy` d\xf9ng trong classifier 2 class\\n* `kullback_leibler_divergence` d\xf9ng \u0111\u1ec3 t\xednh loss gi\u1eefa ph\xe2n ph\u1ed1i th\u1ef1c t\u1ebf v\xe0 th\u1ef1c nghi\u1ec7m\\n  \\n`metrics` n\xf3 l\xe0 th\u01b0\u1edbc \u0111o \u0111\u1ec3 ta \u0111\xe1nh gi\xe1 accuracy c\u1ee7a model :\\n* `binary_accuracy` n\u1ebfu y_true==y_pre th\xec tr\u1ea3 v\u1ec1 1 ng\u01b0\u1ee3c l\u1ea1i 0,d\xf9ng cho 2 class\\n* `categorical_accuracy` t\u01b0\u01a1ng t\u1ef1 binary_accuracy nh\u01b0ng cho nhi\u1ec1u class\\n  \\n`optimizers` d\xf9ng \u0111\u1ec3 ch\u1ecdn thu\u1eadt to\xe1n training.\\n* `SGD` Stochastic gradient descent optimizer\\n* `RMSprop` RMSProp optimizer\\n* `Adam` Adam optimizer\\n  \\n`activations` \u0111\u1ec3 ch\u1ecdn activation function \\n* `linear` nh\u01b0 trong linear regression\\n* `softmax` d\xf9ng trong multi classifier\\n* `relu` max(0,x) d\xf9ng trong c\xe1c layer cnn,rnn \u0111\u1ec3 gi\u1ea3m chi ph\xed t\xednh to\xe1n\\n* `tanh` range (-1,1)\\n* `Sigmoid` range (0,1) d\xf9ng nhi\u1ec1u trong binary class\\n  \\nCallbacks : khi model ch\xfang ta l\u1edbn c\xf3 khi training th\xec g\u1eb7p s\u1ef1 c\u1ed1 ta mu\u1ed1n l\u01b0u l\u1ea1i model \u0111\u1ec3 ch\u1ea1y l\u1ea1i th\xec callback gi\xfap t l\xe0m \u0111i\u1ec1u n\xe0y :\\n* `ModelCheckpoint` l\u01b0u l\u1ea1i model sau m\u1ed7i epoch\\n* `EarlyStopping` stop training khi training ko c\u1ea3i thi\u1ec7n model\\n* `ReduceLROnPlateau` gi\u1ea3m learning m\u1ed7i khi metrics ko \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n\\n  \\nDatasets. Keras h\u1ed7 tr\u1ee3 m\u1ed9t s\u1ed1 dataset theo c\xf4ng th\u1ee9c :\\n* `cifar100` g\u1ed3m 50,000 32x32 color training images, labeled over 100 categories, and 10,000 test images.\\n* `mnist` data 70k image data hand written.\\n* `fashion_mnist` Dataset of 70k 28x28 grayscale images of 10 fashion categories\\n* `imdb` 25,000 movies reviews from IMDB, label \u0111\xe1nh theo pos/neg\\n* `reuters` 11,228 newswires from Reuters, labeled over 46 topics\\n* `boston_housing` data gi\xe1 nh\xe0 \u1edf boston theo 13 features\\n  \\n```py\\nfrom keras.datasets import name_data\\n(X_train,X_test),(y_train,y_test) = name_data.load_data()\\n```\\n`Applications` ch\u1ee9a c\xe1c pre-training weight c\u1ee7a c\xe1c model deep learning n\u1ed5i ti\u1ebfng.`Xception`,`VGG16`,`VGG19`,`resnet50`,`inceptionv3`,\\n`InceptionResNetV2`,`MobileNet`,`DenseNet`,`NASNet` c\u1ea9u tr\xfac chung nh\u01b0 sau :\\n* `preprocess_input` d\xf9ng \u0111\u1ec3 preprocessing input custom same v\u1edbi input c\u1ee7a pretraining\\n* `decode_predictions` d\xf9ng \u0111\u1ec3 xem label predict\\n```py\\nfrom keras.applications.name_pre_train import Name_pre_train\\nfrom keras.applications.name_pre_train import preprocess_input, decode_predictions\\nmodel = Name_pre_train(weights=\'t\xean dataset\')\\n```\\n\\n`backends` banckend c\xf3 ngh\u0129a l\xe0 thay v\xec keras x\xe2y d\u1ef1ng t\u1eeb \u0111\u1ea7u c\xe1c c\xf4ng th\u1ee9c t\u1eeb \u0111\u01a1n gi\u1ea3n \u0111\u1ebfn ph\u1ee9c t\u1ea1p, th\xec n\xf3 d\xf9ng nh\u1eefng th\u01b0 vi\u1ec7n \u0111\xe3 x\xe2y d\u1ef1ng s\u1eb5n r\u1ed3i v\xe0 d\xf9ng th\xf4i. Gi\xfap ti\u1ebft ki\u1ec7m dc th\u1eddi gian v\xe0 ch\xed ph\xed. Trong keras c\xf3 h\u1ed7 tr\u1ee3 3 backend l\xe0 tensorflow,theano v\xe0 CNTK.\\n`initializers` kh\u1edfi t\u1ea1o gi\xe1 tr\u1ecb  weight c\u1ee7a coeff v\xe0 bias tr\u01b0\u1edbc khi training l\u1ea7n l\u01b0\u1ee3t `kernel_initializer` and `bias_initializer`. m\u1eb7c \u0111\u1ecbnh l\xe0 `glorot_uniform` ph\xe2n ph\u1ed1i uniform v\u1edbi gi\xe1 tr\u1ecb 1/c\u0103n(input+output).\\n`regularizers` D\xf9ng \u0111\u1ec3 ph\u1ea1t nh\u1eefng coeff n\xe0o t\xe1c \u0111\u1ed9ng qu\xe1 m\u1ea1nh v\xe0o m\u1ed7i layer th\u01b0\u1eddng d\xf9ng l\xe0 L1 v\xe0 L2\\n`constraints` d\xf9ng \u0111\u1ec3 thi\u1ebft l\u1eadp c\xe1c \u0111i\u1ec1u ki\u1ec7n r\xe0ng bu\u1ed9c khi training\\n`visualization` gi\xfap ch\xfang ta plot l\u1ea1i c\u1ea5u tr\xfac m\u1ea1ng neral network.\\n`Utils` ch\u1ee9a c\xe1c function c\u1ea7n thi\u1ebft gi\xfap ta x\u1eed l\xfd data nhanh h\u01a1n.\\n`normalize` chu\u1ea9n h\xf3a data theo L2\\n`plot_model` gi\xfap ch\xfang ta plot model \\n`to_categorical` covert class sang binary class matrix"},{"id":"Feature-extraction-trong-computer-vision","metadata":{"permalink":"/blog/Feature-extraction-trong-computer-vision","editUrl":"https://github.com/ThorPham/blog/2018-4-22-Feature-extraction-trong-computer-vision/index.md","source":"@site/blog/2018-4-22-Feature-extraction-trong-computer-vision/index.md","title":"Feature extraction trong computer vision","description":"Nh\u01b0 ch\xfang ta \u0111\xe3 bi\u1ebft Feature engineering l\xe0 qu\xe1 tr\xecnh ch\xfang ta th\u1ef1c hi\u1ec7n tr\xedch xu\u1ea5t v\xe0 tr\xedch ch\u1ecdn c\xe1c \u0111\u1eb7c tr\u01b0ng(thu\u1ed9c t\xednh) quan tr\u1ecdng t\u1eeb d\u1eef li\u1ec7u th\xf4 \u0111\u1ec3 s\u1eed d\u1ee5ng l\xe0m \u0111\u1ea1i di\u1ec7n cho c\xe1c m\u1eabu d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n.V\xec v\u1eady trong m\u1ed9t t\u1eadp dataset kh\xf4ng ph\u1ea3i d\u1eef li\u1ec7u n\xe0o c\u0169ng quan tr\u1ecdng, kh\xf4ng ph\u1ea3i \u0111\u1eb7c tr\u01b0ng n\xe0o c\u0169ng d\u1ec5 nh\u1eadn bi\u1ebft. Ch\xednh v\xec th\u1ebf \u0111\u1ed1i v\u1edbi m\u1ed7i lo\u1ea1i d\u1eef li\u1ec7u s\u1ebd c\xf3 nh\u1eefng \u0111\u1eb7c tr\u01b0ng ri\xeang , trong b\xe0i vi\u1ebft n\xe0y ta c\xf9ng t\xecm hi\u1ec3u 2 \u0111\u1eb7c tr\u01b0ng quan tr\u1ecdng trong CV truy\u1ec1n th\u1ed1ng","date":"2018-04-22T00:00:00.000Z","formattedDate":"April 22, 2018","tags":[{"label":"Feature extraction","permalink":"/blog/tags/feature-extraction"},{"label":"computer vision","permalink":"/blog/tags/computer-vision"}],"readingTime":3.705,"truncated":true,"authors":[{"name":"Thorpham","title":"Deep learning enthusiast","url":"https://github.com/ThorPham","imageURL":"https://github.com/ThorPham.png","key":"thorpham"}],"frontMatter":{"slug":"Feature-extraction-trong-computer-vision","title":"Feature extraction trong computer vision","authors":"thorpham","tags":["Feature extraction","computer vision"]},"prevItem":{"title":"T\xecm hi\u1ec3u v\u1ec1 th\u01b0 vi\u1ec7n keras trong deep learning","permalink":"/blog/T\xecm-hi\u1ec3u-v\u1ec1-th\u01b0-vi\u1ec7n-keras-trong-deep-learning"},"nextItem":{"title":"Drowsiness detection v\u1edbi Dlib v\xe0 OpenCV","permalink":"/blog/Drowsiness-detection"}},"content":"*Nh\u01b0 ch\xfang ta \u0111\xe3 bi\u1ebft Feature engineering l\xe0 qu\xe1 tr\xecnh ch\xfang ta th\u1ef1c hi\u1ec7n tr\xedch xu\u1ea5t v\xe0 tr\xedch ch\u1ecdn c\xe1c \u0111\u1eb7c tr\u01b0ng(thu\u1ed9c t\xednh) quan tr\u1ecdng t\u1eeb d\u1eef li\u1ec7u th\xf4 \u0111\u1ec3 s\u1eed d\u1ee5ng l\xe0m \u0111\u1ea1i di\u1ec7n cho c\xe1c m\u1eabu d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n.V\xec v\u1eady trong m\u1ed9t t\u1eadp dataset kh\xf4ng ph\u1ea3i d\u1eef li\u1ec7u n\xe0o c\u0169ng quan tr\u1ecdng, kh\xf4ng ph\u1ea3i \u0111\u1eb7c tr\u01b0ng n\xe0o c\u0169ng d\u1ec5 nh\u1eadn bi\u1ebft. Ch\xednh v\xec th\u1ebf \u0111\u1ed1i v\u1edbi m\u1ed7i lo\u1ea1i d\u1eef li\u1ec7u s\u1ebd c\xf3 nh\u1eefng \u0111\u1eb7c tr\u01b0ng ri\xeang , trong b\xe0i vi\u1ebft n\xe0y ta c\xf9ng t\xecm hi\u1ec3u 2 \u0111\u1eb7c tr\u01b0ng quan tr\u1ecdng trong CV truy\u1ec1n th\u1ed1ng*\\n\x3c!--truncate--\x3e\\nN\u1ed9i dung b\xe0i vi\u1ebft :\\n1. Local binary pattern\\n2. Histogram Oriented of Gradient\\n\\n## Local binary pattern\\n* Local binary pattern n\xf3 l\xe0 m\u1ed9t thu\u1eadt to\xe1n m\xf4 t\u1ea3 texture(c\u1ea7u tr\xfac) c\u1ee7a m\u1ed9t image. \xdd t\u01b0\u1edfng c\u01a1 b\u1ea3n c\u1ee7a n\xf3 l\xe0 m\xf4 ph\u1ecfng l\u1ea1i c\u1ea5u tr\xfac c\u1ee5c b\u1ed9\\n(local texture) c\u1ee7a image b\u1eb1ng c\xe1ch so s\xe1nh m\u1ed7i pixel v\u1edbi c\xe1c pixel l\xe2n c\u1eadn n\xf3(neighbors).Ta s\u1ebd \u0111\u1eb7t m\u1ed9t pixel l\xe0 trung t\xe2m(center) v\xe0 so s\xe1nh\\nv\u1edbi c\xe1c pixel l\xe2n c\u1eadn v\u1edbi n\xf3, n\u1ebfu pixel trung t\xe2m l\u1edbn h\u01a1n ho\u1eb7c b\u1eb1ng pixel l\xe2n c\u1eadn th\xec n\xf3 s\u1ebd tr\u1ea3 v\u1ec1 gi\xe1 tr\u1ecb 1, ng\u01b0\u1ee3c l\u1ea1i 0. V\xed d\u1ee5 ch\xfang ta\\nl\u1ea5y b\xe1n k\xednh 8 pixel l\xe2n c\u1eadn th\xec lbp s\u1ebd c\xf3 d\u1ea1ng 11001111, l\xe0 m\u1ed9t chu\u1ed7i nh\u1ecb ph\xe2n \u0111\u1ec3 \u0111\u01a1n gi\u1ea3n v\xe0 d\u1ec5 \u0111\u1ecdc h\u01a1n ta s\u1ebd chuy\u1ec3n v\u1ec1 d\u1ea1ng decimal 207.\\n\\n\x3c!-- ![LBP](./lbp.jpg) --\x3e\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./lbp.jpg\').default} />\\n</center>\\n\\n* C\xe1ch t\xednh n\xe0y c\xf3 h\u1ea1n ch\u1ebf \u0111\xf3 l\xe0 ch\u1ec9 gi\u1edbi h\u1ea1n 3x3 pixel kh\xf4ng \u0111\u1ee7 \u0111\u1ec3 m\xf4 t\u1ea3 c\xe1c c\u1ea5u tr\xfac large scale n\xean ng\u01b0\u1eddi ta m\u1edf r\u1ed9ng kh\xe1i ni\u1ec7m LBP b\u1eb1ng c\xe1ch \u0111\u1ecbnh ngh\u0129a th\xeam 2 tham s\u1ed1 l\xe0 (P,R) trong \u0111\xf3 P l\xe0 s\u1ed1 pixel l\xe2n c\u1eadn xem x\xe9t  v\xe0 R l\xe0 b\xe1n k\xednh ta qu\xe9t t\u1eeb pixel trung t\xe2m. Nh\u01b0 h\xecnh b\xean d\u01b0\u1edbi.\\n\\n\x3c!-- ![LBP2](./lbp2.jpg) --\x3e\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./lbp2.jpg\').default} />\\n</center>\\n* C\xf4ng th\u1ee9c LBP nh\u01b0 sau :\\n\\n$$\\nLBP_{r,p} = \\\\sum_{n=0}^{p-1}S(X_{r,p,n}-X_{p})2^{n}\\n$$\\n\\n trong \u0111\xf3 :\\n \\n $$ \\n S(x) =  \\\\begin{cases}\\n  1, & \\\\text{if } x >= 1, \\\\\\\\\\n  0, & \\\\text{otherwise}.\\n\\\\end{cases}\\n $$\\n* Code trong python v\u1edbi skimage\\n\\n```py\\nimport numpy as np\\nfrom skimage import io\\nfrom skimage.feature import local_binary_pattern\\nfrom matplotlib import pyplot as plt\\n%matplotlib inline\\n\\nim = io.imread(\\"image.png\\",as_grey=True)\\nlbp = local_binary_pattern(im,8,1,method=\\"uniform\\")\\nplt.figure(figsize=(25,25))\\nplt.subplot(1,3,1)\\nplt.imshow(im,cmap=\\"gray\\")\\nplt.subplot(1,3,2)\\nplt.imshow(lbp,cmap=\\"gray\\")\\n```\\n\\n% ![lbp3](./lbp3.jpg)\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./lbp3.jpg\').default} />\\n</center>\\n\\n## Histogram Oriented of Gradient \\n\\nHistogram Oriented of Gradient (Hog) l\xe0 m\u1ed9t feature descriptor th\u01b0\u1eddng \u0111\u01b0\u1ee3c d\xf9ng trong object recognition. Nh\u01b0 ch\xfang ta \u0111\xe3 bi\u1ebft trong\\nimage processing th\xec kh\xe1i ni\u1ec7m \u0111\u1ea1o h\xe0m r\u1ea5t quan tr\u1ecdng. N\xf3 l\xe0 c\u01a1 s\u1edf c\u1ee7a r\u1ea5t nhi\u1ec1u thu\u1eadt to\xe1n nh\u01b0 edge,coner detection. D\u1ef1a v\xe0o \u0111\u1eb7c \u0111i\u1ec3m n\xe0y ng\u01b0\u1eddi ta m\u1edbi x\xe2y d\u1ef1ng n\xf3 l\xe0m feature tr\xean c\u01a1 s\u1edf derivative. \u0110\u1ea1o h\xe0m c\u1ee7a image l\xe0 m\u1ed9t matrix theo ox v\xe0 oy n\xf3 c\xf3 2 \u0111\u1eb7c tr\u01b0ng l\xe0 \u0111\u1ed9 l\u1edbn(magnitude) v\xe0 h\u01b0\u1edbng(direction). \u0110\u1ec3 l\xe0m feature tr\xean image th\xec kh\xf4ng th\u1ec3 \u0111\u1ec3 2 \u0111\u1ea1i l\u01b0\u1ee3ng n\xe0y r\u1eddi r\u1ea1c \u0111\u01b0\u1ee3c n\xean ng\u01b0\u1eddi ta m\u1edbi ngh\u0129 ra ph\u01b0\u01a1ng ph\xe1p chu\u1ea9n h\xf3a n\xf3 (quantization) \u0111\xf3 l\xe0 \u0111\u01b0a n\xf3 v\u1ec1 d\u1ea1ng histogram c\u1ee7a magnitude theo direction.B\xe2y gi\u1edd ta t\xecm hi\u1ec3u c\xe1c b\u01b0\u1edbc t\xednh to\xe1n ra hog.\\n\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./hog.jpg\').default} />\\n</center>\\n\\nC\xe1c b\u01b0\u1edbc t\xednh hog c\u1ee5 th\u1ec3. X\xe9t tr\xean 1 cell nh\u01b0 trong \u1ea3nh l\xe0 8x8:\\n1. T\xednh \u0111\u1ea1o h\xe0m c\u1ee7a image theo x,y\\n$$\\n\\\\begin{align*}\\n\\\\nabla f(x, y)\\n= \\\\begin{bmatrix}\\n  g_x \\\\\\\\\\n  g_y\\n\\\\end{bmatrix}\\n= \\\\begin{bmatrix}\\n  \\\\frac{\\\\partial f}{\\\\partial x} \\\\\\\\[6pt]\\n  \\\\frac{\\\\partial f}{\\\\partial y}\\n\\\\end{bmatrix}\\n= \\\\begin{bmatrix}\\n  f(x, y+1) - f(x, y-1)\\\\\\\\\\n  f(x+1, y) - f(x-1, y)\\n\\\\end{bmatrix}\\n\\\\end{align*}\\n$$\\n2. T\xednh magitude $g = \\\\sqrt{ g_x^2 + g_y^2 }$ v\xe0 direction $\\\\theta = \\\\arctan{(g_y / g_x)}$\\n3. Chia magitude theo 9 bins( c\xf3 h\u01b0\u1edbng theo direction t\u1eeb 0-180 m\u1ed7i bin 20)\\n4. L\u01b0u \xfd tr\xean 1 block 16x16 th\xec \u0111\u1ec3 tr\xe1nh \u1ea3nh h\u01b0\u1edfng c\u1ee7a \u0111\u1ed9 s\xe1ng t\u1ed1i \u1ea3nh h\u01b0\u1edfng t\u1edbi image ng\u01b0\u1eddi ta s\u1ebd chu\u1ea9n h\xf3a gradient(Normalizing Gradient Vectors). V\xec nh\u01b0 ch\xfang ta bi\u1ebft khi chu\u1ea9n h\xf3a c\u1ed9ng ho\u1eb7c tr\u1eeb 1 \u0111\u1ea1i l\u01b0\u1ee3ng tr\xean image s\u1ebd ko l\xe0m thay \u0111\u1ed5i gradient.\\n5. \\n\u1ea2nh minh h\u1ecda c\xe1ch \u0111\u01b0a magitude v\xe0o bin theo direction  \\n\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src=\\"https://www.learnopencv.com/wp-content/uploads/2016/12/hog-histogram-1.png\\" />\\n</center>\\n\\n  \\n\\nCode trong python : Ta c\xf3 th\u1ec3 d\xf9ng opencv ho\u1eb7c skimage \u0111\u1ec3 t\xednh hog :\\n  * Trong opencv: `cv2.HOGDescriptor` v\u1edbi c\xe1c tham s\u1ed1 win_size,block_size,block_stride,cell_size,num_b\xedn\\n  * Trong skimage : `fucntion hog` v\u1edbi c\xe1c tham s\u1ed1 orientations, pixels_per_cell,cells_per_block"},{"id":"Drowsiness-detection","metadata":{"permalink":"/blog/Drowsiness-detection","editUrl":"https://github.com/ThorPham/blog/2018-4-21-Drowsiness-detection /index.mdx","source":"@site/blog/2018-4-21-Drowsiness-detection /index.mdx","title":"Drowsiness detection v\u1edbi Dlib v\xe0 OpenCV","description":"*B\xe0i tr\u01b0\u1edbc ch\xfang ta \u0111\xe3 t\xecm hi\u1ec3u v\u1ec1 facial landmark. Trong b\xe0i n\xe0y ch\xfang ta s\u1ebd \u1ee9ng d\u1ee5ng facial landmark v\xe0o Drowsiness detection. Drowness detection","date":"2018-04-21T00:00:00.000Z","formattedDate":"April 21, 2018","tags":[{"label":"opencv","permalink":"/blog/tags/opencv"},{"label":"python","permalink":"/blog/tags/python"},{"label":"Dlib","permalink":"/blog/tags/dlib"}],"readingTime":3.83,"truncated":true,"authors":[{"name":"Thorpham","title":"Deep learning enthusiast","url":"https://github.com/ThorPham","imageURL":"https://github.com/ThorPham.png","key":"thorpham"}],"frontMatter":{"slug":"Drowsiness-detection","title":"Drowsiness detection v\u1edbi Dlib v\xe0 OpenCV","authors":"thorpham","tags":["opencv","python","Dlib"]},"prevItem":{"title":"Feature extraction trong computer vision","permalink":"/blog/Feature-extraction-trong-computer-vision"},"nextItem":{"title":"T\xecm hi\u1ec3u eigenFace trong face recognite","permalink":"/blog/T\xecm-hi\u1ec3u-eigenFace-trong-face-recognite"}},"content":"*B\xe0i tr\u01b0\u1edbc ch\xfang ta \u0111\xe3 t\xecm hi\u1ec3u v\u1ec1 facial landmark. Trong b\xe0i n\xe0y ch\xfang ta s\u1ebd \u1ee9ng d\u1ee5ng facial landmark v\xe0o Drowsiness detection. Drowness detection\\nd\xf9ng \u0111\u1ec3 x\xe1c \u0111\u1ecbnh tr\u1ea1ng th\xe1i ng\u1ee7 g\u1eadt hay kh\xf4ng d\u1ef1a v\xe0o facial landmark c\u1ee7a eye. Th\u01b0\u1eddng \u0111\u01b0\u1ee3c c\xe1i t\xe0i x\u1ebf x\u1eed d\u1ee5ng khi \u0111i\u1ec1u khi\u1ec3n ph\u01b0\u01a1ng ti\u1ec7n giao\\nth\xf4ng \u0111\u1ec3 h\u1ea1n ch\u1ebf tai n\u1ea1n.*\\n\x3c!--truncate--\x3e\\nC\u1ea5u tr\xfac c\u1ee7a b\xe0i :\\n  * T\xecm hi\u1ec3u \xfd t\u01b0\u1edfng.\\n  * X\xe2y d\u1ef1ng model.\\n  * Test model\\n  \\n## T\xecm hi\u1ec3u \xfd t\u01b0\u1edfng\\n\xdd t\u01b0\u1edfng c\u0169ng r\u1ea5t \u0111\u01a1n gi\u1ea3n th\xf4i, l\xe0 ch\xfang ta s\u1ebd d\u1ef1a v\xe0o facial landmark c\u1ee7a eyes \u0111\u1ec3 x\xe1c \u0111\u1ecbnh \u0111\u01b0\u1ee3c t\u1ec9 l\u1ec7 n\xe0o \u0111\xf3 nh\u01b0 m\u1ed9t ng\u01b0\u1ee1ng \u0111\u1ec3 xem x\xe9t\\nm\u1eaft \u0111ang nh\u1eafm hay m\u1edf.Trong paper **Real-Time Eye Blink Detection using Facial Landmarks** c\u1ee7a **Tereza Soukupova** v\xe0 **Jan \xb4 Cech** \u0111\xe3\\nt\xecm ra \u0111\u01b0\u1ee3c m\u1ed9t c\xf4ng th\u1ee9c gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 n\xe0y c\xf3 t\xean g\u1ecdi l\xe0 eye aspect ratio(EAR).Ch\xfang ta c\xf9ng t\xecm hi\u1ec3u qua v\u1ec1 c\xf4ng th\u1ee9c n\xe0y.\\n\\n$$\\nEAR =  \\\\frac{||p_{2} - p_{6}|| + ||p_{3} - p_{5}||}{||p_{1} - p_{4}||}\\n$$\\n\\nTrong \u0111\xf3 $p_{i}$ l\xe0 lankmark point c\u1ee7a eye, k\xfd hi\u1ec7u $||$ l\xe0 kho\u1ea3ng c\xe1ch euclide.\\n  \\n<p align=\\"center\\">\\n  <img width=\\"600\\" height=\\"300\\" src={require(\'./drowness1.jpg\').default}/>\\n</p>\\n\\n\u0110\u1ed3 th\u1ecb EAR,trong \u0111\xf3 p1,p2,p3,p4,p5,p6 l\xe0 landmark point c\u1ee7a eye(l\u01b0u \xfd ta s\u1ebd k\xfd hi\u1ec7u b\u1eaft \u0111\u1ea7u b\u1eb1ng 0 thay v\xec b\u1eb1ng 1 trong model).Bi\u1ec3u \u0111\u1ed3 b\xean d\u01b0\u1edbi l\xe0 \u0111\u1ed3 th\u1ecb c\u1ee7a EAR . Khi m\xe0 eye ta th\u1ea5y l\xe0 EAR s\u1ebd n\u1eb1m d\u01b0\u1edbi threshold 0.15 v\xe0 b\xecnh th\u01b0\u1eddng c\u1ee7a n\xf3 s\u1ebd l\u1edbn h\u01a1n 0.25. \\n\u0110\xf3 l\xe0 \xfd t\u01b0\u1edfng c\u1ee7a b\xe0i to\xe1n.\u1ede \u0111\xe2y c\xf3 1 s\u1ed1 l\u01b0u \xfd l\xe0 :\\n   * C\xf3 2 eye n\xean ta s\u1ebd l\u1ea5y trung b\xecnh c\u1ee7a 2 eye \u0111\u1ec3 l\u1ea5y EAR\\n   * \u0110\u1ec3 tr\xe1nh tr\u01b0\u1eddng h\u1ee3p nh\xe1y m\u1eaft hay hay detection sai ta s\u1ebd cho EAR m\u1ed9t kho\u1ea3ng th\u1eddi gian \u0111\u1ee7 l\xe2u \u0111\u1ec3 x\xe1c nh\u1eadn l\xe0 drowsiness.\\n   * Threshold s\u1ebd do ta ch\u1ecdn theo \xfd mu\u1ed1n ta ta th\u1ea5y h\u1ee3p l\xfd.\\n\\n## X\xe2y d\u1ef1ng model\\nTr\u01b0\u1edbc h\u1ebft ta x\xe2y d\u1ef1ng c\xe1c h\xe0m helper .\\n\u0110\u1ea7u ti\xean l\xe0 h\xe0m chuy\u1ec3n landmark point th\xe0nh array . V\xec m\u1eb7c \u0111\u1ecbnh n\xf3 r\u1ea5t kh\xf3 x\xe0i.\\n```py\\ndef landmark_transform(landmarks):\\n    land_mark_array = []\\n    for i in landmarks:\\n        land_mark_array.append([int(i.x),int(i.y)])\\n    return np.array(land_mark_array)\\n```\\nTi\u1ebfp theo l\xe0 h\xe0m t\xednh EAR.\\n```py\\ndef calculate_distance(eye):\\n    assert len(eye)== 6\\n    p0,p1,p2,p3,p4,p5 = eye\\n    distance_p1_p5 = np.sqrt((p1[0]-p5[0])**2 + (p1[1]-p5[1])**2)\\n    distance_p2_p4 = np.sqrt((p2[0]-p4[0])**2 + (p2[1]-p4[1])**2)\\n    distance_p0_p3 = np.sqrt((p0[0]-p3[0])**2 + (p0[1]-p3[1])**2)\\n    EAR = (distance_p1_p5 + distance_p2_p4)/(2*distance_p0_p3)\\n    return EAR\\n```\\nTi\u1ebfp theo l\xe0 h\xe0m v\u1ebd contours cho eye \u0111\u1ec3 ti\u1ec7n theo d\xf5i. Ch\xfang ta d\xf9ng `convexhull` \u0111\u1ec3 x\u1ea5p x\u1ec9 h\xecnh elip gi\u1ed1ng v\u1edbi eye.\\n```py\\ndef draw_contours(image,cnt):\\n    hull = cv2.convexHull(cnt)\\n    cv2.drawContours(image,[hull],-1,(0,255,0),2)\\n```\\nCu\u1ed1i c\xf9ng l\xe0 h\xe0m **alarm** (th\xf4ng b\xe1o) khi drowsiness \u0111\u01b0\u1ee3c ph\xe1t hi\u1ec7n\\n```py\\ndef sound_alarm():\\n    playsound.playsound(\\"sound.mp3\\")\\n```\\nB\xe2y gi\u1edd ta g\u1ed9p c\xe1c function helper \u0111\xe3 t\u1ea1o th\xe0nh m\u1ed9t model ho\xe0n ch\u1ec9nh.\\n\\n```py\\npath = \\"shape_predictor_68_face_landmarks.dat\\"\\ndetector = dlib.get_frontal_face_detector()\\npredict_landmark = dlib.shape_predictor(path)\\n\\ncap = cv2.VideoCapture(0)\\ntotal=0\\nalarm = False\\nwhile cap.isOpened() == True :\\n    ret,frame = cap.read()\\n    frame_gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\\n    rects = detector(frame_gray,1)\\n    if len(rects) > 0 :\\n        for i in rects:\\n            cv2.rectangle(frame,(i.left(),i.top()),(i.right(),i.bottom()),(0,255,0),2)\\n            land_mark = predict_landmark(frame_gray,i)\\n            left_eye = landmark_transform(land_mark.parts()[36:42])\\n            right_eye = landmark_transform(land_mark.parts()[42:48])\\n            draw_contours(frame,left_eye)\\n            draw_contours(frame,right_eye)\\n            EAR_left,EAR_right = calculate_distance(left_eye),calculate_distance(right_eye)\\n            ear = np.round((EAR_left+EAR_right)/2,2)\\n            cv2.putText(frame, \\"EAR :\\" + str(ear) ,(200, 100),cv2.FONT_HERSHEY_SIMPLEX, 1.7, (0, 255, 0), 2)\\n            if ear > 0.25 :\\n                total=0\\n                alarm=False\\n                cv2.putText(frame, \\"Eyes Open \\", (10, 30),cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,255, 0 ), 2)\\n            else:\\n                total+=1\\n                print(total)\\n                if total>10:\\n                    if not alarm:\\n                        sound_alarm()\\n                        cv2.putText(frame, \\"drowsiness detect\\" ,(10, 30),cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\\n    cv2.imshow(\\"image\\", frame)\\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\\n        cv2.destroyAllWindows()\\n        cap.release()\\n        break               \\n```\\n\\nGi\u1ea3i th\xedch code m\u1ed9t t\xed .( l\u01b0u \xfd c\xe1c tham s\u1ed1 do m\xecnh ch\u1ecdn c\xe1c b\u1ea1n c\xf3 th\u1ec3 \u0111i\u1ec1u ch\u1ec9nh theo \xfd m\xecnh).\\n  * Ta s\u1ebd kh\xf4ng nh\u1eafc l\u1ea1i c\xe1ch t\xecm facial landmark v\u1edbi faced detection v\u1edbi Dlib( b\u1ea1n \u0111\u1ecdc c\xf3 th\u1ec3 xem l\u1ea1i \u1edf b\xe0i tr\u01b0\u1edbc)\\n  * C\xe1c th\xf4ng s\u1ed1 ta \u0111\u1eb7t trong model : threshold c\u1ee7a EAR l\xe0 0.25(nh\u1ecf h\u01a1n s\u1ebd xem l\xe0 close eye)\\n  * Point landmark c\u1ee7a eye: left_eye : 37-42,right_eye : 43-49\\n  * Ta s\u1ebd \u0111\u1ebfm s\u1ed1 l\u1ea7n eye close n\u1ebfu n\xf3 v\u01b0\u1ee3t qu\xe1 10 th\xec s\u1ebd c\xf3 \\"alarm\\" qua bi\u1ebfn l\xe0 `total`\\n  * `detector` v\xe0 `predict_landmark` d\xf9ng \u0111\u1ec3 detection face v\xe0 landmark\\n  * N\u1ebfu `detector` th\u1ea5y face th\xec ta s\u1ebd t\xednh` EAR_left` v\xe0 `EAR_right` sau \u0111\xf3 t\xednh trung b\xecnh \u0111\u01b0\u1ee3c `ear`\\n  * Cu\u1ed1i c\xf9ng xem x\xe9t \u0111i\u1ec1u ki\u1ec7n n\u1ebfu total >10 th\xec s\u1ebd `alarm`\\n\\n## Test model\\nTa s\u1ebd test th\u1eed model. V\xec m\xe1y m\xecnh c\u0169 v\xe0 webcame r\u1ea5t t\u1ed1i n\xean nhi\u1ec1u khi b\u1ecb lag ho\u1eb7c \u0111\u1ee9ng h\xecnh.\\n[https://www.youtube.com/watch?v=oROrBeClnec]\\n<div class=\\"x-frame video\\" data-video=\\"https://www.youtube.com/watch?v=oROrBeClnec\\"> </div>\\n\\nTham Kh\u1ea3o :\\n* http://hanzratech.in/\\n* https://pyimagesearch.com\\n* https://learnopencv.com"},{"id":"T\xecm-hi\u1ec3u-eigenFace-trong-face-recognite","metadata":{"permalink":"/blog/T\xecm-hi\u1ec3u-eigenFace-trong-face-recognite","editUrl":"https://github.com/ThorPham/blog/2018-4-19-T\xecm-hi\u1ec3u-eigenFace-trong-face-recognite/index.md","source":"@site/blog/2018-4-19-T\xecm-hi\u1ec3u-eigenFace-trong-face-recognite/index.md","title":"T\xecm hi\u1ec3u eigenFace trong face recognite","description":"*C\xf3 bao gi\u1edd b\u1ea1n v\xe0o facebook r\u1ed3i m\u1ed9t ng\xe0y n\u1ecd c\xf3 m\u1ed9t th\xf4ng b\xe1o hi\u1ec7n l\xean b\u1ea1n \u0111\u01b0\u1ee3c tag trong m\u1ed9t b\u01b0\u1edbc \u1ea3nh n\xe0o \u0111\xf3. \u0110\xe3 bao gi\u1edd b\u1ea1n ngh\u0129 l\xe0m sao","date":"2018-04-19T00:00:00.000Z","formattedDate":"April 19, 2018","tags":[{"label":"python","permalink":"/blog/tags/python"},{"label":"Face recognition","permalink":"/blog/tags/face-recognition"}],"readingTime":5.665,"truncated":true,"authors":[{"name":"Thorpham","title":"Deep learning enthusiast","url":"https://github.com/ThorPham","imageURL":"https://github.com/ThorPham.png","key":"thorpham"}],"frontMatter":{"slug":"T\xecm-hi\u1ec3u-eigenFace-trong-face-recognite","title":"T\xecm hi\u1ec3u eigenFace trong face recognite","authors":"thorpham","tags":["python","Face recognition"]},"prevItem":{"title":"Drowsiness detection v\u1edbi Dlib v\xe0 OpenCV","permalink":"/blog/Drowsiness-detection"},"nextItem":{"title":"T\xecm hi\u1ec3u regression trong object detection","permalink":"/blog/T\xecm-hi\u1ec3u-regression-trong-object-detection"}},"content":"*C\xf3 bao gi\u1edd b\u1ea1n v\xe0o facebook r\u1ed3i m\u1ed9t ng\xe0y n\u1ecd c\xf3 m\u1ed9t th\xf4ng b\xe1o hi\u1ec7n l\xean b\u1ea1n \u0111\u01b0\u1ee3c tag trong m\u1ed9t b\u01b0\u1edbc \u1ea3nh n\xe0o \u0111\xf3. \u0110\xe3 bao gi\u1edd b\u1ea1n ngh\u0129 l\xe0m sao \\nfacebook nh\u1eadn di\u1ec7n ra m\u1eb7t b\u1ea1n, m\xecnh c\u0169ng kh\xf4ng bi\u1ebft n\u1eefa v\xec t\u1ea5t c\u1ea3 thu\u1eadt to\xe1n c\u1ee7a n\xf3 l\xe0 \u0111i\u1ec1u b\xed m\u1eadt. Tuy v\u1eady v\u1eabn c\xf3 nhi\u1ec1u ph\u01b0\u01a1ng ph\xe1p nh\u1eadn \\ndi\u1ec7n gi\u01b0\u01a1ng m\u1eb7t \u0111\u01a1n gi\u1ea3n m\xe0 ta c\xf3 th\u1ec3 th\u1eed. B\xe0i n\xe0y ta s\u1ebd t\xecm hi\u1ec3u v\u1ec1 eigenface v\xe0 c\xf9ng m\u1ed9t model \u0111\u01a1n gi\u1ea3n v\u1edbi Opencv. Eigenface l\u1ea5y \xfd t\u01b0\u1edfng \u0111\u1eb1ng sau t\u1eeb PCA, ch\u1eafc c\u0169ng \u0111\xe3 c\xf3 nhi\u1ec1u ng\u01b0\u1eddi bi\u1ebft \u0111\u1ebfn ph\u01b0\u01a1ng ph\xe1p n\xe0y. PCA l\xe0 m\u1ed9t ph\u01b0\u01a1ng ph\xe1p gi\u1ea3m chi\u1ec1u d\u1eef li\u1ec7u, khi m\xe0 d\u1eef li\u1ec7u c\xf3 chi\u1ec1u l\u1edbn m\xe0 ch\xfang ta ch\u1ec9 c\xf3 th\u1ec3 visualize \u1edf chi\u1ec1u nh\u1ecf h\u01a1n 3 th\xec PCA s\u1ebd l\xe0 m\u1ed9t ph\u01b0\u01a1ng ph\xe1p gi\xfap ta \u0111\u1eefa data v\u1ec1 m\u1ed9t kh\xf4ng gian m\u1edbi(ta g\u1ecdi l\xe0 PCA space) m\xe0 v\u1eabn c\u1ed1 gi\u1eef l\u1ea1i \u0111\u01b0\u1ee3c th\xf4ng tin nhi\u1ec1u nh\u1ea5t c\xf3 th\u1ec3 tr\xean data.*\\n\x3c!--truncate--\x3e\\nN\u1ed9i dung b\xe0i vi\u1ebft : \\n1. T\xecm hi\u1ec3u v\u1ec1 PCA\\n2. T\xecm hi\u1ec3u EigenFace\\n3. Build model\\n\\n## 1. T\xecm hi\u1ec3u v\u1ec1 PCA\\nPCA l\xe0 m\u1ed9t trong nh\u1eefng ph\u01b0\u01a1ng ph\xe1p gi\u1ea3m chi\u1ec1u d\u1eef li\u1ec7u ( Dimensionality reduction techniques ) ph\u1ed5 bi\u1ebfn nh\u1ea5t v\xe0 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong nhi\u1ec1u l\u0129nh v\u1ef1c kh\xe1c nhau. PCA c\xf3 nhi\u1ec1u \u1ee9ng d\u1ee5ng nh\u01b0 t\xecm m\u1ed1i t\u01b0\u01a1ng quan gi\u1eefa c\xe1c bi\u1ebfn ( relationship between observation), tr\xedch xu\u1ea5t nh\u1eefng th\xf4ng\\ntin quan tr\u1ecdng t\u1eeb data,ph\xe1t hi\u1ec7n v\xe0 lo\u1ea1i b\u1ecf outlier v\xe0 gi\u1ea3m chi\u1ec1u chi\u1ec1u d\u1eef li\u1ec7u.\xdd t\u01b0\u1edfng c\u1ee7a ph\u01b0\u01a1ng ph\xe1p PCA l\xe0 t\xecm ra m\u1ed9t kh\xf4ng gian m\u1edbi\\n\u0111\u1ec3 chi\u1ebfu(project) data sao cho variation gi\u1eef l\u1ea1i l\xe0 nhi\u1ec1u nh\u1ea5t. Ta c\xf3 th\u1ec3 h\xecnh dung qua h\xecnh v\u1ebd d\u01b0\u1edbi \u0111\xe2y.\\n\\n\x3c!-- ![pca1](/assets/images/pca1.jpg) --\x3e\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./pca1.jpg\').default} />\\n</center>\\nC\xf3 2 ph\u01b0\u01a1ng ph\xe1p ti\u1ebfp c\u1eadn PCA l\xe0 covarian matrix v\xe0 SVD ch\xfang ta ch\u1ec9 t\xecm hi\u1ec3u v\u1ec1 covarian matrix trong b\xe0i n\xe0y .\\nPh\u01b0\u01a1ng ph\xe1p Covarian matrix : C\xe1c b\u01b0\u1edbc th\u1ef1c hi\u1ec7n thu\u1eadt to\xe1n nh\u01b0 sau :\\n\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./pca.jpg\').default} />\\n</center>\\n\\n* X data c\xf3 chi\u1ec1u MxN ( v\u1edbi N l\xe0 s\u1ed1 sample ,M l\xe0 s\u1ed1 feature).\\n* T\xednh mean c\u1ee7a X :\\n$$\\n\\\\mu = \\\\frac{1}{N}\\\\cdot\\\\sum_{i=1}^{N}x_{i}\\n$$\\n* Tr\u1eeb X v\u1edbi mean c\u1ee7a X :\\n$$\\nD = \\\\{d_{1},d_{2},..,d_{N}\\\\} = \\\\sum_{i=1}^{N}x_{i} - \\\\mu\\n$$\\n* T\xednh to\xe1n covarian :\\n\\n $$\\n \\\\sum = \\\\frac{1}{N-1}\\\\cdot D\\\\cdot D^{T}\\n $$\\n\\n* T\xednh to\xe1n EigenVector **V** v\xe0 EigenValue $\\\\lambda$ c\u1ee7a Covarian $\\\\sum$\\n* Sort EigenValue t\u01b0\u01a1ng \u1ee9ng v\u1edbi EigenVector theo th\u1ee9 t\u1ef1 $\\\\lambda$ gi\u1ea3m d\u1ea7n .\\n* Ch\u1ecdn nh\u1eefng EigenVector t\u01b0\u01a1ng \u1ee9ng v\u1edbi EigenValue l\u1edbn nh\u1ea5t $ W = \\\\{v_{1},v_{2},..v_{k}\\\\} $ . EigenVector W s\u1ebd l\xe0m \u0111\u1ea1i di\u1ec7n \u0111\u1ec3 project X v\xe0o PCA space\\n* T\u1ea5t c\u1ea3 sample X s\u1ebd \u0111\u01b0\u1ee3c project v\xe0o kh\xf4ng gian nh\u1ecf h\u01a1n theo c\xf4ng th\u01b0c $Y = W^{T}\\\\cdot D$\\n\\nL\u01b0u \xfd v\u1ec1 dimension c\xe1i bi\u1ebfn :\\n\\n <center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./dimension.jpg\').default} />\\n</center>\\n\\nX\xe2y d\u1ef1ng PCA space :\\n  * \u0110\u1ec3 x\xe2y d\u1ef1ng kh\xf4ng gian nh\u1ecf h\u01a1n (t\u1eeb M th\xe0nh k), trong \u0111\xf3 k l\xe0 s\u1ed1 eigen value m\xe0 ta ch\u1ecdn. Khi \u0111\xf3 PCA space \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a l\xe0 \\n  $ W = \\\\{v_{1},v_{2},..,v_{k}\\\\} . Ta vi\u1ebft l\u1ea1i bi\u1ebfn Y l\xe0 project c\u1ee7a X qua W nh\u01b0 sau :\\n  $$\\n  Y = W_{T} \\\\cdot D = \\\\sum_{i=1}^{N}(x_{i} - \\\\mu)\\n  $$\\n## T\xecm hi\u1ec3u EigenFace \\nEigenFace Hi\u1ec3u m\u1ed9t c\xe1ch \u0111\u01a1n gi\u1ea3n l\xe0 n\xf3 dung PCA l\xe0 feature extraction sau \u0111\xf3 m\u1edbi \u0111\u01b0a v\xe0o model \u0111\u1ec3 training. M\u1ed7i image c\xf3 chi\u1ec1u ch\u1eb3ng h\u1ea1n 28x28 = 784 pixel n\u1ebfu \u0111\u01b0a h\u1ebft v\xe0o model th\xec c\xf3 m\u1ed9t s\u1ed1 nh\u01b0\u1ee3c \u0111i\u1ec3m sau :\\n* Th\u1eddi gian training l\xe2u v\xec chi\u1ec1u d\u1eef li\u1ec7u l\u1edbn\\n* Kh\xf4ng ph\u1ea3i t\u1ea5t c\u1ea3 c\xe1c v\u1ecb tr\xed tr\xean image \u0111\u1ec1u quan tr\u1ecdng\\nV\xec v\u1eady PCA gi\xfap ta kh\u1eafc ph\u1ee5c c\xe1c nh\u01b0\u1ee3c \u0111i\u1ec3m n\xe0y , n\xf3 gi\xfap ta gi\u1ea3m chi\u1ec1u d\u1eef li\u1ec7u m\xe0 v\u1eabn gi\u1eef l\u1ea1i \u0111\u01b0\u1ee3c nh\u1eefng th\xf4ng tin quan tr\u1ecdng tr\xean image.\\nC\xe1c b\u01b0\u1edbc th\u1ef1c hi\u1ec7n thu\u1eadt to\xe1n :\\n* Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u : Face n\xean \u0111\u01b0\u1ee3c alignment v\xe0 c\xf3 c\xf9ng k\xedch th\u01b0\u1edbc NxN sau \u0111\xf3 chu\u1ea9n h\xf3a b\u1eb1ng c\xe1ch chia 255.\\n* Image sau \u0111\xf3 \u0111\u01b0\u1ee3c Flatten th\xe0nh 1xN^2 pixel, ch\xfang ta c\xf3 M image n\xean data s\u1ebd c\xf3 chi\u1ec1u MxN^2\\n* Sau \u0111\xf3 ch\xfang ta t\xednh mean v\xe0 t\xednh to\xe1n covariance nh\u01b0 \u1edf thu\u1eadt to\xe1n PCA \u1edf tr\xean\\n* \u0110i\u1ec3m kh\xe1c bi\u1ec7t \u1edf \u0111\xe2y l\xe0 covarian c\xf3 chi\u1ec1u N^2xN^2 qu\xe1 l\u1edbn \u0111\u1ec3 t\xednh tr\u1ef1c ti\u1ebfp eigen vector v\xe0 egien value n\xean c\xf3 1 c\xe1i trick \u1edf \u0111\xe2y \u0111\xf3\\nl\xe0 ng\u01b0\u1eddi ta s\u1ebd t\xednh eigen vector c\u1ee7a MxM( v\xec MxM c\xf3 k\xedch th\u01b0\u1edbc nh\u1ecf h\u01a1n nhi\u1ec1u so v\u1edbi N^2xN^2). Sau \u0111\xf3 t\xednh ng\u01b0\u1ee3c l\u1ea1i cho N^2xN^2\\n* Cu\u1ed1i c\xf9ng ch\u1ecdn s\u1ed1 eigen vector \u0111\u1ec3 chi\u1ebfu data sang kh\xf4ng gian m\u1edbi l\u1ea5y n\xf3 l\xe0 feature \u0111\u1ec3 training.\\n## Build model\\nCh\xfang ta c\xf3 th\u1ec3 t\u1ef1 x\xe2y d\u1ef1ng model ho\u1eb7c d\xf9ng th\u01b0 vi\u1ec7n c\xf3 s\u1eb5n trong opencv ch\u1eb3ng h\u1ea1n.Data set b\u1ed9 data 2k image 12 ca s\u1ef9 vi\u1ec7t nam \u0111\xe3 aligment\\nCode v\u1edbi sklearn :\\n```py\\nimport numpy as np\\nimport os\\nimport glob\\nimport cv2\\n\\nnames = [\\"bao thy\\",\\"chi pu\\",\\"dam vinh hung\\",\\"dan truong\\",\\"ha anh tuan\\",\\"ho ngoc ha\\",\\n         \\"huong tram\\",\\"lam truong\\",\\"my tam\\",\\"No phuoc thing\\",\\"son tung\\",\\"tuan hung\\"]\\nname_index = {name:index for index,name in enumerate(names)}\\nindex_name = {index:name for index,name in enumerate(names)}\\n\\nlabel = []\\ndata = []\\nfor name in names :\\n    paths = glob.glob(\\".//\\" +name +\\"//*.png\\")\\n    for path in paths:\\n        image = cv2.imread(path,0)\\n        data.append(image.flatten())\\n        label.append(name_index[name])\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import SVC\\nfrom sklearn.decomposition import RandomizedPCA\\nfrom sklearn.metrics import accuracy_score\\n\\nmean = np.mean(data,axis=0)\\ndata_normal = data-mean\\npca = PCA(n_components=100,svd_solver=\'randomized\',whiten=True)\\nX = pca.fit_transform(data_normal)\\n==> split data to train and test\\nX_train,X_test, y_train,y_test = train_test_split(X,label,test_size=0.3,shuffle=True)\\n==> build model\\nsvm = SVC(C=10)\\nsvm.fit(X_train,y_train)\\ny_pre = svm.predict(X_test)\\nprint(accuracy_score(y_test,y_pre))\\n```\\nAccuarcy ch\u1ec9 c\xf3 62% th\xf4i ha. T\u01b0\u01a1ng \u0111\u1ed1i th\u1ea5p v\xec PCA l\xe0 1 feature extraction d\u1ea1ng shadow learning n\xean feature ch\u1ec9 l\xe0m vi\u1ec7c t\u1ed1t \u0111\u1ed1i v\u1edbi\\nnh\u1eefng image c\xf3 s\u1ef1 kh\xe1c bi\u1ec7t l\u1edbn v\u1ec1 structer and texture nh\u01b0 ch\xf3 m\xe8o.. C\xf2n face th\xec kh\xf3 h\u01a1n ta c\xf3 th\u1ec3 d\xf9ng c\xe1c k\u1ef9 thu\u1eadt feature c\u1ee7a deep learning \u0111\u1ec3 training. B\u1ea1n c\xf3 th\u1ec3 \u0111\u1ecdc \u1edf b\xe0i face veritication :\\n* Build v\u1edbi Opencv\\n* T\u01b0\u01a1ng \u0111\u1ed1i \u0111\u01a1n gi\u1ea3n n\xean m\xecnh ch\u1ec9 show g\u1ee3i \xfd th\xf4i :\\n* \u0110\u1ea7u ti\xean kh\u1edfi t\u1ea1o model v\xe0 training model nh\u01b0 sau :\\n```py\\nrecognizer = cv2.face.EigenFaceRecognizer_create()\\nrecognizer.train(Faces,IDs)\\n```\\nSau \u0111\xf3 l\u01b0u file d\u01b0\u1edbi d\u1ea1ng yml :`recognizer.save(\\"recognier.yml\\")`\\nCu\u1ed1i c\xf9ng l\xe0 recognition realtime\\n```py\\ncascade = cv2.CascadeClassifier(\\"haarcascade_frontalface_alt.xml\\")\\n # Use urllib to get the image and convert into a cv2 usable format\\nimage = cv2.imread(\\"my_tam.png\\")\\nframe_gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\\ndetection = cascade.detectMultiScale(frame_gray,scaleFactor=1.3,minNeighbors=5)\\nfor (x,y,w,h) in detection:\\n    cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)\\n    roi = frame_gray[y:y+h,x:x+w]\\n    roi = imutils.resize(roi,width=96,height=96)\\n    single = recognizer.predict(roi)[0]\\n    print(index_name[single])\\n    cv2.putText(image,index_name[single],(int(x),int(y-10)),cv2.FONT_HERSHEY_COMPLEX_SMALL,2,(0,0,255),2)\\ncv2.imshow(\\"frame\\",image)\\ncv2.waitKey()\\ncv2.destroyAllWindows()\\n```\\nTham kh\u1ea3o : http://blog.manfredas.com/eigenfaces-tutorial/,b\xe0i gi\u1ea3ng computer vision c\u1ee7a th\u1ea7y Mubarak"},{"id":"T\xecm-hi\u1ec3u-regression-trong-object-detection","metadata":{"permalink":"/blog/T\xecm-hi\u1ec3u-regression-trong-object-detection","editUrl":"https://github.com/ThorPham/blog/2018-4-18-T\xecm-hi\u1ec3u-regression-trong-object-detection/index.md","source":"@site/blog/2018-4-18-T\xecm-hi\u1ec3u-regression-trong-object-detection/index.md","title":"T\xecm hi\u1ec3u regression trong object detection","description":"*L\u1ea7n \u0111\u1ea7u ti\xean m\xecnh \u0111\u1ecdc v\u1ec1 thu\u1eadt to\xe1n YOLO(you look only one) l\xe0 tr\xean kh\xf3a \\"Convolution neural network\\" c\u1ee7a th\u1ea7y Andrew Ng tr\xean coursera.","date":"2018-04-18T00:00:00.000Z","formattedDate":"April 18, 2018","tags":[{"label":"python","permalink":"/blog/tags/python"},{"label":"Object detection","permalink":"/blog/tags/object-detection"}],"readingTime":5.125,"truncated":true,"authors":[{"name":"Thorpham","title":"Deep learning enthusiast","url":"https://github.com/ThorPham","imageURL":"https://github.com/ThorPham.png","key":"thorpham"}],"frontMatter":{"slug":"T\xecm-hi\u1ec3u-regression-trong-object-detection","title":"T\xecm hi\u1ec3u regression trong object detection","authors":"thorpham","tags":["python","Object detection"]},"prevItem":{"title":"T\xecm hi\u1ec3u eigenFace trong face recognite","permalink":"/blog/T\xecm-hi\u1ec3u-eigenFace-trong-face-recognite"},"nextItem":{"title":"Nh\u1eadn di\u1ec7n pedestrian v\u1edbi window search","permalink":"/blog/Nh\u1eadn-di\u1ec7n-pedestrian-v\u1edbi-window-search"}},"content":"*L\u1ea7n \u0111\u1ea7u ti\xean m\xecnh \u0111\u1ecdc v\u1ec1 thu\u1eadt to\xe1n YOLO(you look only one) l\xe0 tr\xean kh\xf3a \\"Convolution neural network\\" c\u1ee7a th\u1ea7y Andrew Ng tr\xean coursera.\\nC\xf3 h\xe0ng ng\xe0n c\xe2u h\u1ecfi v\xec sao \u1edf trong \u0111\u1ea7u m\xecnh hi\u1ec7n ra d\xf9 \u0111i h\u1ecfi kh\u1eafp n\u01a1i m\xe0 nhi\u1ec1u trong s\u1ed1 \u0111\xf3 v\u1eabn ch\u01b0a c\xf3 l\u1eddi gi\u1ea3i \u0111\xe1p th\u1ecfa m\xe3n m\xecnh. Trong \u0111\xf3 c\xf3 key word `Bounding-box regression`, m\xecnh suy ngh\u0129 r\u1ea5t nhi\u1ec1u, \u0111\u1ecdc c\u0169ng kha kh\xe1 b\xe0i vi\u1ebft tr\xean m\u1ea1ng m\xe0 v\u1eabn kh\xf4ng hi\u1ec3u n\u1ed5i. M\u1ed9t c\xe2u h\u1ecfi c\u1ee9 l\u1edfn v\u1edfn trong \u0111\u1ea7u m\xecnh l\xe0 c\xe1c `bouding box` trong thu\u1eadt to\xe1n yolo \u0111\u01b0\u1ee3c t\u1ea1o ra nh\u01b0 th\u1ebf n\xe0o ta, tr\u01b0\u1edbc gi\u1edd m\xecnh ch\u1ec9 d\xf9ng regression \u0111\u1ec3 predict  c\xe1c bi\u1ebfn li\xean t\u1ee5c v\u1eady h\u1ecd \xe1p d\u1ee5ng \u0111\u1ec3 detection bounding box ra sao. Ng\u01b0\u1eddi ta build yolo l\xe0 t\u1ed5ng h\u1ee3p c\u1ee7a r\u1ea5t nhi\u1ec1u thu\u1eadt to\xe1n t\u1ea1o n\xean b\u1ed9 x\u01b0\u01a1ng cho yolo .Thi\u1ebft ngh\u0129 nh\u1eefng ng\u01b0\u1eddi m\u1edbi l\u1ea7n \u0111\u1ea7u t\u1eadp t\u1ecde v\xe0o deep learning nh\u01b0 m\xecnh th\xec n\xean chia yolo t\u1eebng ph\u1ea7n \u0111\u1ec3 x\u1eed l\xfd c\xf3 l\u1ebd s\u1ebd d\u1ec5 th\u1edf h\u01a1n. Trong b\xe0i h\xf4m nay m\xecnh s\u1ebd l\xe0m r\xf5 `bounding box` \u0111\u01b0\u1ee3c t\u1ea1o ra t\u1eeb regression nh\u01b0 th\u1ebf n\xe0o b\u1eb1ng m\u1ed9t v\xed d\u1ee5 r\u1ea5t \u0111\u01a1n gi\u1ea3n.*\\n\x3c!--truncate--\x3e\\nC\xe1c b\u01b0\u01a1c th\u1ef1c hi\u1ec7n :\\n* Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u .\\n* Traing model .\\n* \u0110\xe1nh gi\xe1 model\\n  \\n## I.Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u .\\nD\u1eef li\u1ec7u `input` l\xe0 nh\u1eefng image c\xf3 object m\xe0 ta mu\u1ed1n detection v\xe0 `ouput` l\xe0 nh\u1eefng bouding box s\u1ebd c\xf3 d\u1ea1ng (x,y,w,h). Trong \u0111\xf3 x,y l\xe0 t\u1ecda \u0111\u1ed9 leftop c\u1ee7a bounding box, (w,h) l\xe0 width v\xe0 height. Ch\xfang ta s\u1ebd m\xf4 ph\u1ecfng d\u1eef li\u1ec7u nh\u01b0 sau :\\n```py\\nnp.random.seed(10)\\nnumber_data = 5000\\nimg_size = 8\\nmin_size_obj = 1\\nmax_size_obj = 4\\nnumber_obj = 1\\n# x l\xe0 dataset image, y l\xe0 label v\u1edbi 4 tham s\u1ed1(x,y,w.h)\\nbboxes = np.zeros((5000,1,4))\\nimage = np.zeros((5000,img_size,img_size))\\nfor i in range(5000):\\n    for obj in range(number_obj):\\n        w,h = np.random.randint(min_size_obj,max_size_obj,size = 2)\\n        x = np.random.randint(0,img_size-w)\\n        y = np.random.randint(0,img_size-h)\\n        bboxes[i,obj,:] = (x,y,w,h)\\n        image[i,y:y+h,x:x+w] = 1\\n```\\nGi\u1ea3i th\xedch m\u1ed9t t\xed :\\n* Ta s\u1ebd t\u1ea1o 5000 image c\xf3 size (8,8) `image = np.zeros((5000,img_size,img_size))`. Image s\u1ebd c\xf3 background l\xe0 white\\n* 5000 `bounding box` c\xf3 size t\u1eeb w,h t\u1eeb 1-4 v\xe0 c\xf3 m\xe0u \u0111en\\n* M\u1ed7i image ch\u1ec9 c\xf3 duy nh\u1ea5t 1 object\\nImage sau khi t\u1ea1o s\u1ebd nh\u01b0 th\u1ebf n\xe0y :\\n```py\\nplt.figure(figsize=(15,15))\\nplt.axis(\\"off\\")\\nfor i in range(4):\\n    plt.subplot(1,4,i+1)\\n    plt.imshow(image[i],cmap=\\"Greys\\",interpolation=\'none\', origin=\'lower\', extent=[0, img_size, 0, img_size])\\n    for bbox in bboxes[i]:\\n        plt.gca().add_patch(matplotlib.patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], ec=\'r\', fc=\'none\'))\\n```\\n![bounding_box](./bounding.jpg)\\nC\xe1i ch\xfang ta c\u1ea7n predict l\xe0 \u0111\u01b0\u1eddng vi\u1ec1n m\xe0u \u0111\u1ecf. Image s\u1ebd c\xf3 dimension l\xe0 (5000, 8, 8) ,bounding box c\xf3 dimension l\xe0 (5000, 1, 4).\\nCh\xfang ta s\u1ebd d\xf9ng m\u1ed9t m\u1ea1ng neural network \u0111\u01a1n gi\u1ea3n \u0111\u1ec3 training v\u1edbi library keras. \u1ede \u0111\xe2y ng\u01b0\u1eddi ta g\u1ecdi `Bounding-box regression` trong khi d\xf9ng neural network training, r\u1ea5t nhi\u1ec1u ng\u01b0\u1eddi l\u1ea7m t\u01b0\u1edfng l\xe0 d\xf9ng `simple regression`. H\xe3y m\u1edf r\u1ed9ng kh\xe1i ni\u1ec7m `regression` ra m\u1ed9t t\xed, n\xf3 l\xe0 b\xe0i to\xe1n predict khi output l\xe0 bi\u1ebfn li\xean t\u1ee5c. V\xec bounding box \u1edf \u0111\xe2y (x,y,w,h) l\xe0 b\u1ed1n bi\u1ebfn li\xean t\u1ee5c n\xean ta g\u1ecdi l\xe0 b\xe0i to\xe1n regression.\\n\u0110\u1ea7u ti\xean ch\xfang ta s\u1ebd reshape c\xe1c bi\u1ebfn tr\u01b0\u1edbc khi \u0111\u01b0a v\xe0o model. C\u0169ng c\xf3 th\u1ec3 normalizer tr\u01b0\u1edbc khi training \u0111\u1ec3 thu\u1eadt to\xe1n h\u1ed9i t\u1ee5 nhanh h\u01a1n. Nh\u01b0ng do \u1ea3nh k\xedch th\u01b0\u1edbc nh\u1ecf v\xe0 l\xe0 binary n\xean kh\xf4ng c\u1ea7n thi\u1ebft . Sau \u0111\xf3 chia d\u1eef li\u1ec7u th\xe0nh training v\xe0 testing v\u1edbi test_size = 0.3\\n\\n```py\\nfrom sklearn.model_selection import train_test_split\\nX = image.reshape((5000,-1))\\ny = bboxes.reshape((5000,-1))\\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=10)\\n```\\n\\n* X s\u1ebd c\xf3 chi\u1ec1u l\xe0 (5000,64) \\n* y c\xf3 chi\u1ec1u l\xe0 (5000,4)\\n## II.Build model.\\n\\n```py\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense,Dropout,Activation\\n\\nmodel = Sequential()\\nmodel.add(Dense(300,input_dim =64))\\nmodel.add(Dense(100))\\nmodel.add(Dropout(0.2))\\nmodel.add(Activation(\\"relu\\"))\\nmodel.add(Dense(4))\\nmodel.compile(optimizer=\\"adadelta\\",loss=\\"mse\\")\\nmodel.summary()\\n```\\n\\n* Ta d\xf9ng 2 layers : layer 1 l\xe0 300 node,layer 2 l\xe0 100 node v\u1edbi activation l\xe0 `relu`.Cu\u1ed1i c\xf9ng l\xe0 m\u1ed9t layer `dropout` v\u1edbi t\u1ec9 l\u1ec7 20%\\n* Optimizer b\u1eb1ng `adadelta` v\xe0 loss l\xe0 `mean square error`.\\n  \\n\x3c!-- ![summary](./summary.jpg) --\x3e\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./summary.jpg\').default} />\\n</center>\\n\\n## III.Traing model\\n\\n```py\\nmodel.fit(X_train,y_train,epochs=50,batch_size=200)\\ny_predict = model.predict(X_test)\\n```\\n* Training model v\u1edbi 50 epochs v\xe0 batch size m\u1ed7i epochs l\xe0 200. M\xe1y ch\u1ea1y cpu t\u1ea7m ch\u01b0a \u0111\u1ebfn 1p\\n* Sau \u0111\xf3 predict test data d\u01b0\u1edbi variable y_predict\\n  \\n\x3c!-- ![training](./training.jpg)\\n --\x3e\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./training.jpg\').default} />\\n</center>\\n\\n\\n## \u0110\xe1nh gi\xe1 model\\nNh\xecn v\xe0o h\xecnh v\u1ebd \u0111\u1ea7u ti\u1ec1n ta c\xf3 nh\u1eadn x\xe9t l\xe0 : model predict t\u1ed1t l\xe0 khi \u0111\u01b0\u1eddng vi\u1ec1n m\xe0u \u0111\u1ecf v\xe0  `bounding box` n\xf3 c\xe0ng s\xe1t nhau . Nh\u01b0 v\u1eady\\nta c\xf3 th\u1ec3 d\xf9ng c\xe1i n\xe0y \u0111\u1ec3 \u0111\xe1nh gi\xe1 model. Ta \u0111\xe3 quen v\u1edbi kh\xe1i ni\u1ec7m `IOU` l\xe0 `Intersection over Union`. C\xf3 ngh\u0129a l\xe0 ta s\u1ebd \u0111\xe1nh gi\xe1 model b\u1eb1ng t\u1ec9 l\u1ec7 area overlap v\u1edbi area union gi\u1eefa th\u1ef1c t\u1ebf v\xe0 predict. Sau \u0111\xf3 t\xednh mean l\xe0 s\u1ebd ra \u0111\u01b0\u1ee3c t\u1ec9 l\u1ec7 IOU c\u1ee7a model . IOU c\xe0ng cao c\xf3 ngh\u0129a model predict t\u1ed1t v\xe0 ng\u01b0\u1ee3c l\u1ea1i.\\n* X\xe2y d\u1ef1ng m\u1ed9t function t\xednh IOU.\\n```py\\ndef overlaping_area(detection_1,detection_2):\\n    \\n    x_1 = detection_1[0]\\n    y_1 = detection_1[1]\\n    x_w_1 = detection_1[0] + detection_1[2]\\n    y_h_1 = detection_1[1] + detection_1[3]\\n    \\n    x_2 = detection_2[0]\\n    y_2 = detection_2[1]\\n    x_w_2 = detection_2[0] + detection_2[2]\\n    y_h_2 = detection_2[1] + detection_2[3]\\n    # t\xednh overlap theo ox,oy .N\u1ebfu ko giao nhau tr\u1ea3 v\u1ec1 0\\n    overlap_x = max(0,min(x_w_1,x_w_2) - max(x_1,x_2))\\n    overlap_y = max(0,min(y_h_1,y_h_2) - max(y_1,y_2))\\n    # t\xednh area overlap\\n    overlap_area = overlap_x*overlap_y\\n    # t\xednh total area h\u1ee3p c\u1ee7a 2 detection\\n    total_area = detection_1[2]*detection_1[3] + detection_2[2]*detection_2[3] - overlap_area\\n    \\n    return np.round(overlap_area/float(total_area),3)\\n```\\nC\xe0i n\xe0y m\xecnh \u0111\xe3 \u0111\u1ec1 c\u1eadp nhi\u1ec1u trong b\xe0i vi\u1ebft tr\u01b0\u1edbc n\xean kh\xf4ng nh\u1eafc l\u1ea1i \u1edf \u0111\xe2y.\\nEvaluation model b\u1eb1ng IOU\\n```py\\nX_test_image = X_test.reshape((-1,8,8))\\nbbox_y = y_predict.reshape((-1,1,4))\\nbbx_y_true = y_test.reshape((-1,1,4))\\nplt.figure(figsize=(15,15))\\nfor i in range(4):\\n    iou = overlaping_area(bbox_y[i].flatten(),bbx_y_true[i].flatten())\\n    plt.subplot(1,4,i+1)\\n    plt.imshow(X_test_image[i],cmap=\\"Greys\\",interpolation=\'none\', origin=\'lower\', extent=[0, img_size, 0, img_size])\\n    for bbox in bbox_y[i]:\\n        plt.gca().add_patch(matplotlib.patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], ec=\'r\', fc=\'none\'))\\n        plt.title(\\"IOU = \\"+str(iou),color=\'blue\')\\n```\\n![evaluation](./evaluation.jpg)\\n```py\\nIOU = []\\nfor i in range(len(X_test)):\\n    iou = overlaping_area(bbox_y[i].flatten(),bbx_y_true[i].flatten())\\n    IOU.append(iou)\\nnp.mean(IOU)\\n```\\nTa t\xednh \u0111\u01b0\u1ee3c mean cua IOU `0.79` t\u1ee9c 79% t\u01b0\u01a1ng \u0111\u1ed1i t\u1ed1t, ch\xfang ta c\xf3 th\u1ec3 c\u1ea3i thi\u1ec7n model b\u1eb1ng m\u1ed9t s\u1ed1 c\xe1ch nh\u01b0 : normalizer data tr\u01b0\u1edbc khi training, thay \u0111\u1ed5i s\u1ed1 node tr\xean m\u1ed7i layer ho\u1eb7c thay \u0111\u1ed5i active fuction"},{"id":"Nh\u1eadn-di\u1ec7n-pedestrian-v\u1edbi-window-search","metadata":{"permalink":"/blog/Nh\u1eadn-di\u1ec7n-pedestrian-v\u1edbi-window-search","editUrl":"https://github.com/ThorPham/blog/2018-4-11-Nh\u1eadn-di\u1ec7n -pedestrian-v\u1edbi-window-search/index.mdx","source":"@site/blog/2018-4-11-Nh\u1eadn-di\u1ec7n -pedestrian-v\u1edbi-window-search/index.mdx","title":"Nh\u1eadn di\u1ec7n pedestrian v\u1edbi window search","description":"Object regconite bao g\u1ed3m 2 ph\u1ea7n vi\u1ec7c \u0111\xf3 l\xe0 object classifier v\xe0  object detection. Hi\u1ec3u m\u1ed9t c\xe1ch \u0111\u01a1n gi\u1ea3n \u0111\xf3 l\xe0 n\u1ebfu ch\xfang ta mu\u1ed1n m\xe1y t\xednh nh\u1eadn d\u1ea1ng \u0111\u01b0\u1ee3c con m\xe8o hay con ch\xf3 th\xec tr\u01b0\u1edbc ti\xean n\xf3 s\u1ebd ph\u1ea3i detecter \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\xf3 tr\xean image v\xe0 sau \u0111\xf3 xem \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\xf3 l\xe0 c\xe1i g\xec b\u1eb1ng c\xe1ch classifier .V\u1edbi s\u1ef1 ph\xe1t tri\u1ec3n c\u1ee7a deep learning nh\u01b0 hi\u1ec7n nay \u0111\xe3 c\xf3 r\u1ea5t nhi\u1ec1u thu\u1eadt to\xe1n gi\xfap ta gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 n\xe0y nh\u01b0 R-CNN,Fast or Faster R-CNN,YOLO hay SSD v\u1edbi t\u1ed1c \u0111\u1ed9 x\u1eed l\xfd nhanh v\xe0 \u0111\u1ed9 ch\xednh x\xe1c cao. Tuy v\u1eady nh\u1eefng c\xe1ch truy\u1ec1n th\u1ed1ng v\u1eabn l\xe0 s\u1ef1 l\u1ef1a chon t\u1ed1t khi m\xe0 ch\xfang ta c\xf3 \xedt d\u1eef li\u1ec7u v\xe0 mu\u1ed1n build m\u1ed9t model n\xe0o \u0111\xf3 \u0111\u01a1n gi\u1ea3n h\u01a1n nh\u1eefng c\xe1i ph\u1ee9c t\u1ea1p h\u01a1n nh\u01b0 deep learning. Trong b\xe0i n\xe0y ch\xfang ta s\u1ebd nh\u1eadn di\u1ec7n pedestrian b\u1eb1ng ph\u01b0\u01a1ng ph\xe1p c\u1ed5 \u0111i\u1ec3n trong computer vision v\xe0 sau \u0111\xf3 b\u1ea1n c\xf3 th\u1ec3 t\u1ef1 build m\u1ed9t model custom n\xe0o \u0111\xf3 theo \xfd c\u1ee7a b\u1ea1n .Thu\u1eadt to\xe1n s\u1eed d\u1ee5ng trong b\xe0i l\xe0 HOG + SVM + Window search.","date":"2018-04-11T00:00:00.000Z","formattedDate":"April 11, 2018","tags":[{"label":"opencv","permalink":"/blog/tags/opencv"},{"label":"python","permalink":"/blog/tags/python"},{"label":"computer vision","permalink":"/blog/tags/computer-vision"}],"readingTime":7.53,"truncated":true,"authors":[{"name":"Thorpham","title":"Deep learning enthusiast","url":"https://github.com/ThorPham","imageURL":"https://github.com/ThorPham.png","key":"thorpham"}],"frontMatter":{"slug":"Nh\u1eadn-di\u1ec7n-pedestrian-v\u1edbi-window-search","title":"Nh\u1eadn di\u1ec7n pedestrian v\u1edbi window search","authors":"thorpham","tags":["opencv","python","computer vision"]},"prevItem":{"title":"T\xecm hi\u1ec3u regression trong object detection","permalink":"/blog/T\xecm-hi\u1ec3u-regression-trong-object-detection"},"nextItem":{"title":"T\xecm hi\u1ec3u v\u1ec1 Word2Vec","permalink":"/blog/T\xecm-hi\u1ec3u-v\u1ec1-Word2Vec"}},"content":"*Object regconite bao g\u1ed3m 2 ph\u1ea7n vi\u1ec7c \u0111\xf3 l\xe0 object classifier v\xe0  object detection. Hi\u1ec3u m\u1ed9t c\xe1ch \u0111\u01a1n gi\u1ea3n \u0111\xf3 l\xe0 n\u1ebfu ch\xfang ta mu\u1ed1n m\xe1y t\xednh nh\u1eadn d\u1ea1ng \u0111\u01b0\u1ee3c con m\xe8o hay con ch\xf3 th\xec tr\u01b0\u1edbc ti\xean n\xf3 s\u1ebd ph\u1ea3i detecter \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\xf3 tr\xean image v\xe0 sau \u0111\xf3 xem \u0111\u1ed1i t\u01b0\u1ee3ng \u0111\xf3 l\xe0 c\xe1i g\xec b\u1eb1ng c\xe1ch classifier .V\u1edbi s\u1ef1 ph\xe1t tri\u1ec3n c\u1ee7a deep learning nh\u01b0 hi\u1ec7n nay \u0111\xe3 c\xf3 r\u1ea5t nhi\u1ec1u thu\u1eadt to\xe1n gi\xfap ta gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 n\xe0y nh\u01b0 R-CNN,Fast or Faster R-CNN,YOLO hay SSD v\u1edbi t\u1ed1c \u0111\u1ed9 x\u1eed l\xfd nhanh v\xe0 \u0111\u1ed9 ch\xednh x\xe1c cao. Tuy v\u1eady nh\u1eefng c\xe1ch truy\u1ec1n th\u1ed1ng v\u1eabn l\xe0 s\u1ef1 l\u1ef1a chon t\u1ed1t khi m\xe0 ch\xfang ta c\xf3 \xedt d\u1eef li\u1ec7u v\xe0 mu\u1ed1n build m\u1ed9t model n\xe0o \u0111\xf3 \u0111\u01a1n gi\u1ea3n h\u01a1n nh\u1eefng c\xe1i ph\u1ee9c t\u1ea1p h\u01a1n nh\u01b0 deep learning. Trong b\xe0i n\xe0y ch\xfang ta s\u1ebd nh\u1eadn di\u1ec7n pedestrian b\u1eb1ng ph\u01b0\u01a1ng ph\xe1p c\u1ed5 \u0111i\u1ec3n trong computer vision v\xe0 sau \u0111\xf3 b\u1ea1n c\xf3 th\u1ec3 t\u1ef1 build m\u1ed9t model custom n\xe0o \u0111\xf3 theo \xfd c\u1ee7a b\u1ea1n .Thu\u1eadt to\xe1n s\u1eed d\u1ee5ng trong b\xe0i l\xe0 HOG + SVM + Window search.*\\n\x3c!--truncate--\x3e\\nC\xe1ch b\u01b0\u1edbc th\u1ef1c hi\u1ec7n ta chia l\xe0m 2 giai \u0111o\u1ea1n t\u01b0\u01a1ng \u1ee9ng v\u1edbi classifier v\xe0 detecter :\\n\\n**Giai \u0111o\u1ea1n 1 classifier**\\n    1. Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u\\n    2. Tr\xedch ch\u1ecdn \u0111\u1eb7c tr\u01b0ng\\n    3. Build model\\n    4. \u0110\xe1nh gi\xe1 v\xe0 c\u1ea3i thi\u1ec7n model\\n\\n**Giai \u0111o\u1ea1n 2  Detection**\\n    1. X\xe2y d\u1ef1ng sliding window\\n    2. X\xe2y d\u1ef1ng NMS(non-maxinum-suppression)\\n    3. Detecter\\n## Giai \u0111o\u1ea1n 1 classifier\\n\\n### 1. Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u\\nD\u1eef li\u1ec7u ch\xfang ta c\u1ea7n chu\u1ea9n b\u1ecb g\u1ed3m 2 ph\u1ea7n . M\u1ed9t l\xe0 positive sample ( g\u1ecdi t\u1eaft l\xe0 pos) l\xe0 data pedestrian v\xe0 ch\xfang ta g\u1eafn label cho n\xf3 l\xe0 1. Th\u1ee9 hai l\xe0 negative sample (Neg) l\xe0 d\u1eef li\u1ec7u kh\xf4ng ch\u1ee9a pedestrian b\u1ea1n c\xf3 th\u1ec3 l\u1ea5y nh\u01b0 background, car, house ... v\xe0 ta g\u1eafn nh\xe3n l\xe0 -1.(l\u01b0u \xfd n\u1ebfu training trong opecv th\xec nh\xe3n g\u1eafn b\u1eaft bu\u1ed9c l\xe0 1 v\xe0 -1 ).\\n```py\\n# image positive\\npath_pos = glob.glob(\\"./pedestrians128x64/\\"+\\"*.ppm\\")\\nplt.subplots(figsize =(10,5))\\nfor i in range(6):\\n    image1 = io.imread(path_pos[i])\\n    plt.subplot(1,6,i +1)\\n    io.imshow(image1)\\n# image negative\\npath_neg = glob.glob(\\"./pedestrians_neg/\\"+\\"*.jpg\\")\\n```\\nD\u1eef li\u1ec7u c\u1ee7a ta g\u1ed3m c\xf3 924 image pos c\xf3 chi\u1ec1u (128, 64, 3) v\xe0 ta s\u1ebd t\u1ea1o (15x50) image neg c\xf3 chi\u1ec1u (128, 64, 3).\\n\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./pedestian1.jpg\').default} />\\n</center>\\n\\n### 2. Tr\xedch ch\u1ecdn \u0111\u1eb7c tr\u01b0ng \\nTa s\u1ebd d\xf9ng hog \u0111\u1ec3 tr\xedch ch\u1ecdn \u0111\u1eb7c tr\u01b0ng\\n\\n```py\\ndef hog_feature(image):\\n    feature_hog = hog(image,orientations=9,pixels_per_cell=(8,8),\\n    cells_per_block=(2,2),block_norm=\\"L2\\")\\n    return feature_hog\\n    \\n#feature extraction for image pos    \\nX_pos = []\\ny_pos = []\\nfor path in path_pos :\\n    im = io.imread(path,as_grey=True)\\n    im_feature = hog_feature(im)\\n    X_pos.append(im_feature)\\n    y_pos.append(1)\\n    \\n#feature extraction for image neg\\nX_neg = []\\ny_neg = []\\nw = 64\\nh = 128\\nfor path in path_neg :\\n    im = io.imread(path,as_grey=True)\\n    for j in range(15):\\n        x = np.random.randint(0,im.shape[1]-w)\\n        y = np.random.randint(0,im.shape[0]-h)\\n        im_crop = im[y:y+h,x:x+w]\\n        im_feature = hog_feature(im_crop)\\n        X_neg.append(im_feature)\\n        y_neg.append(-1)\\n        \\n```\\n\u0110\u1ea7u ti\xean ta \u0111\u1ecbnh ngh\u0129a 1 function t\xednh hog g\u1ed3m c\xe1c tham s\u1ed1 `orientations=9,pixels_per_cell=(8,8),cells_per_block=(2,2),block_norm=\\"L2\\"`\\nSau \u0111\xf3 t\xednh hog tr\xean pos v\xe0 neg sample\\nCu\u1ed1i c\xf9ng ta stack pos v\xe0 neg l\u1ea1i \u0111\u1ec3 chu\u1ea9n b\u1ecb training\\n```py\\nX_pos = np.array(X_pos)\\nX_neg = np.array(X_neg)\\nX_train = np.concatenate((X_pos,X_neg))\\ny_pos = np.array(y_pos)\\ny_neg = np.array(y_neg)\\ny_train = np.concatenate((y_pos,y_neg))\\n```\\nD\u1eef li\u1ec7u trining g\u1ed3m c\xf3 `X_traing` c\xf3 shape (1674, 3780) g\u1ed3m 1674 image v\xe0 3780 feature, `y_training` c\xf3 shape l\xe0 (1674,) g\u1ed3m 2 gi\xe1 tr\u1ecb 1 l\xe0 pedestrian v\xe0 -1 l\xe0 non-pedestrian\\n### 3. Build model\\nCh\xfang ta s\u1ebd training model b\u1eb1ng thu\u1eadt to\xe1n svm c\xf3 trong th\u01b0 vi\u1ec7n sklearn.\\n```py\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.metrics import classification_report\\nmodel = LinearSVC(C=0.01)\\nmodel.fit(X_train,y_train)\\ny_predict = model.predict(X_train)\\nprint(classification_report(y_train,y_predict))\\n```\\nK\u1ebft qu\u1ea3 nh\u01b0 sau :\\n\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./confustion_matrix.jpg\').default} />\\n</center>\\n\\n### 4. \u0110\xe1nh gi\xe1 v\xe0 c\u1ea3i thi\u1ec7n model\\nAmazing! k\u1ebft qu\u1ea3 accuracy = 100% . Qu\xe1 cao ph\u1ea3i ko. Nh\u01b0ng \u0111\u1eebng m\u1eebng v\u1ed9i v\xec data c\u1ee7a ch\xfang ta r\u1ea5t nh\u1ecf v\xe0 ta d\xf9ng to\xe0n b\u1ed9 data v\xe0o training m\xe0 ko chia ra data testing n\xean r\u1ea5t c\xf3 th\u1ec3 b\u1ecb overfiting. Khi \u0111\xf3 model \u0111\u01b0a v\xe0o ho\u1ea1t \u0111\u1ed9ng s\u1ebd predict kh\xf4ng t\u1ed1t. \u0110\u1ec3 tr\xe1nh \u0111i\u1ec1u n\xe0y\\nta c\xf3 th\u1ec3 thay \u0111\u1ed5i threshold  ( v\xec khi predict tr\xean image l\u1edbn s\u1ebd c\xf3 r\u1ea5t nhi\u1ec1u non-pedestrian h\u01a1n l\xe0 pedestrian).\u1ede trong sklearn m\u1eb7c \u0111\u1ecbnh `model.prediction` l\xe0 0.5 n\xean ta kh\xf4ng th\u1ec3 n\xe0o thay \u0111\u1ed5i \u0111\u01b0\u1ee3c n\xf3. Ta ch\u1ec9 c\xf3 th\u1ec3 thay \u0111\u1ed5i qua `decision_function`\\n\\n```py\\nfrom sklearn.metrics import precision_recall_curve\\nfrom sklearn.model_selection import cross_val_predict\\ny_scores = cross_val_predict(model, X_train, y_train, cv=3,\\n                             method=\\"decision_function\\")[:,1]\\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\\n\\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\\n    plt.plot(thresholds, precisions[:-1], \\"b--\\", label=\\"Precision\\")\\n    plt.plot(thresholds, recalls[:-1], \\"g-\\", label=\\"Recall\\")\\n    plt.xlabel(\\"Threshold\\")\\n    plt.legend(loc=\\"center left\\")\\n    plt.ylim([0, 1])\\n\\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\\n```\\nTa th\u1ea5y khi threshold = 0 recall = 1 , khi recall = 0.8 th\xec threshold t\u0103ng l\xean 0.7\\n## Giai \u0111o\u1ea1n 2  Detecter\\nB\xe2y gi\u1edd ta s\u1ebd detecter pedestrian tr\xean \u1ea3nh l\u1edbn.\\n### 1. X\xe2y d\u1ef1ng sliding window\\n```py\\ndef sliding_window(image,window_size,step_size):\\n    for y in range(0,image.shape[0]-window_size[1],step_size[1]):\\n        for x in range(0,image.shape[1]-window_size[0],step_size[0]):\\n            roi = image[y:y+window_size[1],x:x+window_size[0]]\\n            yield (x,y,roi)\\n```\\nFunction `sliding_window` c\xf3 3 para : 1 l\xe0 `image` (l\xe0 1 \u1ea3nh x\xe1m), hai l\xe0 `window_size` c\xf3 chi\u1ec1u (mxn) l\xe0 k\xedch th\u01b0\u1edbc window tr\xean image, cu\u1ed1i c\xf9ng l\xe0 `step_size` c\xf3 chi\u1ec1u (w,h) l\xe0 stride theo ox,oy tr\xean image.Gi\xe1 tr\u1ecb tr\u1ea3 v\u1ec1 l\xe0 v\u1ecb tr\xed (x,y) t\u01b0\u01a1ng \u1ee9ng l\xe0 (top-left) v\xe0 roi l\xe0 slide window t\u01b0\u01a1ng \u1ee9ng.\\n### 2. X\xe2y d\u1ef1ng NMS(non-maxinum-suppression)\\nTa ch\u1ec9 gi\u1eef l\u1ea1i 1 window tr\xean 1 object m\xe0 th\xf4i n\xean ta s\u1ebd d\xf9ng NMS \u0111\u1ec3 lo\u1ea1i b\u1ecf c\xe1c window c\xf2n l\u1ea1i, gi\u1eef l\u1ea1i window t\u1ed1i \u01b0u nh\u1ea5t.\u0110\u1ea7u ti\xean ta c\u1ea7n t\xednh area overlap gi\u1eefa 2 window.\\n```py\\ndef overlaping_area(detection_1,detection_2):\\n    #detection_1,detection_2 format [x_left_top,y_left_top,score,width,height]\\n    x_1 = detection_1[0]\\n    y_1 = detection_1[1]\\n    x_w_1 = detection_1[0] + detection_1[3]\\n    y_h_1 = detection_1[1] + detection_1[4]\\n    \\n    x_2 = detection_2[0]\\n    y_2 = detection_2[1]\\n    x_w_2 = detection_2[0] + detection_2[3]\\n    y_h_2 = detection_2[1] + detection_2[4]\\n    #t\xednh overlap theo ox,oy .N\u1ebfu ko giao nhau tr\u1ea3 v\u1ec1 0\\n    overlap_x = max(0,min(x_w_1,x_w_2) - max(x_1,x_2))\\n    overlap_y = max(0,min(y_h_1,y_h_2) - max(y_1,y_2))\\n    #t\xednh area overlap\\n    overlap_area = overlap_x*overlap_y\\n    #t\xednh total area h\u1ee3p c\u1ee7a 2 detection\\n    total_area = detection_1[3]*detection_1[4] + detection_2[3]*detection_2[4] - overlap_area\\n    \\n    return overlap_area/float(total_area)\\n```\\n\u0110\xe2y l\xe0 b\xe0i to\xe1n t\xecm intersection gi\u1eefa 2 rectangle b\u1ea1n c\xf3 th\u1ec3 search google xem c\xe1i gi\u1ea3i quy\u1ebft. H\xe0m overlaping_area s\u1ebd tr\u1ea3 v\u1ec1 t\u1ec9 l\u1ec7 overlap tr\xean t\u1ed5ng area gi\u1eefa 2 rectangle.\\nTi\u1ebfp theo ch\xfang ta x\xe2y d\u1ef1ng l\xe0m NMS .\\n```py\\ndef nms(detections,threshold =0.4):\\n    # decections format [x_left_top,y_left_top,score,width,height]\\n    # n\u1ebfu area overlap l\u1edbn h\u01a1n threshold th\xec s\u1ebd remove detection n\xe0o c\xf3 score nh\u1ecf h\u01a1n\\n    if len(detections)==0:\\n        return []\\n    #sort detection theo score\\n    detections = sorted(detections,key = lambda detections : detections[2],reverse = True)\\n    #create new detection\\n    new_detections = []\\n    new_detections.append(detections[0])\\n    del detections[0]\\n    for index,detection in enumerate(detections):\\n        for new_detection in new_detections:\\n            if overlaping_area(detection,new_detection)> threshold : #compare areaoverlap v\u1edbi threshold\\n                del detections[index]\\n                break\\n        else :\\n            new_detections.append(detection)\\n            del detections[index]\\n    return new_detections\\n```\\n\xdd t\u01b0\u1edfng l\xe0 ch\xfang ta s\u1ebd sort c\xe1c detection theo score( decision_function) theo th\u1ee9 t\u1ef1 gi\u1ea3m d\u1ea7n. Sau \u0111\xf3 so s\xe1nh c\xe1c detection v\u1edbi nhau, n\u1ebfu area overlap h\u01a1n threshold th\xec ta s\u1ebd gi\u1eef l\u1ea1i detection n\xe0o c\xf3 score l\u1edbn h\u01a1n.\\n### 3. Detecter\\n\u0110\u1ebfn \u0111\xe2y ta s\u1ebd stack c\xe1c function \u0111\xe3 t\u1ea1o l\u1ea1i v\u1edbi nhau th\xe0nh m\u1ed9t kh\u1ed1i \u0111\u1ec3 detection tr\xean \u1ea3nh l\u1edbn.\\n\\n```py\\nimage = cv2.imread(\\"pedestrian.jpg\\")\\nimage_test = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\\n\\nwindow_size = (64,128)\\nstep_size = (10,10)\\ndetections = []\\ndownscale=1.5\\nscale = 0\\nfor image_scale in pyramid_gaussian(image_test,downscale=2):\\n    scale += 1\\n    if image_scale.shape[0] < window_size[1] or image_scale.shape[1] < window_size[0]:\\n        break\\n    for (x,y,roi) in sliding_window(image_scale,window_size,step_size):\\n        feature = hog_feature(roi)\\n        predict = model.predict(feature.reshape((-1,3780)))\\n        score = model.decision_function(feature.reshape((-1,3780)))\\n        if (predict == 1) and (score>0.5):\\n            detections.append([x,y,np.round(score,4),window_size[0],window_size[1]])\\ndetections = nms(detections,0.5)\\nfor (x,y,_,w,h) in detections :\\n    cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),3)\\ncv2.imshow(\\"roi\\",image)\\ncv2.waitKey()\\ncv2.destroyAllWindows()\\n```\\nGi\u1ea3i th\xedch code 1 t\xed:\\n  * V\xec window size c\u1ed1 \u0111\u1ecbnh m\xe0 object m\u1ed7i image s\u1ebd c\xf3 k\xedch th\u01b0\u1edbc kh\xe1c nhau n\xean ta s\u1ebd d\xf9ng 1 function t\u1ea1o image pyramid, trong b\xe0i ta s\u1ebd s\u1eed d\u1ee5ng pyramid_gaussian v\u1edbi downscale = 2 , c\xf3 ngh\u0129a sau m\u1ed7i l\u1ea7n ch\u1ea1y image s\u1ebd gi\u1ea3m xu\u1ed1ng 1 n\u1eefa\\n  * \u0110\u1ec3 gi\u1ea3m b\u1edbt nhi\u1ec5u ta s\u1ebd s\u1eed d\u1ee5ng score > 0.25\\n  * \u1ede \u0111\xe2y c\xf3 m\u1ed9t nh\u01b0\u1ee3c \u0111i\u1ec3m l\xe0 khi image downscale th\xec bounding box c\u1ee7a ta s\u1ebd kh\xf4ng \u0111\u1ed5i l\xe0m cho object kh\xf4ng \u0111\u01b0\u1ee3c bao to\xe0n b\u1ed9 b\u1edfi bounding box m\xecnh \u0111\u1ecbnh s\u1ebd t\u0103ng k\xedch th\u01b0\u1edbc bounding box b\u1eb1ng c\xe1ch nh\xe2n cho n\xf3 1 t\u1ef7 l\u1ec7 b\u1eb1ng (downscale^scale) nh\u01b0ng k\u1ebft qu\u1ea3 l\xe0 bouding box qu\xe1 to. Hi\u1ec7n gi\u1edd m\xecnh ch\u01b0a t\xecm ra c\xe1ch x\u1eed l\xfd. C\xf3 th\u1ec3 xem minh h\u1ecda \u1edf h\xecnh d\u01b0\u1edbi.\\n\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./final1.jpg\').default} />\\n</center>\\n<center>\\n   <img width=\\"600\\" height=\\"300\\" src={require(\'./final2.jpg\').default} />\\n</center>  \\n\\n## K\u1ebft lu\u1eadn :\\nThu\u1eadt to\xe1n build model nhanh tuy nhi\xean c\xf3 m\u1ed9t nh\u01b0\u1ee3c \u0111i\u1ec3m l\xe0 predict tr\xean camera r\u1ea5t delay b\u1edfi v\xec ta s\u1eed d\u1ee5ng window search n\xean predict r\u1ea5t nhi\u1ec1u image d\u1eabn \u0111\u1ebfn t\u1ed1n th\u1eddi gian r\u1ea5t nhi\u1ec1u. Ng\xe0y nay ng\u01b0\u1eddi ta \u0111\xe3 gi\u1ea3i quy\u1ebft \u0111\u01b0\u1ee3c v\u1ea5n \u0111\u1ec1 n\xe0y b\u1eb1ng c\xe1ch s\u1eed d\u1ee5ng selective search c\xf3 ngh\u0129a l\xe0 ko search windown to\xe0n image n\u1eefa m\xe0 search c\xf3 ch\u1ecdn l\u1ecdc, nh\u1eefng region proposal m\xe0 c\xf3 nhi\u1ec1u kh\u1eb3n n\u0103ng c\xf3 object nh\u1ea5t \u0111i\u1ec3n h\xecnh l\xe0 thu\u1eadt to\xe1n R-CNN.\\n\\nTham Kh\u1ea3o : \\n* http://hanzratech.in/\\n* https://pyimagesearch.com\\n* https://learnopencv.com"},{"id":"T\xecm-hi\u1ec3u-v\u1ec1-Word2Vec","metadata":{"permalink":"/blog/T\xecm-hi\u1ec3u-v\u1ec1-Word2Vec","editUrl":"https://github.com/ThorPham/blog/2018-4-8-T\xecm-hi\u1ec3u-v\u1ec1-Word2Vec/index.md","source":"@site/blog/2018-4-8-T\xecm-hi\u1ec3u-v\u1ec1-Word2Vec/index.md","title":"T\xecm hi\u1ec3u v\u1ec1 Word2Vec","description":"*Nh\u01b0 ch\xfang ta \u0111\xe3 bi\u1ebft m\xe1y t\xednh \u0111\u01b0\u1ee3c c\u1ea5u t\u1ea1o t\u1eeb nh\u1eefng con s\u1ed1, do \u0111\xf3 n\xf3 ch\u1ec9 c\xf3 th\u1ec3 \u0111\u1ecdc \u0111\u01b0\u1ee3c d\u1eef li\u1ec7u s\u1ed1 m\xe0 th\xf4i. Trong natural language processing","date":"2018-04-08T00:00:00.000Z","formattedDate":"April 8, 2018","tags":[{"label":"NLP","permalink":"/blog/tags/nlp"},{"label":"python","permalink":"/blog/tags/python"},{"label":"Word2Vec","permalink":"/blog/tags/word-2-vec"}],"readingTime":6.89,"truncated":true,"authors":[{"name":"Thorpham","title":"Deep learning enthusiast","url":"https://github.com/ThorPham","imageURL":"https://github.com/ThorPham.png","key":"thorpham"}],"frontMatter":{"slug":"T\xecm-hi\u1ec3u-v\u1ec1-Word2Vec","title":"T\xecm hi\u1ec3u v\u1ec1 Word2Vec","authors":"thorpham","tags":["NLP","python","Word2Vec"]},"prevItem":{"title":"Nh\u1eadn di\u1ec7n pedestrian v\u1edbi window search","permalink":"/blog/Nh\u1eadn-di\u1ec7n-pedestrian-v\u1edbi-window-search"}},"content":"*Nh\u01b0 ch\xfang ta \u0111\xe3 bi\u1ebft m\xe1y t\xednh \u0111\u01b0\u1ee3c c\u1ea5u t\u1ea1o t\u1eeb nh\u1eefng con s\u1ed1, do \u0111\xf3 n\xf3 ch\u1ec9 c\xf3 th\u1ec3 \u0111\u1ecdc \u0111\u01b0\u1ee3c d\u1eef li\u1ec7u s\u1ed1 m\xe0 th\xf4i. Trong natural language processing\\nth\xec \u0111\u1ec3 x\u1eed l\xfd d\u1eef li\u1ec7u text ch\xfang ta c\u0169ng ph\u1ea3i chuy\u1ec3n d\u1eef li\u1ec7u t\u1eeb text sang numeric, t\u1ee9c l\xe0 \u0111\u01b0a n\xf3 v\xe0o m\u1ed9t kh\xf4ng gian m\u1edbi ng\u01b0\u1eddi ta th\u01b0\u1eddng\\ng\u1ecdi l\xe0 embbding. Tr\u01b0\u1edbc \u0111\xe2y ng\u01b0\u1eddi ta m\xe3 h\xf3a theo ki\u1ec3u one hot encoding t\u1ee9c l\xe0 t\u1ea1o  m\u1ed9t vocabualary cho d\u1eef li\u1ec7u v\xe0 m\xe3 h\xf3a c\xe1c word trong document\\nth\xe0nh nh\u1eefng vectoc, n\u1ebfu word \u0111\xf3 c\xf3 trong document th\xec m\xe3 h\xf3a l\xe0 1 c\xf2n kh\xf4ng c\xf3 s\u1ebd l\xe0 0. K\u1ebft qu\u1ea3 t\u1ea1o ra m\u1ed9t sparse matrix, t\u1ee9c l\xe0 matrix h\u1ea7u h\u1ebft \\nl\xe0 0.C\xe1c m\xe3 h\xf3a n\xe0y c\xf3 nhi\u1ec1u nh\u01b0\u1ee3c \u0111i\u1ec3m \u0111\xf3 l\xe0 th\u1ee9 nh\u1ea5t l\xe0 s\u1ed1 chi\u1ec1u c\u1ee7a n\xf3 r\u1ea5t l\u1edbn (NxM, N l\xe0 s\u1ed1 document c\xf2n M l\xe0 s\u1ed1 vocabulary), th\u1ee9 2 c\xe1c word\\nkh\xf4ng c\xf3 quan h\u1ec7 v\u1edbi nhau. \u0110i\u1ec1u \u0111\xf3 d\u1eabn \u0111\u1ebfn ng\u01b0\u1eddi ta ngh\u0129 ra m\u1ed9t model m\u1edbi c\xf3 t\xean l\xe0 **Word embbding**, \u1edf \u0111\xf3 c\xe1c word s\u1ebd c\xf3 quan h\u1ec7 v\u1edbi nhau v\u1ec1 semantic\\nt\u1ee9c l\xe0 v\xed d\u1ee5 nh\u01b0 paris-tokyo,man-women,boy-girl nh\u1eefng c\u1eb7p t\u1eeb n\xe0y s\u1ebd c\xf3 kho\u1ea3ng c\xe1ch g\u1ea7n nhau h\u01a1n trong Word embbding space. V\xed d\u1ee5 \u0111i\u1ec3n h\xecnh m\xe0 ta th\xe2y\\n\u0111\xf3 l\xe0 ph\u01b0\u01a1ng tr\xecnh king - queen = man - women . C\xe1i \u01b0u \u0111i\u1ec3m th\u1ee9 2 l\xe0 s\u1ed1 chi\u1ec1u c\u1ee7a n\xf3 s\u1ebd gi\u1ea3m ch\u1ec9 c\xf2n NxD.*\\n\x3c!--truncate--\x3e\\nWord embbding c\xf3 2 model n\u1ed5i ti\u1ebfng l\xe0 word2vec v\xe0 Glove.\\nWord2vec \u0111\u01b0\u1ee3c t\u1ea1o ra n\u0103m 2013 b\u1edfi m\u1ed9t k\u1ef9 s\u01b0 \u1edf google c\xf3 t\xean l\xe0 **Tomas Mikolov**. N\xf3 l\xe0 m\u1ed9t model unsupervised learning,\u0111\u01b0\u1ee3c training t\u1eeb large corpus . Chi\u1ec1u c\u1ee7a Word2vec nh\u1ecf h\u01a1n nhi\u1ec1u so v\u1edbi one-hot-encoding, v\u1edbi s\u1ed1 chi\u1ec1u l\xe0 NxD v\u1edbi N l\xe0 Number of document v\xe0 D l\xe0 s\u1ed1 chi\u1ec1u word embedding . Word2vec c\xf3 2 model l\xe0 skip-gram v\xe0 Cbow :\\n* Skip-gram model l\xe0 model predict word surrounding khi cho m\u1ed9t t\u1eeb cho tr\u01b0\u1edbc, v\xed d\u1ee5 nh\u01b0 text = \\"I love you so much\\". Khi d\xf9ng 1 window search c\xf3 size 3 ta thu \u0111\u01b0\u1ee3c  : {(i,you),love},{(love,so),you},{(you,much),so}. Nhi\u1ec7m v\u1ee5 c\u1ee7a n\xf3 l\xe0 khi cho 1 t\u1eeb center v\xed d\u1ee5 l\xe0 love th\xec ph\u1ea3i predict c\xe1c t\u1eeb xung quang l\xe0 i, you.\\n* Cbow l\xe0 vi\u1ebft t\u1eaft c\u1ee7a continous bag of word . Model n\xe0y ng\u01b0\u1ee3c v\u1edbi model skip-gram t\u1ee9c l\xe0 cho nh\u1eefng t\u1eeb surrounding predict word current.\\n* Trong th\u1ef1c t\u1ebf ng\u01b0\u1eddi ta ch\u1ec9 ch\u1ecdn m\u1ed9t trong  2 model \u0111\u1ec3 training, Cbow th\xec training nhanh h\u01a1n nh\u01b0ng \u0111\u1ed9 ch\xednh x\xe1c kh\xf4ng cao b\u1eb1ng skip-gram v\xe0 ng\u01b0\u1ee3c l\u1ea1i\\nGlove c\u0169ng \u0111\u01b0\u1ee3c t\u1ea1o ra n\u0103m 2013 b\u1edfi m\u1ed9t nh\xf3m nghi\xean c\u1ee9u \u1edf stanford. N\xf3 d\u1ef1a tr\xean word-count base model. N\xf3 d\xf9ng k\u1ef9 thu\u1eadt matrix factorization \u0111\u1ec3 \u0111\u01b0a matrix ban \u0111\u1ea7u v\u1ec1 c\xe1c matrix nh\u1ecf h\u01a1n t\u01b0\u01a1ng t\u1ef1 nh\u01b0 model \u1edf recommend system. M\xecnh ch\u01b0a nghi\xean c\u1ee9u model n\xe0y c\xf3 g\xec n\xf3 n\xf3i l\u1ea1i sau qua c\xe1c b\xe0i vi\u1ebft kh\xe1c.\\n\\nC\u1ea5u tr\xfac b\xe0i:\\n  * Math of Word2vec \\n  * Build model from scratch.\\n## Math of Word2vec\\nTrong b\xe0i n\xe0y ta ch\u1ec9 t\xecm hi\u1ec3u model Skip-gram model. Cbow l\xe0 model ng\u01b0\u1ee3c l\u1ea1i. Skip-gram model c\xf3 c\u1ea5u tr\xfac nh\u01b0 h\xecnh v\u1ebd d\u01b0\u1edbi \u0111\xe2y.\\n\\n<center>\\n   <img width=\\"600\\" height=\\"200\\" src={require(\'./word2vec1.jpg\').default} />\\n</center>\\n\\n* Input l\xe0 one-hot-vector m\u1ed7i word s\u1ebd c\xf3 d\u1ea1ng ${x_{1},x_{2},..x_{v}}$ trong \u0111\xf3 V l\xe0 s\u1ed1 vocabulary, l\xe0 m\u1ed9t vector trong \u0111\xf3 m\u1ed7i word s\u1ebd c\xf3\\ngi\xe1 tr\u1ecb 1 t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u1edbi index trong vocabulary v\xe0 c\xf2n l\u1ea1i s\u1ebd l\xe0 0.\\n* Weight matrix gi\u1eefa input v\xe0 hidden layer l\xe0 matrix W(c\xf3 dimention VxN) c\xf3 active function l\xe0 linear, weight gi\u1eefa hidden v\xe0 out put l\xe0 $W^{\'}$ (c\xf3 dimention l\xe0 NxV) active function c\u1ee7a out put l\xe0 soft max.\\n* M\u1ed7i row c\u1ee7a W l\xe0 vector N chi\u1ec1u \u0111\u1ea1i di\u1ec7n cho $v_{w}$ l\xe0 m\u1ed7i word trong input layer. M\u1ed7i row c\u1ee7a W l\xe0 $v_{w}^{T}$ . L\u01b0u \xfd l\xe0 input l\xe0 1 one hot vector (s\u1ebd c\xf3 d\u1ea1ng 000100) ch\u1ec9 c\xf3 1 ph\u1ea7n t\u1eed b\u1eb1ng 1 n\xean.\\n\\n$$\\nh = W^{T}x = v_{w}^{T}\\n$$\\n\\n* T\u1eeb hidden layer \u0111\u1ebfn out put l\xe0 matrix $W^{\'} = {w_{i,j}^{\'}}$ . Ta t\xednh score $u_{i}$ cho m\u1ed7i word trong vocabulary.\\n\\n$$\\nu_{j} = v_{w_{j}}^{\'}h\\n$$ \\n\\n* Trong \u0111\xf3 $v_{w_{j}}$ l\xe0 vector colum j trong $W^{\'}$. Ti\u1ebfp \u0111\xf3 ta s\u1eed d\u1ee5ng soft max funtion.\\n\\n$$ \\nP(w_{j}|w_{I}) = y_{i} = \\\\frac{exp(u_{j})}{\\\\sum_{j^{\'}=1}^{V}exp(u_{j^{\'}})} = = \\\\frac{exp(v^{\'T}_{w_{j}}v_{w_{I}})}{\\\\sum_{j^{\'}=1}^{V}exp(v^{\'T}_{w_{j^{\'}}}v_{w_{I}})}\\n$$\\n\\n* Trong \u0111\xf3 $v_{w}$ v\xe0 $v_{w^{\'}}$ l\xe0 2 vector \u0111\u1ea1i di\u1ec7n cho word w \u0111\xean t\u1eeb matrix W v\xe0 $W^{\'}$ .\\n* Ng\u01b0\u1eddi ta d\xf9ng maximum likehood v\u1edbi gradient descent \u0111\u1ec3 gi\u1ea3i quy\u1ebft b\xe0i to\xe1n n\xe0y nh\u01b0ng v\xec vocabulary l\u1edbn n\xean t\xednh to\xe1n m\u1eabu s\u1ed1 n\xf3 t\xednh tr\xean to\xe0n b\u1ed9 vocabulary n\xean chi ph\xed t\xednh to\xe1n l\u1edbn n\xean ng\u01b0\u1eddi ta d\xf9ng 2 ph\u01b0\u01a1ng ph\xe1p gi\u1ea3i quy\u1ebft l\xe0 Hierarchical Softmax ho\u1eb7c Negative Sampling chi ti\u1ebft trong paper \\"word2vec Parameter Learning Explained\\" c\u1ee7a Xin Rong\\n## Build model from scratch.\\n* Model b\u1eb1ng math ch\u1eafc chi ti\u1ebft \u0111\u1ebfn m\u1ea5y c\u0169ng kh\xf4ng b\u1eb1ng m\u1ed9t v\xed d\u1ee5 th\u1ef1c t\u1ebf. N\xf3i th\u1eadt to\xe1n m\xecnh r\u1ea5t k\xe9m n\xean m\xecnh hay t\xecm \u0111\u1ecdc c\xe1c v\xed d\u1ee5 tr\u1ef1c quan r\u1ed3i suy ng\u01b0\u1ee3c l\u1ea1i c\xf4ng th\u1ee9c to\xe1n h\u1ecdc.\\n* Ch\xfang ta s\u1ebd x\xe2y d\u1ef1ng model skip-gram \u0111\u01a1n gi\u1ea3n nh\u01b0 sau : Gi\u1ea3 s\u1eed ta c\xf3 \u0111o\u1ea1n text : \\"He is the king . The king is royal . She is the royal  queen\\" c\xe1c b\u01b0\u1edbc s\u1ebd l\xe0 :\\n  * L\xe0m s\u1ea1ch d\u1eef li\u1ec7u , lower v\xe0 b\u1ecf d\u1ea5u ch\u1ea5m\\n  * X\xe2y d\u1ef1ng vocabulary v\xe0 2 c\xe1i dictionary , 1 c\xe1i \u0111\u1ec3 t\xecm index theo word c\xe1i c\xf2n l\u1ea1i th\xec ng\u01b0\u1ee3c l\u1ea1i\\n  * Ti\u1ebfp theo x\xe2y d\u1ef1ng d\u1eef li\u1ec7u training, d\xf9ng 1 window search \u0111\u1ec3 stride tr\xean text, d\u1eef li\u1ec7u s\u1ebd c\xf3 d\u1ea1ng  (he,is),(he,the)..\\n  * M\xe3 h\xf3a d\u1eef li\u1ec7u v\u1ec1 numeric d\u1ef1a v\xe0o 2 c\xe1i dictionary v\u1eeba t\u1ea1o\\n  * Traing model\\n  * T\xecm matrix word embbding v\xe0o v\u1ebd \u0111\u1ed3 th\u1ecb minh h\u1ecda.\\n* \u0110\u1ea7u ti\xean ch\xfang ta l\xe0m s\u1ea1ch d\u1eef li\u1ec7u v\xe0 x\xe2y d\u1ef1ng vocabulary . {\'queen\', \'he\', \'royal\', \'is\', \'king\', \'the\', \'she\'}\\n* Ti\u1ebfp theo l\xe0 2 dictionary \\n  * word theo index l\xe0 {\'queen\': 0, \'he\': 1, \'royal\': 2, \'is\': 3, \'king\': 4, \'the\': 5, \'she\': 6}\\n  * Index theo word th\xec ng\u01b0\u1ee3c l\u1ea1i : {0: \'queen\', 1: \'he\', 2: \'royal\', 3: \'is\', 4: \'king\', 5: \'the\', 6: \'she\'}\\n  * Ti\u1ebfp theo x\xe2y d\u1ef1ng data training s\u1ebd c\xf3 d\u1ea1ng :(he,is),(he,the)... trong \u0111\xf3 c\xe1i \u0111\u1ea7u s\u1ebd l\xe0 input, c\xe1i sau s\u1ebd l\xe0 out put\\n  * m\xe3 h\xf3a n\xf3 v\u1ec1 numeric : s\u1ebd c\xf3 d\u1ea1ng sau :input : [ 0.,  0.,  0.,  1.,  0.,  0.,  0.] , ouput : [ 1.,  0.,  0.,  0.,  0.,  0.,  0.]\\n  l\u01b0u \xfd size vocabulary l\xe0 7\\n  * Ta s\u1ebd training model : V\xec vocabualary n\xe0y nh\u1ecf n\xean ta d\xf9ng gradient descent training loss softmax minimum lu\xf4n.\\n```python\\nimport numpy as np\\nimport tensorflow as tf\\n\\ncorpus_raw =\\"He is the king . The king is royal . She is the royal  queen\\"\\n#Convert to lower case\\ncorpus_raw = corpus_raw.lower()\\n\\nwords = []\\nfor word in corpus_raw.split():\\n    if word != \\".\\": #we need remove \\".\\"\\n        words.append(word)\\nwords = set(words) #We create dictionary so remove duplicate word\\n\\nword2int = {}\\nint2word = {}\\nvocab_size = len(words)\\nfor i,word in enumerate(words):\\n    word2int[word] = i\\n    int2word[i] = word\\n#Raw sentence as list \\nraw_sentence = corpus_raw.split(\\".\\")\\nsentences = []\\nfor sentence in raw_sentence:\\n    sentences.append(sentence.split())\\n#Generate training data\\ndata = []\\nWindow_size = 2\\nfor sentence in sentences :\\n    for word_index,word in enumerate(sentence):\\n        for nb_word in sentence[max(word_index - Window_size,0): min(word_index+ Window_size,len(sentence)) +1 ]:\\n            if nb_word != word :\\n                data.append([word,nb_word]) \\n #function to convert numbers to one hot vectors\\ndef to_one_hot(data_point_index, vocab_size):\\n    temp = np.zeros(vocab_size)\\n    temp[data_point_index] = 1\\n    \\nx_train = [] #input word\\ny_train = [] #output word\\nfor data_word in data:\\n    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\\n    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\\n  \\n\\nx_train = np.asarray(x_train)\\ny_train = np.asarray(y_train)\\n    return temp  \\n    \\n#placeholder\\nX = tf.placeholder(tf.float32,[None,7])\\nY = tf.placeholder(tf.float32,[None,7])\\n#variable hiden 1\\nW1 = tf.Variable(tf.random_normal([7,5]))\\nb1 = tf.Variable(tf.constant(0.1,shape =[5]))\\nhiden_1 = tf.matmul(X,W1) + b1\\n#variable hiden 2\\nW2 = tf.Variable(tf.random_normal([5,7]))\\nb2 = tf.Variable(tf.constant(0.1,shape = [7]))\\nhiden_2 = tf.matmul(hiden_1,W2) + b2\\n#loss function\\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = Y,logits=hiden_2))\\n#optimizer\\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cross_entropy)\\n#initializer\\ninit = tf.global_variables_initializer()\\nsess = tf.Session()\\nsess.run(init)\\nfor i in range(1000):\\n    sess.run(optimizer,feed_dict={X:x_train,Y:y_train})\\npredict = tf.equal(tf.arg_max(hiden_2,1),tf.arg_max(Y,1))\\naccuracy = tf.reduce_mean(tf.cast(predict,tf.float32))\\n```"}]}')}}]);